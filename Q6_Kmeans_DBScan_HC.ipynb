{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ac7e81",
   "metadata": {},
   "source": [
    "# Question 6.\n",
    "Use Simple Kmeans, DBScan, Hierachical clustering algorithms for clustering. Compare the performance of clusters by changing the parameters involved in the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40e67a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.cluster\n",
    "import scipy.cluster.hierarchy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1142bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('iris.data',header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89263cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2, -1, -1,  3, -1,  4, -1,  5, -1,  6,  7, -1, -1, -1,  3,\n",
       "        0, -1,  8,  9,  8, -1, 10,  6,  1,  4, -1, -1,  2, -1,  9, -1, -1,\n",
       "        5, 10, -1,  5, -1,  4,  0, -1, -1,  0,  8,  7,  8, -1, -1, 10, 11,\n",
       "       12, 13, -1, 14, 15, 16, -1, -1, -1, -1, 17, 18, 19, -1, 20, 21, 15,\n",
       "       -1, 22, -1, 23, 24, 23, -1, 25, -1, 26, 19, 27, 28, 28, 15, -1, -1,\n",
       "       -1, 20, -1, 21, 22, -1, 17, 27, -1, 29, -1, -1, 30, -1, 15, 16, 15,\n",
       "       -1, 30, 25, -1, -1, -1, -1, -1, 12, 14, 26, -1, 15, 12, 25, -1, -1,\n",
       "       18, 11, 29, -1, 31, 32, -1, 31, 17, 14, -1, -1, -1, 14, 31, -1, -1,\n",
       "       33, -1, 17, 13, 20, 13, 15, -1, 32, 26, 24, 25, 33, 17],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering = sklearn.cluster.DBSCAN(eps=0.1, min_samples=2).fit(df[[0,1]])\n",
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6fdcd98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = sklearn.cluster.KMeans(n_clusters=2, random_state=0).fit(df[[0,1]])\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbf0d373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Z = scipy.cluster.hierarchy.linkage(df[[1,2]], 'single')\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "dn = scipy.cluster.hierarchy.dendrogram(Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b71fd28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function linkage in module scipy.cluster.hierarchy:\n",
      "\n",
      "linkage(y, method='single', metric='euclidean', optimal_ordering=False)\n",
      "    Perform hierarchical/agglomerative clustering.\n",
      "    \n",
      "    The input y may be either a 1-D condensed distance matrix\n",
      "    or a 2-D array of observation vectors.\n",
      "    \n",
      "    If y is a 1-D condensed distance matrix,\n",
      "    then y must be a :math:`\\binom{n}{2}` sized\n",
      "    vector, where n is the number of original observations paired\n",
      "    in the distance matrix. The behavior of this function is very\n",
      "    similar to the MATLAB linkage function.\n",
      "    \n",
      "    A :math:`(n-1)` by 4 matrix ``Z`` is returned. At the\n",
      "    :math:`i`-th iteration, clusters with indices ``Z[i, 0]`` and\n",
      "    ``Z[i, 1]`` are combined to form cluster :math:`n + i`. A\n",
      "    cluster with an index less than :math:`n` corresponds to one of\n",
      "    the :math:`n` original observations. The distance between\n",
      "    clusters ``Z[i, 0]`` and ``Z[i, 1]`` is given by ``Z[i, 2]``. The\n",
      "    fourth value ``Z[i, 3]`` represents the number of original\n",
      "    observations in the newly formed cluster.\n",
      "    \n",
      "    The following linkage methods are used to compute the distance\n",
      "    :math:`d(s, t)` between two clusters :math:`s` and\n",
      "    :math:`t`. The algorithm begins with a forest of clusters that\n",
      "    have yet to be used in the hierarchy being formed. When two\n",
      "    clusters :math:`s` and :math:`t` from this forest are combined\n",
      "    into a single cluster :math:`u`, :math:`s` and :math:`t` are\n",
      "    removed from the forest, and :math:`u` is added to the\n",
      "    forest. When only one cluster remains in the forest, the algorithm\n",
      "    stops, and this cluster becomes the root.\n",
      "    \n",
      "    A distance matrix is maintained at each iteration. The ``d[i,j]``\n",
      "    entry corresponds to the distance between cluster :math:`i` and\n",
      "    :math:`j` in the original forest.\n",
      "    \n",
      "    At each iteration, the algorithm must update the distance matrix\n",
      "    to reflect the distance of the newly formed cluster u with the\n",
      "    remaining clusters in the forest.\n",
      "    \n",
      "    Suppose there are :math:`|u|` original observations\n",
      "    :math:`u[0], \\ldots, u[|u|-1]` in cluster :math:`u` and\n",
      "    :math:`|v|` original objects :math:`v[0], \\ldots, v[|v|-1]` in\n",
      "    cluster :math:`v`. Recall, :math:`s` and :math:`t` are\n",
      "    combined to form cluster :math:`u`. Let :math:`v` be any\n",
      "    remaining cluster in the forest that is not :math:`u`.\n",
      "    \n",
      "    The following are methods for calculating the distance between the\n",
      "    newly formed cluster :math:`u` and each :math:`v`.\n",
      "    \n",
      "      * method='single' assigns\n",
      "    \n",
      "        .. math::\n",
      "           d(u,v) = \\min(dist(u[i],v[j]))\n",
      "    \n",
      "        for all points :math:`i` in cluster :math:`u` and\n",
      "        :math:`j` in cluster :math:`v`. This is also known as the\n",
      "        Nearest Point Algorithm.\n",
      "    \n",
      "      * method='complete' assigns\n",
      "    \n",
      "        .. math::\n",
      "           d(u, v) = \\max(dist(u[i],v[j]))\n",
      "    \n",
      "        for all points :math:`i` in cluster u and :math:`j` in\n",
      "        cluster :math:`v`. This is also known by the Farthest Point\n",
      "        Algorithm or Voor Hees Algorithm.\n",
      "    \n",
      "      * method='average' assigns\n",
      "    \n",
      "        .. math::\n",
      "           d(u,v) = \\sum_{ij} \\frac{d(u[i], v[j])}\n",
      "                                   {(|u|*|v|)}\n",
      "    \n",
      "        for all points :math:`i` and :math:`j` where :math:`|u|`\n",
      "        and :math:`|v|` are the cardinalities of clusters :math:`u`\n",
      "        and :math:`v`, respectively. This is also called the UPGMA\n",
      "        algorithm.\n",
      "    \n",
      "      * method='weighted' assigns\n",
      "    \n",
      "        .. math::\n",
      "           d(u,v) = (dist(s,v) + dist(t,v))/2\n",
      "    \n",
      "        where cluster u was formed with cluster s and t and v\n",
      "        is a remaining cluster in the forest (also called WPGMA).\n",
      "    \n",
      "      * method='centroid' assigns\n",
      "    \n",
      "        .. math::\n",
      "           dist(s,t) = ||c_s-c_t||_2\n",
      "    \n",
      "        where :math:`c_s` and :math:`c_t` are the centroids of\n",
      "        clusters :math:`s` and :math:`t`, respectively. When two\n",
      "        clusters :math:`s` and :math:`t` are combined into a new\n",
      "        cluster :math:`u`, the new centroid is computed over all the\n",
      "        original objects in clusters :math:`s` and :math:`t`. The\n",
      "        distance then becomes the Euclidean distance between the\n",
      "        centroid of :math:`u` and the centroid of a remaining cluster\n",
      "        :math:`v` in the forest. This is also known as the UPGMC\n",
      "        algorithm.\n",
      "    \n",
      "      * method='median' assigns :math:`d(s,t)` like the ``centroid``\n",
      "        method. When two clusters :math:`s` and :math:`t` are combined\n",
      "        into a new cluster :math:`u`, the average of centroids s and t\n",
      "        give the new centroid :math:`u`. This is also known as the\n",
      "        WPGMC algorithm.\n",
      "    \n",
      "      * method='ward' uses the Ward variance minimization algorithm.\n",
      "        The new entry :math:`d(u,v)` is computed as follows,\n",
      "    \n",
      "        .. math::\n",
      "    \n",
      "           d(u,v) = \\sqrt{\\frac{|v|+|s|}\n",
      "                               {T}d(v,s)^2\n",
      "                        + \\frac{|v|+|t|}\n",
      "                               {T}d(v,t)^2\n",
      "                        - \\frac{|v|}\n",
      "                               {T}d(s,t)^2}\n",
      "    \n",
      "        where :math:`u` is the newly joined cluster consisting of\n",
      "        clusters :math:`s` and :math:`t`, :math:`v` is an unused\n",
      "        cluster in the forest, :math:`T=|v|+|s|+|t|`, and\n",
      "        :math:`|*|` is the cardinality of its argument. This is also\n",
      "        known as the incremental algorithm.\n",
      "    \n",
      "    Warning: When the minimum distance pair in the forest is chosen, there\n",
      "    may be two or more pairs with the same minimum distance. This\n",
      "    implementation may choose a different minimum than the MATLAB\n",
      "    version.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y : ndarray\n",
      "        A condensed distance matrix. A condensed distance matrix\n",
      "        is a flat array containing the upper triangular of the distance matrix.\n",
      "        This is the form that ``pdist`` returns. Alternatively, a collection of\n",
      "        :math:`m` observation vectors in :math:`n` dimensions may be passed as\n",
      "        an :math:`m` by :math:`n` array. All elements of the condensed distance\n",
      "        matrix must be finite, i.e., no NaNs or infs.\n",
      "    method : str, optional\n",
      "        The linkage algorithm to use. See the ``Linkage Methods`` section below\n",
      "        for full descriptions.\n",
      "    metric : str or function, optional\n",
      "        The distance metric to use in the case that y is a collection of\n",
      "        observation vectors; ignored otherwise. See the ``pdist``\n",
      "        function for a list of valid distance metrics. A custom distance\n",
      "        function can also be used.\n",
      "    optimal_ordering : bool, optional\n",
      "        If True, the linkage matrix will be reordered so that the distance\n",
      "        between successive leaves is minimal. This results in a more intuitive\n",
      "        tree structure when the data are visualized. defaults to False, because\n",
      "        this algorithm can be slow, particularly on large datasets [2]_. See\n",
      "        also the `optimal_leaf_ordering` function.\n",
      "    \n",
      "        .. versionadded:: 1.0.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Z : ndarray\n",
      "        The hierarchical clustering encoded as a linkage matrix.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    1. For method 'single', an optimized algorithm based on minimum spanning\n",
      "       tree is implemented. It has time complexity :math:`O(n^2)`.\n",
      "       For methods 'complete', 'average', 'weighted' and 'ward', an algorithm\n",
      "       called nearest-neighbors chain is implemented. It also has time\n",
      "       complexity :math:`O(n^2)`.\n",
      "       For other methods, a naive algorithm is implemented with :math:`O(n^3)`\n",
      "       time complexity.\n",
      "       All algorithms use :math:`O(n^2)` memory.\n",
      "       Refer to [1]_ for details about the algorithms.\n",
      "    2. Methods 'centroid', 'median', and 'ward' are correctly defined only if\n",
      "       Euclidean pairwise metric is used. If `y` is passed as precomputed\n",
      "       pairwise distances, then it is the user's responsibility to assure that\n",
      "       these distances are in fact Euclidean, otherwise the produced result\n",
      "       will be incorrect.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    scipy.spatial.distance.pdist : pairwise distance metrics\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Daniel Mullner, \"Modern hierarchical, agglomerative clustering\n",
      "           algorithms\", :arXiv:`1109.2378v1`.\n",
      "    .. [2] Ziv Bar-Joseph, David K. Gifford, Tommi S. Jaakkola, \"Fast optimal\n",
      "           leaf ordering for hierarchical clustering\", 2001. Bioinformatics\n",
      "           :doi:`10.1093/bioinformatics/17.suppl_1.S22`\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from scipy.cluster.hierarchy import dendrogram, linkage\n",
      "    >>> from matplotlib import pyplot as plt\n",
      "    >>> X = [[i] for i in [2, 8, 0, 4, 1, 9, 9, 0]]\n",
      "    \n",
      "    >>> Z = linkage(X, 'ward')\n",
      "    >>> fig = plt.figure(figsize=(25, 10))\n",
      "    >>> dn = dendrogram(Z)\n",
      "    \n",
      "    >>> Z = linkage(X, 'single')\n",
      "    >>> fig = plt.figure(figsize=(25, 10))\n",
      "    >>> dn = dendrogram(Z)\n",
      "    >>> plt.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(scipy.cluster.hierarchy.linkage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "089c6691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn\n",
      "\n",
      "DESCRIPTION\n",
      "    Machine learning module for Python\n",
      "    ==================================\n",
      "    \n",
      "    sklearn is a Python module integrating classical machine\n",
      "    learning algorithms in the tightly-knit world of scientific Python\n",
      "    packages (numpy, scipy, matplotlib).\n",
      "    \n",
      "    It aims to provide simple and efficient solutions to learning problems\n",
      "    that are accessible to everybody and reusable in various contexts:\n",
      "    machine-learning as a versatile tool for science and engineering.\n",
      "    \n",
      "    See http://scikit-learn.org for complete documentation.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __check_build (package)\n",
      "    _build_utils (package)\n",
      "    _config\n",
      "    _distributor_init\n",
      "    _isotonic\n",
      "    _loss (package)\n",
      "    _min_dependencies\n",
      "    base\n",
      "    calibration\n",
      "    cluster (package)\n",
      "    compose (package)\n",
      "    conftest\n",
      "    covariance (package)\n",
      "    cross_decomposition (package)\n",
      "    datasets (package)\n",
      "    decomposition (package)\n",
      "    discriminant_analysis\n",
      "    dummy\n",
      "    ensemble (package)\n",
      "    exceptions\n",
      "    experimental (package)\n",
      "    externals (package)\n",
      "    feature_extraction (package)\n",
      "    feature_selection (package)\n",
      "    gaussian_process (package)\n",
      "    impute (package)\n",
      "    inspection (package)\n",
      "    isotonic\n",
      "    kernel_approximation\n",
      "    kernel_ridge\n",
      "    linear_model (package)\n",
      "    manifold (package)\n",
      "    metrics (package)\n",
      "    mixture (package)\n",
      "    model_selection (package)\n",
      "    multiclass\n",
      "    multioutput\n",
      "    naive_bayes\n",
      "    neighbors (package)\n",
      "    neural_network (package)\n",
      "    pipeline\n",
      "    preprocessing (package)\n",
      "    random_projection\n",
      "    semi_supervised (package)\n",
      "    setup\n",
      "    svm (package)\n",
      "    tests (package)\n",
      "    tree (package)\n",
      "    utils (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    clone(estimator, *, safe=True)\n",
      "        Construct a new unfitted estimator with the same parameters.\n",
      "        \n",
      "        Clone does a deep copy of the model in an estimator\n",
      "        without actually copying attached data. It returns a new estimator\n",
      "        with the same parameters that has not been fitted on any data.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : {list, tuple, set} of estimator instance or a single             estimator instance\n",
      "            The estimator or group of estimators to be cloned.\n",
      "        safe : bool, default=True\n",
      "            If safe is False, clone will fall back to a deep copy on objects\n",
      "            that are not estimators.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        estimator : object\n",
      "            The deep copy of the input, an estimator if input is an estimator.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If the estimator's `random_state` parameter is an integer (or if the\n",
      "        estimator doesn't have a `random_state` parameter), an *exact clone* is\n",
      "        returned: the clone and the original estimator will give the exact same\n",
      "        results. Otherwise, *statistical clone* is returned: the clone might\n",
      "        return different results from the original estimator. More details can be\n",
      "        found in :ref:`randomness`.\n",
      "    \n",
      "    config_context(*, assume_finite=None, working_memory=None, print_changed_only=None, display=None)\n",
      "        Context manager for global scikit-learn configuration.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, default=None\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error. If None, the existing value won't change.\n",
      "            The default value is False.\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. If None, the existing value won't change.\n",
      "            The default value is 1024.\n",
      "        \n",
      "        print_changed_only : bool, default=None\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()', but would print\n",
      "            'SVC(C=1.0, cache_size=200, ...)' with all the non-changed parameters\n",
      "            when False. If None, the existing value won't change.\n",
      "            The default value is True.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Default changed from False to True.\n",
      "        \n",
      "        display : {'text', 'diagram'}, default=None\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. If None, the existing value won't change.\n",
      "            The default value is 'text'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        Yields\n",
      "        ------\n",
      "        None.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        set_config : Set global scikit-learn configuration.\n",
      "        get_config : Retrieve current values of the global configuration.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        All settings, not just those presently modified, will be returned to\n",
      "        their previous values when the context manager is exited.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import sklearn\n",
      "        >>> from sklearn.utils.validation import assert_all_finite\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     assert_all_finite([float('nan')])\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     with sklearn.config_context(assume_finite=False):\n",
      "        ...         assert_all_finite([float('nan')])\n",
      "        Traceback (most recent call last):\n",
      "        ...\n",
      "        ValueError: Input contains NaN...\n",
      "    \n",
      "    get_config()\n",
      "        Retrieve current values for configuration set by :func:`set_config`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        config : dict\n",
      "            Keys are parameter names that can be passed to :func:`set_config`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context : Context manager for global scikit-learn configuration.\n",
      "        set_config : Set global scikit-learn configuration.\n",
      "    \n",
      "    set_config(assume_finite=None, working_memory=None, print_changed_only=None, display=None)\n",
      "        Set global scikit-learn configuration\n",
      "        \n",
      "        .. versionadded:: 0.19\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, default=None\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error.  Global default: False.\n",
      "        \n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. Global default: 1024.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        print_changed_only : bool, default=None\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()' while the default\n",
      "            behaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\n",
      "            all the non-changed parameters.\n",
      "        \n",
      "            .. versionadded:: 0.21\n",
      "        \n",
      "        display : {'text', 'diagram'}, default=None\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. Default is 'text'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context : Context manager for global scikit-learn configuration.\n",
      "        get_config : Retrieve current values of the global configuration.\n",
      "    \n",
      "    show_versions()\n",
      "        Print useful debugging information\"\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "\n",
      "DATA\n",
      "    __SKLEARN_SETUP__ = False\n",
      "    __all__ = ['calibration', 'cluster', 'covariance', 'cross_decompositio...\n",
      "\n",
      "VERSION\n",
      "    1.0.2\n",
      "\n",
      "FILE\n",
      "    c:\\users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24eca5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.cluster in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.cluster\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.cluster` module gathers popular unsupervised clustering\n",
      "    algorithms.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _affinity_propagation\n",
      "    _agglomerative\n",
      "    _bicluster\n",
      "    _birch\n",
      "    _dbscan\n",
      "    _dbscan_inner\n",
      "    _feature_agglomeration\n",
      "    _hierarchical_fast\n",
      "    _k_means_common\n",
      "    _k_means_elkan\n",
      "    _k_means_lloyd\n",
      "    _k_means_minibatch\n",
      "    _kmeans\n",
      "    _mean_shift\n",
      "    _optics\n",
      "    _spectral\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        sklearn.cluster._affinity_propagation.AffinityPropagation(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._agglomerative.AgglomerativeClustering(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.cluster._agglomerative.FeatureAgglomeration(sklearn.cluster._agglomerative.AgglomerativeClustering, sklearn.cluster._feature_agglomeration.AgglomerationTransform)\n",
      "        sklearn.cluster._birch.Birch(sklearn.base.ClusterMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._dbscan.DBSCAN(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._kmeans.KMeans(sklearn.base.TransformerMixin, sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.cluster._kmeans.MiniBatchKMeans\n",
      "        sklearn.cluster._mean_shift.MeanShift(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._optics.OPTICS(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._spectral.SpectralClustering(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.ClusterMixin(builtins.object)\n",
      "        sklearn.cluster._affinity_propagation.AffinityPropagation(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._agglomerative.AgglomerativeClustering(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.cluster._agglomerative.FeatureAgglomeration(sklearn.cluster._agglomerative.AgglomerativeClustering, sklearn.cluster._feature_agglomeration.AgglomerationTransform)\n",
      "        sklearn.cluster._birch.Birch(sklearn.base.ClusterMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._dbscan.DBSCAN(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._kmeans.KMeans(sklearn.base.TransformerMixin, sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.cluster._kmeans.MiniBatchKMeans\n",
      "        sklearn.cluster._mean_shift.MeanShift(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._optics.OPTICS(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._spectral.SpectralClustering(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.TransformerMixin(builtins.object)\n",
      "        sklearn.cluster._birch.Birch(sklearn.base.ClusterMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._kmeans.KMeans(sklearn.base.TransformerMixin, sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.cluster._kmeans.MiniBatchKMeans\n",
      "    sklearn.cluster._bicluster.BaseSpectral(sklearn.base.BiclusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._bicluster.SpectralBiclustering\n",
      "        sklearn.cluster._bicluster.SpectralCoclustering\n",
      "    \n",
      "    class AffinityPropagation(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "     |  AffinityPropagation(*, damping=0.5, max_iter=200, convergence_iter=15, copy=True, preference=None, affinity='euclidean', verbose=False, random_state=None)\n",
      "     |  \n",
      "     |  Perform Affinity Propagation Clustering of data.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <affinity_propagation>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  damping : float, default=0.5\n",
      "     |      Damping factor in the range `[0.5, 1.0)` is the extent to\n",
      "     |      which the current value is maintained relative to\n",
      "     |      incoming values (weighted 1 - damping). This in order\n",
      "     |      to avoid numerical oscillations when updating these\n",
      "     |      values (messages).\n",
      "     |  \n",
      "     |  max_iter : int, default=200\n",
      "     |      Maximum number of iterations.\n",
      "     |  \n",
      "     |  convergence_iter : int, default=15\n",
      "     |      Number of iterations with no change in the number\n",
      "     |      of estimated clusters that stops the convergence.\n",
      "     |  \n",
      "     |  copy : bool, default=True\n",
      "     |      Make a copy of input data.\n",
      "     |  \n",
      "     |  preference : array-like of shape (n_samples,) or float, default=None\n",
      "     |      Preferences for each point - points with larger values of\n",
      "     |      preferences are more likely to be chosen as exemplars. The number\n",
      "     |      of exemplars, ie of clusters, is influenced by the input\n",
      "     |      preferences value. If the preferences are not passed as arguments,\n",
      "     |      they will be set to the median of the input similarities.\n",
      "     |  \n",
      "     |  affinity : {'euclidean', 'precomputed'}, default='euclidean'\n",
      "     |      Which affinity to use. At the moment 'precomputed' and\n",
      "     |      ``euclidean`` are supported. 'euclidean' uses the\n",
      "     |      negative squared euclidean distance between points.\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Whether to be verbose.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Pseudo-random number generator to control the starting state.\n",
      "     |      Use an int for reproducible results across function calls.\n",
      "     |      See the :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |          this parameter was previously hardcoded as 0.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cluster_centers_indices_ : ndarray of shape (n_clusters,)\n",
      "     |      Indices of cluster centers.\n",
      "     |  \n",
      "     |  cluster_centers_ : ndarray of shape (n_clusters, n_features)\n",
      "     |      Cluster centers (if affinity != ``precomputed``).\n",
      "     |  \n",
      "     |  labels_ : ndarray of shape (n_samples,)\n",
      "     |      Labels of each point.\n",
      "     |  \n",
      "     |  affinity_matrix_ : ndarray of shape (n_samples, n_samples)\n",
      "     |      Stores the affinity matrix used in ``fit``.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations taken to converge.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  AgglomerativeClustering : Recursively merges the pair of\n",
      "     |      clusters that minimally increases a given linkage distance.\n",
      "     |  FeatureAgglomeration : Similar to AgglomerativeClustering,\n",
      "     |      but recursively merges features instead of samples.\n",
      "     |  KMeans : K-Means clustering.\n",
      "     |  MiniBatchKMeans : Mini-Batch K-Means clustering.\n",
      "     |  MeanShift : Mean shift clustering using a flat kernel.\n",
      "     |  SpectralClustering : Apply clustering to a projection\n",
      "     |      of the normalized Laplacian.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n",
      "     |  <sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n",
      "     |  \n",
      "     |  The algorithmic complexity of affinity propagation is quadratic\n",
      "     |  in the number of points.\n",
      "     |  \n",
      "     |  When ``fit`` does not converge, ``cluster_centers_`` becomes an empty\n",
      "     |  array and all training samples will be labelled as ``-1``. In addition,\n",
      "     |  ``predict`` will then label every sample as ``-1``.\n",
      "     |  \n",
      "     |  When all training samples have equal similarities and equal preferences,\n",
      "     |  the assignment of cluster centers and labels depends on the preference.\n",
      "     |  If the preference is smaller than the similarities, ``fit`` will result in\n",
      "     |  a single cluster center and label ``0`` for every sample. Otherwise, every\n",
      "     |  training sample becomes its own cluster center and is assigned a unique\n",
      "     |  label.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n",
      "     |  Between Data Points\", Science Feb. 2007\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import AffinityPropagation\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
      "     |  ...               [4, 2], [4, 4], [4, 0]])\n",
      "     |  >>> clustering = AffinityPropagation(random_state=5).fit(X)\n",
      "     |  >>> clustering\n",
      "     |  AffinityPropagation(random_state=5)\n",
      "     |  >>> clustering.labels_\n",
      "     |  array([0, 0, 0, 1, 1, 1])\n",
      "     |  >>> clustering.predict([[0, 0], [4, 4]])\n",
      "     |  array([0, 1])\n",
      "     |  >>> clustering.cluster_centers_\n",
      "     |  array([[1, 2],\n",
      "     |         [4, 2]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AffinityPropagation\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, damping=0.5, max_iter=200, convergence_iter=15, copy=True, preference=None, affinity='euclidean', verbose=False, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit the clustering from features, or affinity matrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)\n",
      "     |          Training instances to cluster, or similarities / affinities between\n",
      "     |          instances if ``affinity='precomputed'``. If a sparse feature matrix\n",
      "     |          is provided, it will be converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None)\n",
      "     |      Fit clustering from features/affinity matrix; return cluster labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)\n",
      "     |          Training instances to cluster, or similarities / affinities between\n",
      "     |          instances if ``affinity='precomputed'``. If a sparse feature matrix\n",
      "     |          is provided, it will be converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,)\n",
      "     |          Cluster labels.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict the closest cluster each sample in X belongs to.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data to predict. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,)\n",
      "     |          Cluster labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class AgglomerativeClustering(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "     |  AgglomerativeClustering(n_clusters=2, *, affinity='euclidean', memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', distance_threshold=None, compute_distances=False)\n",
      "     |  \n",
      "     |  Agglomerative Clustering.\n",
      "     |  \n",
      "     |  Recursively merges pair of clusters of sample data; uses linkage distance.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <hierarchical_clustering>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_clusters : int or None, default=2\n",
      "     |      The number of clusters to find. It must be ``None`` if\n",
      "     |      ``distance_threshold`` is not ``None``.\n",
      "     |  \n",
      "     |  affinity : str or callable, default='euclidean'\n",
      "     |      Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n",
      "     |      \"manhattan\", \"cosine\", or \"precomputed\".\n",
      "     |      If linkage is \"ward\", only \"euclidean\" is accepted.\n",
      "     |      If \"precomputed\", a distance matrix (instead of a similarity matrix)\n",
      "     |      is needed as input for the fit method.\n",
      "     |  \n",
      "     |  memory : str or object with the joblib.Memory interface, default=None\n",
      "     |      Used to cache the output of the computation of the tree.\n",
      "     |      By default, no caching is done. If a string is given, it is the\n",
      "     |      path to the caching directory.\n",
      "     |  \n",
      "     |  connectivity : array-like or callable, default=None\n",
      "     |      Connectivity matrix. Defines for each sample the neighboring\n",
      "     |      samples following a given structure of the data.\n",
      "     |      This can be a connectivity matrix itself or a callable that transforms\n",
      "     |      the data into a connectivity matrix, such as derived from\n",
      "     |      `kneighbors_graph`. Default is ``None``, i.e, the\n",
      "     |      hierarchical clustering algorithm is unstructured.\n",
      "     |  \n",
      "     |  compute_full_tree : 'auto' or bool, default='auto'\n",
      "     |      Stop early the construction of the tree at ``n_clusters``. This is\n",
      "     |      useful to decrease computation time if the number of clusters is not\n",
      "     |      small compared to the number of samples. This option is useful only\n",
      "     |      when specifying a connectivity matrix. Note also that when varying the\n",
      "     |      number of clusters and using caching, it may be advantageous to compute\n",
      "     |      the full tree. It must be ``True`` if ``distance_threshold`` is not\n",
      "     |      ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n",
      "     |      to `True` when `distance_threshold` is not `None` or that `n_clusters`\n",
      "     |      is inferior to the maximum between 100 or `0.02 * n_samples`.\n",
      "     |      Otherwise, \"auto\" is equivalent to `False`.\n",
      "     |  \n",
      "     |  linkage : {'ward', 'complete', 'average', 'single'}, default='ward'\n",
      "     |      Which linkage criterion to use. The linkage criterion determines which\n",
      "     |      distance to use between sets of observation. The algorithm will merge\n",
      "     |      the pairs of cluster that minimize this criterion.\n",
      "     |  \n",
      "     |      - 'ward' minimizes the variance of the clusters being merged.\n",
      "     |      - 'average' uses the average of the distances of each observation of\n",
      "     |        the two sets.\n",
      "     |      - 'complete' or 'maximum' linkage uses the maximum distances between\n",
      "     |        all observations of the two sets.\n",
      "     |      - 'single' uses the minimum of the distances between all observations\n",
      "     |        of the two sets.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added the 'single' option\n",
      "     |  \n",
      "     |  distance_threshold : float, default=None\n",
      "     |      The linkage distance threshold above which, clusters will not be\n",
      "     |      merged. If not ``None``, ``n_clusters`` must be ``None`` and\n",
      "     |      ``compute_full_tree`` must be ``True``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.21\n",
      "     |  \n",
      "     |  compute_distances : bool, default=False\n",
      "     |      Computes distances between clusters even if `distance_threshold` is not\n",
      "     |      used. This can be used to make dendrogram visualization, but introduces\n",
      "     |      a computational and memory overhead.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_clusters_ : int\n",
      "     |      The number of clusters found by the algorithm. If\n",
      "     |      ``distance_threshold=None``, it will be equal to the given\n",
      "     |      ``n_clusters``.\n",
      "     |  \n",
      "     |  labels_ : ndarray of shape (n_samples)\n",
      "     |      Cluster labels for each point.\n",
      "     |  \n",
      "     |  n_leaves_ : int\n",
      "     |      Number of leaves in the hierarchical tree.\n",
      "     |  \n",
      "     |  n_connected_components_ : int\n",
      "     |      The estimated number of connected components in the graph.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.21\n",
      "     |          ``n_connected_components_`` was added to replace ``n_components_``.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  children_ : array-like of shape (n_samples-1, 2)\n",
      "     |      The children of each non-leaf node. Values less than `n_samples`\n",
      "     |      correspond to leaves of the tree which are the original samples.\n",
      "     |      A node `i` greater than or equal to `n_samples` is a non-leaf\n",
      "     |      node and has children `children_[i - n_samples]`. Alternatively\n",
      "     |      at the i-th iteration, children[i][0] and children[i][1]\n",
      "     |      are merged to form node `n_samples + i`.\n",
      "     |  \n",
      "     |  distances_ : array-like of shape (n_nodes-1,)\n",
      "     |      Distances between nodes in the corresponding place in `children_`.\n",
      "     |      Only computed if `distance_threshold` is used or `compute_distances`\n",
      "     |      is set to `True`.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  FeatureAgglomeration : Agglomerative clustering but for features instead of\n",
      "     |      samples.\n",
      "     |  ward_tree : Hierarchical clustering with ward linkage.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import AgglomerativeClustering\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
      "     |  ...               [4, 2], [4, 4], [4, 0]])\n",
      "     |  >>> clustering = AgglomerativeClustering().fit(X)\n",
      "     |  >>> clustering\n",
      "     |  AgglomerativeClustering()\n",
      "     |  >>> clustering.labels_\n",
      "     |  array([1, 1, 1, 0, 0, 0])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AgglomerativeClustering\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_clusters=2, *, affinity='euclidean', memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', distance_threshold=None, compute_distances=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit the hierarchical clustering from features, or distance matrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features) or                 (n_samples, n_samples)\n",
      "     |          Training instances to cluster, or distances between instances if\n",
      "     |          ``affinity='precomputed'``.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the fitted instance.\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None)\n",
      "     |      Fit and return the result of each sample's clustering assignment.\n",
      "     |      \n",
      "     |      In addition to fitting, this method also return the result of the\n",
      "     |      clustering assignment for each sample in the training set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features) or                 (n_samples, n_samples)\n",
      "     |          Training instances to cluster, or distances between instances if\n",
      "     |          ``affinity='precomputed'``.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,)\n",
      "     |          Cluster labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Birch(sklearn.base.ClusterMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  Birch(*, threshold=0.5, branching_factor=50, n_clusters=3, compute_labels=True, copy=True)\n",
      "     |  \n",
      "     |  Implements the BIRCH clustering algorithm.\n",
      "     |  \n",
      "     |  It is a memory-efficient, online-learning algorithm provided as an\n",
      "     |  alternative to :class:`MiniBatchKMeans`. It constructs a tree\n",
      "     |  data structure with the cluster centroids being read off the leaf.\n",
      "     |  These can be either the final cluster centroids or can be provided as input\n",
      "     |  to another clustering algorithm such as :class:`AgglomerativeClustering`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <birch>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.16\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  threshold : float, default=0.5\n",
      "     |      The radius of the subcluster obtained by merging a new sample and the\n",
      "     |      closest subcluster should be lesser than the threshold. Otherwise a new\n",
      "     |      subcluster is started. Setting this value to be very low promotes\n",
      "     |      splitting and vice-versa.\n",
      "     |  \n",
      "     |  branching_factor : int, default=50\n",
      "     |      Maximum number of CF subclusters in each node. If a new samples enters\n",
      "     |      such that the number of subclusters exceed the branching_factor then\n",
      "     |      that node is split into two nodes with the subclusters redistributed\n",
      "     |      in each. The parent subcluster of that node is removed and two new\n",
      "     |      subclusters are added as parents of the 2 split nodes.\n",
      "     |  \n",
      "     |  n_clusters : int, instance of sklearn.cluster model, default=3\n",
      "     |      Number of clusters after the final clustering step, which treats the\n",
      "     |      subclusters from the leaves as new samples.\n",
      "     |  \n",
      "     |      - `None` : the final clustering step is not performed and the\n",
      "     |        subclusters are returned as they are.\n",
      "     |  \n",
      "     |      - :mod:`sklearn.cluster` Estimator : If a model is provided, the model\n",
      "     |        is fit treating the subclusters as new samples and the initial data\n",
      "     |        is mapped to the label of the closest subcluster.\n",
      "     |  \n",
      "     |      - `int` : the model fit is :class:`AgglomerativeClustering` with\n",
      "     |        `n_clusters` set to be equal to the int.\n",
      "     |  \n",
      "     |  compute_labels : bool, default=True\n",
      "     |      Whether or not to compute labels for each fit.\n",
      "     |  \n",
      "     |  copy : bool, default=True\n",
      "     |      Whether or not to make a copy of the given data. If set to False,\n",
      "     |      the initial data will be overwritten.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  root_ : _CFNode\n",
      "     |      Root of the CFTree.\n",
      "     |  \n",
      "     |  dummy_leaf_ : _CFNode\n",
      "     |      Start pointer to all the leaves.\n",
      "     |  \n",
      "     |  subcluster_centers_ : ndarray\n",
      "     |      Centroids of all subclusters read directly from the leaves.\n",
      "     |  \n",
      "     |  subcluster_labels_ : ndarray\n",
      "     |      Labels assigned to the centroids of the subclusters after\n",
      "     |      they are clustered globally.\n",
      "     |  \n",
      "     |  labels_ : ndarray of shape (n_samples,)\n",
      "     |      Array of labels assigned to the input data.\n",
      "     |      if partial_fit is used instead of fit, they are assigned to the\n",
      "     |      last batch of data.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  MiniBatchKMeans : Alternative implementation that does incremental updates\n",
      "     |      of the centers' positions using mini-batches.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The tree data structure consists of nodes with each node consisting of\n",
      "     |  a number of subclusters. The maximum number of subclusters in a node\n",
      "     |  is determined by the branching factor. Each subcluster maintains a\n",
      "     |  linear sum, squared sum and the number of samples in that subcluster.\n",
      "     |  In addition, each subcluster can also have a node as its child, if the\n",
      "     |  subcluster is not a member of a leaf node.\n",
      "     |  \n",
      "     |  For a new point entering the root, it is merged with the subcluster closest\n",
      "     |  to it and the linear sum, squared sum and the number of samples of that\n",
      "     |  subcluster are updated. This is done recursively till the properties of\n",
      "     |  the leaf node are updated.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  * Tian Zhang, Raghu Ramakrishnan, Maron Livny\n",
      "     |    BIRCH: An efficient data clustering method for large databases.\n",
      "     |    https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf\n",
      "     |  \n",
      "     |  * Roberto Perdisci\n",
      "     |    JBirch - Java implementation of BIRCH clustering algorithm\n",
      "     |    https://code.google.com/archive/p/jbirch\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import Birch\n",
      "     |  >>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]\n",
      "     |  >>> brc = Birch(n_clusters=None)\n",
      "     |  >>> brc.fit(X)\n",
      "     |  Birch(n_clusters=None)\n",
      "     |  >>> brc.predict(X)\n",
      "     |  array([0, 0, 0, 1, 1, 1])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Birch\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, threshold=0.5, branching_factor=50, n_clusters=3, compute_labels=True, copy=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Build a CF Tree for the input data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  partial_fit(self, X=None, y=None)\n",
      "     |      Online learning. Prevents rebuilding of CFTree from scratch.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features),             default=None\n",
      "     |          Input data. If X is not provided, only the global clustering\n",
      "     |          step is done.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict data using the ``centroids_`` of subclusters.\n",
      "     |      \n",
      "     |      Avoid computation of the row norms of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape(n_samples,)\n",
      "     |          Labelled data.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform X into subcluster centroids dimension.\n",
      "     |      \n",
      "     |      Each dimension represents the distance from the sample point to each\n",
      "     |      cluster centroid.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_trans : {array-like, sparse matrix} of shape (n_samples, n_clusters)\n",
      "     |          Transformed data.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  fit_\n",
      "     |      DEPRECATED: `fit_` is deprecated in 1.0 and will be removed in 1.2.\n",
      "     |  \n",
      "     |  partial_fit_\n",
      "     |      DEPRECATED: `partial_fit_` is deprecated in 1.0 and will be removed in 1.2.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None)\n",
      "     |      Perform clustering on `X` and returns cluster labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,), dtype=np.int64\n",
      "     |          Cluster labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class DBSCAN(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "     |  DBSCAN(eps=0.5, *, min_samples=5, metric='euclidean', metric_params=None, algorithm='auto', leaf_size=30, p=None, n_jobs=None)\n",
      "     |  \n",
      "     |  Perform DBSCAN clustering from vector array or distance matrix.\n",
      "     |  \n",
      "     |  DBSCAN - Density-Based Spatial Clustering of Applications with Noise.\n",
      "     |  Finds core samples of high density and expands clusters from them.\n",
      "     |  Good for data which contains clusters of similar density.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <dbscan>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  eps : float, default=0.5\n",
      "     |      The maximum distance between two samples for one to be considered\n",
      "     |      as in the neighborhood of the other. This is not a maximum bound\n",
      "     |      on the distances of points within a cluster. This is the most\n",
      "     |      important DBSCAN parameter to choose appropriately for your data set\n",
      "     |      and distance function.\n",
      "     |  \n",
      "     |  min_samples : int, default=5\n",
      "     |      The number of samples (or total weight) in a neighborhood for a point\n",
      "     |      to be considered as a core point. This includes the point itself.\n",
      "     |  \n",
      "     |  metric : str, or callable, default='euclidean'\n",
      "     |      The metric to use when calculating distance between instances in a\n",
      "     |      feature array. If metric is a string or callable, it must be one of\n",
      "     |      the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n",
      "     |      its metric parameter.\n",
      "     |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "     |      must be square. X may be a :term:`Glossary <sparse graph>`, in which\n",
      "     |      case only \"nonzero\" elements may be considered neighbors for DBSCAN.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         metric *precomputed* to accept precomputed sparse matrix.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      The algorithm to be used by the NearestNeighbors module\n",
      "     |      to compute pointwise distances and find nearest neighbors.\n",
      "     |      See NearestNeighbors module documentation for details.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to BallTree or cKDTree. This can affect the speed\n",
      "     |      of the construction and query, as well as the memory required\n",
      "     |      to store the tree. The optimal value depends\n",
      "     |      on the nature of the problem.\n",
      "     |  \n",
      "     |  p : float, default=None\n",
      "     |      The power of the Minkowski metric to be used to calculate distance\n",
      "     |      between points. If None, then ``p=2`` (equivalent to the Euclidean\n",
      "     |      distance).\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of parallel jobs to run.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  core_sample_indices_ : ndarray of shape (n_core_samples,)\n",
      "     |      Indices of core samples.\n",
      "     |  \n",
      "     |  components_ : ndarray of shape (n_core_samples, n_features)\n",
      "     |      Copy of each core sample found by training.\n",
      "     |  \n",
      "     |  labels_ : ndarray of shape (n_samples)\n",
      "     |      Cluster labels for each point in the dataset given to fit().\n",
      "     |      Noisy samples are given the label -1.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  OPTICS : A similar clustering at multiple values of eps. Our implementation\n",
      "     |      is optimized for memory usage.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For an example, see :ref:`examples/cluster/plot_dbscan.py\n",
      "     |  <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n",
      "     |  \n",
      "     |  This implementation bulk-computes all neighborhood queries, which increases\n",
      "     |  the memory complexity to O(n.d) where d is the average number of neighbors,\n",
      "     |  while original DBSCAN had memory complexity O(n). It may attract a higher\n",
      "     |  memory complexity when querying these nearest neighborhoods, depending\n",
      "     |  on the ``algorithm``.\n",
      "     |  \n",
      "     |  One way to avoid the query complexity is to pre-compute sparse\n",
      "     |  neighborhoods in chunks using\n",
      "     |  :func:`NearestNeighbors.radius_neighbors_graph\n",
      "     |  <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n",
      "     |  ``mode='distance'``, then using ``metric='precomputed'`` here.\n",
      "     |  \n",
      "     |  Another way to reduce memory and computation time is to remove\n",
      "     |  (near-)duplicate points and use ``sample_weight`` instead.\n",
      "     |  \n",
      "     |  :class:`cluster.OPTICS` provides a similar clustering with lower memory\n",
      "     |  usage.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Ester, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\n",
      "     |  Algorithm for Discovering Clusters in Large Spatial Databases with Noise\".\n",
      "     |  In: Proceedings of the 2nd International Conference on Knowledge Discovery\n",
      "     |  and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n",
      "     |  \n",
      "     |  Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n",
      "     |  DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\n",
      "     |  ACM Transactions on Database Systems (TODS), 42(3), 19.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import DBSCAN\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 2], [2, 2], [2, 3],\n",
      "     |  ...               [8, 7], [8, 8], [25, 80]])\n",
      "     |  >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n",
      "     |  >>> clustering.labels_\n",
      "     |  array([ 0,  0,  0,  1,  1, -1])\n",
      "     |  >>> clustering\n",
      "     |  DBSCAN(eps=3, min_samples=2)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DBSCAN\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, eps=0.5, *, min_samples=5, metric='euclidean', metric_params=None, algorithm='auto', leaf_size=30, p=None, n_jobs=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Perform DBSCAN clustering from features, or distance matrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)\n",
      "     |          Training instances to cluster, or distances between instances if\n",
      "     |          ``metric='precomputed'``. If a sparse matrix is provided, it will\n",
      "     |          be converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weight of each sample, such that a sample with a weight of at least\n",
      "     |          ``min_samples`` is by itself a core sample; a sample with a\n",
      "     |          negative weight may inhibit its eps-neighbor from being core.\n",
      "     |          Note that weights are absolute, and default to 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns a fitted instance of self.\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None, sample_weight=None)\n",
      "     |      Compute clusters from a data or distance matrix and predict labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)\n",
      "     |          Training instances to cluster, or distances between instances if\n",
      "     |          ``metric='precomputed'``. If a sparse matrix is provided, it will\n",
      "     |          be converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weight of each sample, such that a sample with a weight of at least\n",
      "     |          ``min_samples`` is by itself a core sample; a sample with a\n",
      "     |          negative weight may inhibit its eps-neighbor from being core.\n",
      "     |          Note that weights are absolute, and default to 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,)\n",
      "     |          Cluster labels. Noisy samples are given the label -1.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class FeatureAgglomeration(AgglomerativeClustering, sklearn.cluster._feature_agglomeration.AgglomerationTransform)\n",
      "     |  FeatureAgglomeration(n_clusters=2, *, affinity='euclidean', memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', pooling_func=<function mean at 0x0000025675D54CA0>, distance_threshold=None, compute_distances=False)\n",
      "     |  \n",
      "     |  Agglomerate features.\n",
      "     |  \n",
      "     |  Recursively merges pair of clusters of features.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <hierarchical_clustering>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_clusters : int, default=2\n",
      "     |      The number of clusters to find. It must be ``None`` if\n",
      "     |      ``distance_threshold`` is not ``None``.\n",
      "     |  \n",
      "     |  affinity : str or callable, default='euclidean'\n",
      "     |      Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n",
      "     |      \"manhattan\", \"cosine\", or 'precomputed'.\n",
      "     |      If linkage is \"ward\", only \"euclidean\" is accepted.\n",
      "     |  \n",
      "     |  memory : str or object with the joblib.Memory interface, default=None\n",
      "     |      Used to cache the output of the computation of the tree.\n",
      "     |      By default, no caching is done. If a string is given, it is the\n",
      "     |      path to the caching directory.\n",
      "     |  \n",
      "     |  connectivity : array-like or callable, default=None\n",
      "     |      Connectivity matrix. Defines for each feature the neighboring\n",
      "     |      features following a given structure of the data.\n",
      "     |      This can be a connectivity matrix itself or a callable that transforms\n",
      "     |      the data into a connectivity matrix, such as derived from\n",
      "     |      `kneighbors_graph`. Default is `None`, i.e, the\n",
      "     |      hierarchical clustering algorithm is unstructured.\n",
      "     |  \n",
      "     |  compute_full_tree : 'auto' or bool, default='auto'\n",
      "     |      Stop early the construction of the tree at `n_clusters`. This is useful\n",
      "     |      to decrease computation time if the number of clusters is not small\n",
      "     |      compared to the number of features. This option is useful only when\n",
      "     |      specifying a connectivity matrix. Note also that when varying the\n",
      "     |      number of clusters and using caching, it may be advantageous to compute\n",
      "     |      the full tree. It must be ``True`` if ``distance_threshold`` is not\n",
      "     |      ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n",
      "     |      to `True` when `distance_threshold` is not `None` or that `n_clusters`\n",
      "     |      is inferior to the maximum between 100 or `0.02 * n_samples`.\n",
      "     |      Otherwise, \"auto\" is equivalent to `False`.\n",
      "     |  \n",
      "     |  linkage : {\"ward\", \"complete\", \"average\", \"single\"}, default=\"ward\"\n",
      "     |      Which linkage criterion to use. The linkage criterion determines which\n",
      "     |      distance to use between sets of features. The algorithm will merge\n",
      "     |      the pairs of cluster that minimize this criterion.\n",
      "     |  \n",
      "     |      - \"ward\" minimizes the variance of the clusters being merged.\n",
      "     |      - \"complete\" or maximum linkage uses the maximum distances between\n",
      "     |        all features of the two sets.\n",
      "     |      - \"average\" uses the average of the distances of each feature of\n",
      "     |        the two sets.\n",
      "     |      - \"single\" uses the minimum of the distances between all features\n",
      "     |        of the two sets.\n",
      "     |  \n",
      "     |  pooling_func : callable, default=np.mean\n",
      "     |      This combines the values of agglomerated features into a single\n",
      "     |      value, and should accept an array of shape [M, N] and the keyword\n",
      "     |      argument `axis=1`, and reduce it to an array of size [M].\n",
      "     |  \n",
      "     |  distance_threshold : float, default=None\n",
      "     |      The linkage distance threshold above which, clusters will not be\n",
      "     |      merged. If not ``None``, ``n_clusters`` must be ``None`` and\n",
      "     |      ``compute_full_tree`` must be ``True``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.21\n",
      "     |  \n",
      "     |  compute_distances : bool, default=False\n",
      "     |      Computes distances between clusters even if `distance_threshold` is not\n",
      "     |      used. This can be used to make dendrogram visualization, but introduces\n",
      "     |      a computational and memory overhead.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_clusters_ : int\n",
      "     |      The number of clusters found by the algorithm. If\n",
      "     |      ``distance_threshold=None``, it will be equal to the given\n",
      "     |      ``n_clusters``.\n",
      "     |  \n",
      "     |  labels_ : array-like of (n_features,)\n",
      "     |      Cluster labels for each feature.\n",
      "     |  \n",
      "     |  n_leaves_ : int\n",
      "     |      Number of leaves in the hierarchical tree.\n",
      "     |  \n",
      "     |  n_connected_components_ : int\n",
      "     |      The estimated number of connected components in the graph.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.21\n",
      "     |          ``n_connected_components_`` was added to replace ``n_components_``.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  children_ : array-like of shape (n_nodes-1, 2)\n",
      "     |      The children of each non-leaf node. Values less than `n_features`\n",
      "     |      correspond to leaves of the tree which are the original samples.\n",
      "     |      A node `i` greater than or equal to `n_features` is a non-leaf\n",
      "     |      node and has children `children_[i - n_features]`. Alternatively\n",
      "     |      at the i-th iteration, children[i][0] and children[i][1]\n",
      "     |      are merged to form node `n_features + i`.\n",
      "     |  \n",
      "     |  distances_ : array-like of shape (n_nodes-1,)\n",
      "     |      Distances between nodes in the corresponding place in `children_`.\n",
      "     |      Only computed if `distance_threshold` is used or `compute_distances`\n",
      "     |      is set to `True`.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  AgglomerativeClustering : Agglomerative clustering samples instead of\n",
      "     |      features.\n",
      "     |  ward_tree : Hierarchical clustering with ward linkage.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn import datasets, cluster\n",
      "     |  >>> digits = datasets.load_digits()\n",
      "     |  >>> images = digits.images\n",
      "     |  >>> X = np.reshape(images, (len(images), -1))\n",
      "     |  >>> agglo = cluster.FeatureAgglomeration(n_clusters=32)\n",
      "     |  >>> agglo.fit(X)\n",
      "     |  FeatureAgglomeration(n_clusters=32)\n",
      "     |  >>> X_reduced = agglo.transform(X)\n",
      "     |  >>> X_reduced.shape\n",
      "     |  (1797, 32)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      FeatureAgglomeration\n",
      "     |      AgglomerativeClustering\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.cluster._feature_agglomeration.AgglomerationTransform\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_clusters=2, *, affinity='euclidean', memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', pooling_func=<function mean at 0x0000025675D54CA0>, distance_threshold=None, compute_distances=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit the hierarchical clustering on the data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the transformer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  fit_predict\n",
      "     |      Fit and return the result of each sample's clustering assignment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.cluster._feature_agglomeration.AgglomerationTransform:\n",
      "     |  \n",
      "     |  inverse_transform(self, Xred)\n",
      "     |      Inverse the transformation and return a vector of size `n_features`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      Xred : array-like of shape (n_samples, n_clusters) or (n_clusters,)\n",
      "     |          The values to be assigned to each cluster of samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : ndarray of shape (n_samples, n_features) or (n_features,)\n",
      "     |          A vector of size `n_samples` with the values of `Xred` assigned to\n",
      "     |          each of the cluster of samples.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform a new matrix using the built clustering.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features) or                 (n_samples, n_samples)\n",
      "     |          A M by N array of M observations in N dimensions or a length\n",
      "     |          M array of M one-dimensional observations.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Y : ndarray of shape (n_samples, n_clusters) or (n_clusters,)\n",
      "     |          The pooled values for each feature cluster.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "    \n",
      "    class KMeans(sklearn.base.TransformerMixin, sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "     |  KMeans(n_clusters=8, *, init='k-means++', n_init=10, max_iter=300, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='auto')\n",
      "     |  \n",
      "     |  K-Means clustering.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <k_means>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  n_clusters : int, default=8\n",
      "     |      The number of clusters to form as well as the number of\n",
      "     |      centroids to generate.\n",
      "     |  \n",
      "     |  init : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n",
      "     |      Method for initialization:\n",
      "     |  \n",
      "     |      'k-means++' : selects initial cluster centers for k-mean\n",
      "     |      clustering in a smart way to speed up convergence. See section\n",
      "     |      Notes in k_init for more details.\n",
      "     |  \n",
      "     |      'random': choose `n_clusters` observations (rows) at random from data\n",
      "     |      for the initial centroids.\n",
      "     |  \n",
      "     |      If an array is passed, it should be of shape (n_clusters, n_features)\n",
      "     |      and gives the initial centers.\n",
      "     |  \n",
      "     |      If a callable is passed, it should take arguments X, n_clusters and a\n",
      "     |      random state and return an initialization.\n",
      "     |  \n",
      "     |  n_init : int, default=10\n",
      "     |      Number of time the k-means algorithm will be run with different\n",
      "     |      centroid seeds. The final results will be the best output of\n",
      "     |      n_init consecutive runs in terms of inertia.\n",
      "     |  \n",
      "     |  max_iter : int, default=300\n",
      "     |      Maximum number of iterations of the k-means algorithm for a\n",
      "     |      single run.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Relative tolerance with regards to Frobenius norm of the difference\n",
      "     |      in the cluster centers of two consecutive iterations to declare\n",
      "     |      convergence.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Verbosity mode.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Determines random number generation for centroid initialization. Use\n",
      "     |      an int to make the randomness deterministic.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  copy_x : bool, default=True\n",
      "     |      When pre-computing distances it is more numerically accurate to center\n",
      "     |      the data first. If copy_x is True (default), then the original data is\n",
      "     |      not modified. If False, the original data is modified, and put back\n",
      "     |      before the function returns, but small numerical differences may be\n",
      "     |      introduced by subtracting and then adding the data mean. Note that if\n",
      "     |      the original data is not C-contiguous, a copy will be made even if\n",
      "     |      copy_x is False. If the original data is sparse, but not in CSR format,\n",
      "     |      a copy will be made even if copy_x is False.\n",
      "     |  \n",
      "     |  algorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n",
      "     |      K-means algorithm to use. The classical EM-style algorithm is \"full\".\n",
      "     |      The \"elkan\" variation is more efficient on data with well-defined\n",
      "     |      clusters, by using the triangle inequality. However it's more memory\n",
      "     |      intensive due to the allocation of an extra array of shape\n",
      "     |      (n_samples, n_clusters).\n",
      "     |  \n",
      "     |      For now \"auto\" (kept for backward compatibility) chooses \"elkan\" but it\n",
      "     |      might change in the future for a better heuristic.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |          Added Elkan algorithm\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cluster_centers_ : ndarray of shape (n_clusters, n_features)\n",
      "     |      Coordinates of cluster centers. If the algorithm stops before fully\n",
      "     |      converging (see ``tol`` and ``max_iter``), these will not be\n",
      "     |      consistent with ``labels_``.\n",
      "     |  \n",
      "     |  labels_ : ndarray of shape (n_samples,)\n",
      "     |      Labels of each point\n",
      "     |  \n",
      "     |  inertia_ : float\n",
      "     |      Sum of squared distances of samples to their closest cluster center,\n",
      "     |      weighted by the sample weights if provided.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  MiniBatchKMeans : Alternative online implementation that does incremental\n",
      "     |      updates of the centers positions using mini-batches.\n",
      "     |      For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n",
      "     |      probably much faster than the default batch implementation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The k-means problem is solved using either Lloyd's or Elkan's algorithm.\n",
      "     |  \n",
      "     |  The average complexity is given by O(k n T), where n is the number of\n",
      "     |  samples and T is the number of iteration.\n",
      "     |  \n",
      "     |  The worst case complexity is given by O(n^(k+2/p)) with\n",
      "     |  n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n",
      "     |  'How slow is the k-means method?' SoCG2006)\n",
      "     |  \n",
      "     |  In practice, the k-means algorithm is very fast (one of the fastest\n",
      "     |  clustering algorithms available), but it falls in local minima. That's why\n",
      "     |  it can be useful to restart it several times.\n",
      "     |  \n",
      "     |  If the algorithm stops before fully converging (because of ``tol`` or\n",
      "     |  ``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\n",
      "     |  i.e. the ``cluster_centers_`` will not be the means of the points in each\n",
      "     |  cluster. Also, the estimator will reassign ``labels_`` after the last\n",
      "     |  iteration to make ``labels_`` consistent with ``predict`` on the training\n",
      "     |  set.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  \n",
      "     |  >>> from sklearn.cluster import KMeans\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
      "     |  ...               [10, 2], [10, 4], [10, 0]])\n",
      "     |  >>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
      "     |  >>> kmeans.labels_\n",
      "     |  array([1, 1, 1, 0, 0, 0], dtype=int32)\n",
      "     |  >>> kmeans.predict([[0, 0], [12, 3]])\n",
      "     |  array([1, 0], dtype=int32)\n",
      "     |  >>> kmeans.cluster_centers_\n",
      "     |  array([[10.,  2.],\n",
      "     |         [ 1.,  2.]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KMeans\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_clusters=8, *, init='k-means++', n_init=10, max_iter=300, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Compute k-means clustering.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training instances to cluster. It must be noted that the data\n",
      "     |          will be converted to C ordering, which will cause a memory\n",
      "     |          copy if the given data is not C-contiguous.\n",
      "     |          If a sparse matrix is passed, a copy will be made if it's not in\n",
      "     |          CSR format.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.20\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None, sample_weight=None)\n",
      "     |      Compute cluster centers and predict cluster index for each sample.\n",
      "     |      \n",
      "     |      Convenience method; equivalent to calling fit(X) followed by\n",
      "     |      predict(X).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data to transform.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,)\n",
      "     |          Index of the cluster each sample belongs to.\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, sample_weight=None)\n",
      "     |      Compute clustering and transform X to cluster-distance space.\n",
      "     |      \n",
      "     |      Equivalent to fit(X).transform(X), but more efficiently implemented.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data to transform.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray of shape (n_samples, n_clusters)\n",
      "     |          X transformed in the new space.\n",
      "     |  \n",
      "     |  predict(self, X, sample_weight=None)\n",
      "     |      Predict the closest cluster each sample in X belongs to.\n",
      "     |      \n",
      "     |      In the vector quantization literature, `cluster_centers_` is called\n",
      "     |      the code book and each value returned by `predict` is the index of\n",
      "     |      the closest code in the code book.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data to predict.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,)\n",
      "     |          Index of the cluster each sample belongs to.\n",
      "     |  \n",
      "     |  score(self, X, y=None, sample_weight=None)\n",
      "     |      Opposite of the value of X on the K-means objective.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Opposite of the value of X on the K-means objective.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform X to a cluster-distance space.\n",
      "     |      \n",
      "     |      In the new space, each dimension is the distance to the cluster\n",
      "     |      centers. Note that even if X is sparse, the array returned by\n",
      "     |      `transform` will typically be dense.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data to transform.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray of shape (n_samples, n_clusters)\n",
      "     |          X transformed in the new space.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MeanShift(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "     |  MeanShift(*, bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300)\n",
      "     |  \n",
      "     |  Mean shift clustering using a flat kernel.\n",
      "     |  \n",
      "     |  Mean shift clustering aims to discover \"blobs\" in a smooth density of\n",
      "     |  samples. It is a centroid-based algorithm, which works by updating\n",
      "     |  candidates for centroids to be the mean of the points within a given\n",
      "     |  region. These candidates are then filtered in a post-processing stage to\n",
      "     |  eliminate near-duplicates to form the final set of centroids.\n",
      "     |  \n",
      "     |  Seeding is performed using a binning technique for scalability.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <mean_shift>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  bandwidth : float, default=None\n",
      "     |      Bandwidth used in the RBF kernel.\n",
      "     |  \n",
      "     |      If not given, the bandwidth is estimated using\n",
      "     |      sklearn.cluster.estimate_bandwidth; see the documentation for that\n",
      "     |      function for hints on scalability (see also the Notes, below).\n",
      "     |  \n",
      "     |  seeds : array-like of shape (n_samples, n_features), default=None\n",
      "     |      Seeds used to initialize kernels. If not set,\n",
      "     |      the seeds are calculated by clustering.get_bin_seeds\n",
      "     |      with bandwidth as the grid size and default values for\n",
      "     |      other parameters.\n",
      "     |  \n",
      "     |  bin_seeding : bool, default=False\n",
      "     |      If true, initial kernel locations are not locations of all\n",
      "     |      points, but rather the location of the discretized version of\n",
      "     |      points, where points are binned onto a grid whose coarseness\n",
      "     |      corresponds to the bandwidth. Setting this option to True will speed\n",
      "     |      up the algorithm because fewer seeds will be initialized.\n",
      "     |      The default value is False.\n",
      "     |      Ignored if seeds argument is not None.\n",
      "     |  \n",
      "     |  min_bin_freq : int, default=1\n",
      "     |     To speed up the algorithm, accept only those bins with at least\n",
      "     |     min_bin_freq points as seeds.\n",
      "     |  \n",
      "     |  cluster_all : bool, default=True\n",
      "     |      If true, then all points are clustered, even those orphans that are\n",
      "     |      not within any kernel. Orphans are assigned to the nearest kernel.\n",
      "     |      If false, then orphans are given cluster label -1.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to use for the computation. This works by computing\n",
      "     |      each of the n_init runs in parallel.\n",
      "     |  \n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  max_iter : int, default=300\n",
      "     |      Maximum number of iterations, per seed point before the clustering\n",
      "     |      operation terminates (for that seed point), if has not converged yet.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cluster_centers_ : ndarray of shape (n_clusters, n_features)\n",
      "     |      Coordinates of cluster centers.\n",
      "     |  \n",
      "     |  labels_ : ndarray of shape (n_samples,)\n",
      "     |      Labels of each point.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Maximum number of iterations performed on each seed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  KMeans : K-Means clustering.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  \n",
      "     |  Scalability:\n",
      "     |  \n",
      "     |  Because this implementation uses a flat kernel and\n",
      "     |  a Ball Tree to look up members of each kernel, the complexity will tend\n",
      "     |  towards O(T*n*log(n)) in lower dimensions, with n the number of samples\n",
      "     |  and T the number of points. In higher dimensions the complexity will\n",
      "     |  tend towards O(T*n^2).\n",
      "     |  \n",
      "     |  Scalability can be boosted by using fewer seeds, for example by using\n",
      "     |  a higher value of min_bin_freq in the get_bin_seeds function.\n",
      "     |  \n",
      "     |  Note that the estimate_bandwidth function is much less scalable than the\n",
      "     |  mean shift algorithm and will be the bottleneck if it is used.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  Dorin Comaniciu and Peter Meer, \"Mean Shift: A robust approach toward\n",
      "     |  feature space analysis\". IEEE Transactions on Pattern Analysis and\n",
      "     |  Machine Intelligence. 2002. pp. 603-619.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import MeanShift\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 1], [2, 1], [1, 0],\n",
      "     |  ...               [4, 7], [3, 5], [3, 6]])\n",
      "     |  >>> clustering = MeanShift(bandwidth=2).fit(X)\n",
      "     |  >>> clustering.labels_\n",
      "     |  array([1, 1, 1, 0, 0, 0])\n",
      "     |  >>> clustering.predict([[0, 0], [5, 5]])\n",
      "     |  array([1, 0])\n",
      "     |  >>> clustering\n",
      "     |  MeanShift(bandwidth=2)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MeanShift\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Perform clustering.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Samples to cluster.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |             Fitted instance.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict the closest cluster each sample in X belongs to.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          New data to predict.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,)\n",
      "     |          Index of the cluster each sample belongs to.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None)\n",
      "     |      Perform clustering on `X` and returns cluster labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,), dtype=np.int64\n",
      "     |          Cluster labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MiniBatchKMeans(KMeans)\n",
      "     |  MiniBatchKMeans(n_clusters=8, *, init='k-means++', max_iter=100, batch_size=1024, verbose=0, compute_labels=True, random_state=None, tol=0.0, max_no_improvement=10, init_size=None, n_init=3, reassignment_ratio=0.01)\n",
      "     |  \n",
      "     |  Mini-Batch K-Means clustering.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <mini_batch_kmeans>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  n_clusters : int, default=8\n",
      "     |      The number of clusters to form as well as the number of\n",
      "     |      centroids to generate.\n",
      "     |  \n",
      "     |  init : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n",
      "     |      Method for initialization:\n",
      "     |  \n",
      "     |      'k-means++' : selects initial cluster centers for k-mean\n",
      "     |      clustering in a smart way to speed up convergence. See section\n",
      "     |      Notes in k_init for more details.\n",
      "     |  \n",
      "     |      'random': choose `n_clusters` observations (rows) at random from data\n",
      "     |      for the initial centroids.\n",
      "     |  \n",
      "     |      If an array is passed, it should be of shape (n_clusters, n_features)\n",
      "     |      and gives the initial centers.\n",
      "     |  \n",
      "     |      If a callable is passed, it should take arguments X, n_clusters and a\n",
      "     |      random state and return an initialization.\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      Maximum number of iterations over the complete dataset before\n",
      "     |      stopping independently of any early stopping criterion heuristics.\n",
      "     |  \n",
      "     |  batch_size : int, default=1024\n",
      "     |      Size of the mini batches.\n",
      "     |      For faster compuations, you can set the ``batch_size`` greater than\n",
      "     |      256 * number of cores to enable parallelism on all cores.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.0\n",
      "     |         `batch_size` default changed from 100 to 1024.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Verbosity mode.\n",
      "     |  \n",
      "     |  compute_labels : bool, default=True\n",
      "     |      Compute label assignment and inertia for the complete dataset\n",
      "     |      once the minibatch optimization has converged in fit.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Determines random number generation for centroid initialization and\n",
      "     |      random reassignment. Use an int to make the randomness deterministic.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  tol : float, default=0.0\n",
      "     |      Control early stopping based on the relative center changes as\n",
      "     |      measured by a smoothed, variance-normalized of the mean center\n",
      "     |      squared position changes. This early stopping heuristics is\n",
      "     |      closer to the one used for the batch variant of the algorithms\n",
      "     |      but induces a slight computational and memory overhead over the\n",
      "     |      inertia heuristic.\n",
      "     |  \n",
      "     |      To disable convergence detection based on normalized center\n",
      "     |      change, set tol to 0.0 (default).\n",
      "     |  \n",
      "     |  max_no_improvement : int, default=10\n",
      "     |      Control early stopping based on the consecutive number of mini\n",
      "     |      batches that does not yield an improvement on the smoothed inertia.\n",
      "     |  \n",
      "     |      To disable convergence detection based on inertia, set\n",
      "     |      max_no_improvement to None.\n",
      "     |  \n",
      "     |  init_size : int, default=None\n",
      "     |      Number of samples to randomly sample for speeding up the\n",
      "     |      initialization (sometimes at the expense of accuracy): the\n",
      "     |      only algorithm is initialized by running a batch KMeans on a\n",
      "     |      random subset of the data. This needs to be larger than n_clusters.\n",
      "     |  \n",
      "     |      If `None`, the heuristic is `init_size = 3 * batch_size` if\n",
      "     |      `3 * batch_size < n_clusters`, else `init_size = 3 * n_clusters`.\n",
      "     |  \n",
      "     |  n_init : int, default=3\n",
      "     |      Number of random initializations that are tried.\n",
      "     |      In contrast to KMeans, the algorithm is only run once, using the\n",
      "     |      best of the ``n_init`` initializations as measured by inertia.\n",
      "     |  \n",
      "     |  reassignment_ratio : float, default=0.01\n",
      "     |      Control the fraction of the maximum number of counts for a center to\n",
      "     |      be reassigned. A higher value means that low count centers are more\n",
      "     |      easily reassigned, which means that the model will take longer to\n",
      "     |      converge, but should converge in a better clustering. However, too high\n",
      "     |      a value may cause convergence issues, especially with a small batch\n",
      "     |      size.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  cluster_centers_ : ndarray of shape (n_clusters, n_features)\n",
      "     |      Coordinates of cluster centers.\n",
      "     |  \n",
      "     |  labels_ : ndarray of shape (n_samples,)\n",
      "     |      Labels of each point (if compute_labels is set to True).\n",
      "     |  \n",
      "     |  inertia_ : float\n",
      "     |      The value of the inertia criterion associated with the chosen\n",
      "     |      partition if compute_labels is set to True. If compute_labels is set to\n",
      "     |      False, it's an approximation of the inertia based on an exponentially\n",
      "     |      weighted average of the batch inertiae.\n",
      "     |      The inertia is defined as the sum of square distances of samples to\n",
      "     |      their cluster center, weighted by the sample weights if provided.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations over the full dataset.\n",
      "     |  \n",
      "     |  n_steps_ : int\n",
      "     |      Number of minibatches processed.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  counts_ : ndarray of shape (n_clusters,)\n",
      "     |      Weight sum of each cluster.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.24\n",
      "     |         This attribute is deprecated in 0.24 and will be removed in\n",
      "     |         1.1 (renaming of 0.26).\n",
      "     |  \n",
      "     |  init_size_ : int\n",
      "     |      The effective number of samples used for the initialization.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.24\n",
      "     |         This attribute is deprecated in 0.24 and will be removed in\n",
      "     |         1.1 (renaming of 0.26).\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  KMeans : The classic implementation of the clustering method based on the\n",
      "     |      Lloyd's algorithm. It consumes the whole set of input data at each\n",
      "     |      iteration.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import MiniBatchKMeans\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
      "     |  ...               [4, 2], [4, 0], [4, 4],\n",
      "     |  ...               [4, 5], [0, 1], [2, 2],\n",
      "     |  ...               [3, 2], [5, 5], [1, -1]])\n",
      "     |  >>> # manually fit on batches\n",
      "     |  >>> kmeans = MiniBatchKMeans(n_clusters=2,\n",
      "     |  ...                          random_state=0,\n",
      "     |  ...                          batch_size=6)\n",
      "     |  >>> kmeans = kmeans.partial_fit(X[0:6,:])\n",
      "     |  >>> kmeans = kmeans.partial_fit(X[6:12,:])\n",
      "     |  >>> kmeans.cluster_centers_\n",
      "     |  array([[2. , 1. ],\n",
      "     |         [3.5, 4.5]])\n",
      "     |  >>> kmeans.predict([[0, 0], [4, 4]])\n",
      "     |  array([0, 1], dtype=int32)\n",
      "     |  >>> # fit on the whole data\n",
      "     |  >>> kmeans = MiniBatchKMeans(n_clusters=2,\n",
      "     |  ...                          random_state=0,\n",
      "     |  ...                          batch_size=6,\n",
      "     |  ...                          max_iter=10).fit(X)\n",
      "     |  >>> kmeans.cluster_centers_\n",
      "     |  array([[1.19..., 1.22...],\n",
      "     |         [4.03..., 2.46...]])\n",
      "     |  >>> kmeans.predict([[0, 0], [4, 4]])\n",
      "     |  array([0, 1], dtype=int32)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MiniBatchKMeans\n",
      "     |      KMeans\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_clusters=8, *, init='k-means++', max_iter=100, batch_size=1024, verbose=0, compute_labels=True, random_state=None, tol=0.0, max_no_improvement=10, init_size=None, n_init=3, reassignment_ratio=0.01)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Compute the centroids on X by chunking it into mini-batches.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training instances to cluster. It must be noted that the data\n",
      "     |          will be converted to C ordering, which will cause a memory copy\n",
      "     |          if the given data is not C-contiguous.\n",
      "     |          If a sparse matrix is passed, a copy will be made if it's not in\n",
      "     |          CSR format.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.20\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y=None, sample_weight=None)\n",
      "     |      Update k means estimate on a single mini-batch X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training instances to cluster. It must be noted that the data\n",
      "     |          will be converted to C ordering, which will cause a memory copy\n",
      "     |          if the given data is not C-contiguous.\n",
      "     |          If a sparse matrix is passed, a copy will be made if it's not in\n",
      "     |          CSR format.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Return updated estimator.\n",
      "     |  \n",
      "     |  predict(self, X, sample_weight=None)\n",
      "     |      Predict the closest cluster each sample in X belongs to.\n",
      "     |      \n",
      "     |      In the vector quantization literature, `cluster_centers_` is called\n",
      "     |      the code book and each value returned by `predict` is the index of\n",
      "     |      the closest code in the code book.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data to predict.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,)\n",
      "     |          Index of the cluster each sample belongs to.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  counts_\n",
      "     |      DEPRECATED: The attribute `counts_` is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "     |  \n",
      "     |  init_size_\n",
      "     |      DEPRECATED: The attribute `init_size_` is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "     |  \n",
      "     |  random_state_\n",
      "     |      DEPRECATED: The attribute `random_state_` is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from KMeans:\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None, sample_weight=None)\n",
      "     |      Compute cluster centers and predict cluster index for each sample.\n",
      "     |      \n",
      "     |      Convenience method; equivalent to calling fit(X) followed by\n",
      "     |      predict(X).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data to transform.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,)\n",
      "     |          Index of the cluster each sample belongs to.\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, sample_weight=None)\n",
      "     |      Compute clustering and transform X to cluster-distance space.\n",
      "     |      \n",
      "     |      Equivalent to fit(X).transform(X), but more efficiently implemented.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data to transform.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray of shape (n_samples, n_clusters)\n",
      "     |          X transformed in the new space.\n",
      "     |  \n",
      "     |  score(self, X, y=None, sample_weight=None)\n",
      "     |      Opposite of the value of X on the K-means objective.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Opposite of the value of X on the K-means objective.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform X to a cluster-distance space.\n",
      "     |      \n",
      "     |      In the new space, each dimension is the distance to the cluster\n",
      "     |      centers. Note that even if X is sparse, the array returned by\n",
      "     |      `transform` will typically be dense.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data to transform.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray of shape (n_samples, n_clusters)\n",
      "     |          X transformed in the new space.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class OPTICS(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "     |  OPTICS(*, min_samples=5, max_eps=inf, metric='minkowski', p=2, metric_params=None, cluster_method='xi', eps=None, xi=0.05, predecessor_correction=True, min_cluster_size=None, algorithm='auto', leaf_size=30, memory=None, n_jobs=None)\n",
      "     |  \n",
      "     |  Estimate clustering structure from vector array.\n",
      "     |  \n",
      "     |  OPTICS (Ordering Points To Identify the Clustering Structure), closely\n",
      "     |  related to DBSCAN, finds core sample of high density and expands clusters\n",
      "     |  from them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable\n",
      "     |  neighborhood radius. Better suited for usage on large datasets than the\n",
      "     |  current sklearn implementation of DBSCAN.\n",
      "     |  \n",
      "     |  Clusters are then extracted using a DBSCAN-like method\n",
      "     |  (cluster_method = 'dbscan') or an automatic\n",
      "     |  technique proposed in [1]_ (cluster_method = 'xi').\n",
      "     |  \n",
      "     |  This implementation deviates from the original OPTICS by first performing\n",
      "     |  k-nearest-neighborhood searches on all points to identify core sizes, then\n",
      "     |  computing only the distances to unprocessed points when constructing the\n",
      "     |  cluster order. Note that we do not employ a heap to manage the expansion\n",
      "     |  candidates, so the time complexity will be O(n^2).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <optics>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  min_samples : int > 1 or float between 0 and 1, default=5\n",
      "     |      The number of samples in a neighborhood for a point to be considered as\n",
      "     |      a core point. Also, up and down steep regions can't have more than\n",
      "     |      ``min_samples`` consecutive non-steep points. Expressed as an absolute\n",
      "     |      number or a fraction of the number of samples (rounded to be at least\n",
      "     |      2).\n",
      "     |  \n",
      "     |  max_eps : float, default=np.inf\n",
      "     |      The maximum distance between two samples for one to be considered as\n",
      "     |      in the neighborhood of the other. Default value of ``np.inf`` will\n",
      "     |      identify clusters across all scales; reducing ``max_eps`` will result\n",
      "     |      in shorter run times.\n",
      "     |  \n",
      "     |  metric : str or callable, default='minkowski'\n",
      "     |      Metric to use for distance computation. Any metric from scikit-learn\n",
      "     |      or scipy.spatial.distance can be used.\n",
      "     |  \n",
      "     |      If metric is a callable function, it is called on each\n",
      "     |      pair of instances (rows) and the resulting value recorded. The callable\n",
      "     |      should take two arrays as input and return one value indicating the\n",
      "     |      distance between them. This works for Scipy's metrics, but is less\n",
      "     |      efficient than passing the metric name as a string. If metric is\n",
      "     |      \"precomputed\", X is assumed to be a distance matrix and must be square.\n",
      "     |  \n",
      "     |      Valid values for metric are:\n",
      "     |  \n",
      "     |      - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "     |        'manhattan']\n",
      "     |  \n",
      "     |      - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "     |        'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "     |        'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "     |        'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "     |        'yule']\n",
      "     |  \n",
      "     |      See the documentation for scipy.spatial.distance for details on these\n",
      "     |      metrics.\n",
      "     |  \n",
      "     |  p : int, default=2\n",
      "     |      Parameter for the Minkowski metric from\n",
      "     |      :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\n",
      "     |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  cluster_method : str, default='xi'\n",
      "     |      The extraction method used to extract clusters using the calculated\n",
      "     |      reachability and ordering. Possible values are \"xi\" and \"dbscan\".\n",
      "     |  \n",
      "     |  eps : float, default=None\n",
      "     |      The maximum distance between two samples for one to be considered as\n",
      "     |      in the neighborhood of the other. By default it assumes the same value\n",
      "     |      as ``max_eps``.\n",
      "     |      Used only when ``cluster_method='dbscan'``.\n",
      "     |  \n",
      "     |  xi : float between 0 and 1, default=0.05\n",
      "     |      Determines the minimum steepness on the reachability plot that\n",
      "     |      constitutes a cluster boundary. For example, an upwards point in the\n",
      "     |      reachability plot is defined by the ratio from one point to its\n",
      "     |      successor being at most 1-xi.\n",
      "     |      Used only when ``cluster_method='xi'``.\n",
      "     |  \n",
      "     |  predecessor_correction : bool, default=True\n",
      "     |      Correct clusters according to the predecessors calculated by OPTICS\n",
      "     |      [2]_. This parameter has minimal effect on most datasets.\n",
      "     |      Used only when ``cluster_method='xi'``.\n",
      "     |  \n",
      "     |  min_cluster_size : int > 1 or float between 0 and 1, default=None\n",
      "     |      Minimum number of samples in an OPTICS cluster, expressed as an\n",
      "     |      absolute number or a fraction of the number of samples (rounded to be\n",
      "     |      at least 2). If ``None``, the value of ``min_samples`` is used instead.\n",
      "     |      Used only when ``cluster_method='xi'``.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method. (default)\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n",
      "     |      affect the speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree. The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  memory : str or object with the joblib.Memory interface, default=None\n",
      "     |      Used to cache the output of the computation of the tree.\n",
      "     |      By default, no caching is done. If a string is given, it is the\n",
      "     |      path to the caching directory.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  labels_ : ndarray of shape (n_samples,)\n",
      "     |      Cluster labels for each point in the dataset given to fit().\n",
      "     |      Noisy samples and points which are not included in a leaf cluster\n",
      "     |      of ``cluster_hierarchy_`` are labeled as -1.\n",
      "     |  \n",
      "     |  reachability_ : ndarray of shape (n_samples,)\n",
      "     |      Reachability distances per sample, indexed by object order. Use\n",
      "     |      ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n",
      "     |  \n",
      "     |  ordering_ : ndarray of shape (n_samples,)\n",
      "     |      The cluster ordered list of sample indices.\n",
      "     |  \n",
      "     |  core_distances_ : ndarray of shape (n_samples,)\n",
      "     |      Distance at which each sample becomes a core point, indexed by object\n",
      "     |      order. Points which will never be core have a distance of inf. Use\n",
      "     |      ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n",
      "     |  \n",
      "     |  predecessor_ : ndarray of shape (n_samples,)\n",
      "     |      Point that a sample was reached from, indexed by object order.\n",
      "     |      Seed points have a predecessor of -1.\n",
      "     |  \n",
      "     |  cluster_hierarchy_ : ndarray of shape (n_clusters, 2)\n",
      "     |      The list of clusters in the form of ``[start, end]`` in each row, with\n",
      "     |      all indices inclusive. The clusters are ordered according to\n",
      "     |      ``(end, -start)`` (ascending) so that larger clusters encompassing\n",
      "     |      smaller clusters come after those smaller ones. Since ``labels_`` does\n",
      "     |      not reflect the hierarchy, usually\n",
      "     |      ``len(cluster_hierarchy_) > np.unique(optics.labels_)``. Please also\n",
      "     |      note that these indices are of the ``ordering_``, i.e.\n",
      "     |      ``X[ordering_][start:end + 1]`` form a cluster.\n",
      "     |      Only available when ``cluster_method='xi'``.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  DBSCAN : A similar clustering for a specified neighborhood radius (eps).\n",
      "     |      Our implementation is optimized for runtime.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n",
      "     |     and Jörg Sander. \"OPTICS: ordering points to identify the clustering\n",
      "     |     structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n",
      "     |  \n",
      "     |  .. [2] Schubert, Erich, Michael Gertz.\n",
      "     |     \"Improving the Cluster Structure Extracted from OPTICS Plots.\" Proc. of\n",
      "     |     the Conference \"Lernen, Wissen, Daten, Analysen\" (LWDA) (2018): 318-329.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import OPTICS\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 2], [2, 5], [3, 6],\n",
      "     |  ...               [8, 7], [8, 8], [7, 3]])\n",
      "     |  >>> clustering = OPTICS(min_samples=2).fit(X)\n",
      "     |  >>> clustering.labels_\n",
      "     |  array([0, 0, 0, 1, 1, 1])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OPTICS\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, min_samples=5, max_eps=inf, metric='minkowski', p=2, metric_params=None, cluster_method='xi', eps=None, xi=0.05, predecessor_correction=True, min_cluster_size=None, algorithm='auto', leaf_size=30, memory=None, n_jobs=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Perform OPTICS clustering.\n",
      "     |      \n",
      "     |      Extracts an ordered list of points and reachability distances, and\n",
      "     |      performs initial clustering using ``max_eps`` distance specified at\n",
      "     |      OPTICS object instantiation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features), or                 (n_samples, n_samples) if metric=’precomputed’\n",
      "     |          A feature array, or array of distances between samples if\n",
      "     |          metric='precomputed'.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns a fitted instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None)\n",
      "     |      Perform clustering on `X` and returns cluster labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,), dtype=np.int64\n",
      "     |          Cluster labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SpectralBiclustering(BaseSpectral)\n",
      "     |  SpectralBiclustering(n_clusters=3, *, method='bistochastic', n_components=6, n_best=3, svd_method='randomized', n_svd_vecs=None, mini_batch=False, init='k-means++', n_init=10, random_state=None)\n",
      "     |  \n",
      "     |  Spectral biclustering (Kluger, 2003).\n",
      "     |  \n",
      "     |  Partitions rows and columns under the assumption that the data has\n",
      "     |  an underlying checkerboard structure. For instance, if there are\n",
      "     |  two row partitions and three column partitions, each row will\n",
      "     |  belong to three biclusters, and each column will belong to two\n",
      "     |  biclusters. The outer product of the corresponding row and column\n",
      "     |  label vectors gives this checkerboard structure.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <spectral_biclustering>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_clusters : int or tuple (n_row_clusters, n_column_clusters), default=3\n",
      "     |      The number of row and column clusters in the checkerboard\n",
      "     |      structure.\n",
      "     |  \n",
      "     |  method : {'bistochastic', 'scale', 'log'}, default='bistochastic'\n",
      "     |      Method of normalizing and converting singular vectors into\n",
      "     |      biclusters. May be one of 'scale', 'bistochastic', or 'log'.\n",
      "     |      The authors recommend using 'log'. If the data is sparse,\n",
      "     |      however, log normalization will not work, which is why the\n",
      "     |      default is 'bistochastic'.\n",
      "     |  \n",
      "     |      .. warning::\n",
      "     |         if `method='log'`, the data must be sparse.\n",
      "     |  \n",
      "     |  n_components : int, default=6\n",
      "     |      Number of singular vectors to check.\n",
      "     |  \n",
      "     |  n_best : int, default=3\n",
      "     |      Number of best singular vectors to which to project the data\n",
      "     |      for clustering.\n",
      "     |  \n",
      "     |  svd_method : {'randomized', 'arpack'}, default='randomized'\n",
      "     |      Selects the algorithm for finding singular vectors. May be\n",
      "     |      'randomized' or 'arpack'. If 'randomized', uses\n",
      "     |      :func:`~sklearn.utils.extmath.randomized_svd`, which may be faster\n",
      "     |      for large matrices. If 'arpack', uses\n",
      "     |      `scipy.sparse.linalg.svds`, which is more accurate, but\n",
      "     |      possibly slower in some cases.\n",
      "     |  \n",
      "     |  n_svd_vecs : int, default=None\n",
      "     |      Number of vectors to use in calculating the SVD. Corresponds\n",
      "     |      to `ncv` when `svd_method=arpack` and `n_oversamples` when\n",
      "     |      `svd_method` is 'randomized`.\n",
      "     |  \n",
      "     |  mini_batch : bool, default=False\n",
      "     |      Whether to use mini-batch k-means, which is faster but may get\n",
      "     |      different results.\n",
      "     |  \n",
      "     |  init : {'k-means++', 'random'} or ndarray of (n_clusters, n_features),             default='k-means++'\n",
      "     |      Method for initialization of k-means algorithm; defaults to\n",
      "     |      'k-means++'.\n",
      "     |  \n",
      "     |  n_init : int, default=10\n",
      "     |      Number of random initializations that are tried with the\n",
      "     |      k-means algorithm.\n",
      "     |  \n",
      "     |      If mini-batch k-means is used, the best initialization is\n",
      "     |      chosen and the algorithm runs once. Otherwise, the algorithm\n",
      "     |      is run for each initialization and the best solution chosen.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used for randomizing the singular value decomposition and the k-means\n",
      "     |      initialization. Use an int to make the randomness deterministic.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  rows_ : array-like of shape (n_row_clusters, n_rows)\n",
      "     |      Results of the clustering. `rows[i, r]` is True if\n",
      "     |      cluster `i` contains row `r`. Available only after calling ``fit``.\n",
      "     |  \n",
      "     |  columns_ : array-like of shape (n_column_clusters, n_columns)\n",
      "     |      Results of the clustering, like `rows`.\n",
      "     |  \n",
      "     |  row_labels_ : array-like of shape (n_rows,)\n",
      "     |      Row partition labels.\n",
      "     |  \n",
      "     |  column_labels_ : array-like of shape (n_cols,)\n",
      "     |      Column partition labels.\n",
      "     |  \n",
      "     |  biclusters_ : tuple of two ndarrays\n",
      "     |      The tuple contains the `rows_` and `columns_` arrays.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  SpectralCoclustering : Spectral Co-Clustering algorithm (Dhillon, 2001).\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  * Kluger, Yuval, et. al., 2003. `Spectral biclustering of microarray\n",
      "     |    data: coclustering genes and conditions\n",
      "     |    <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608>`__.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import SpectralBiclustering\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 1], [2, 1], [1, 0],\n",
      "     |  ...               [4, 7], [3, 5], [3, 6]])\n",
      "     |  >>> clustering = SpectralBiclustering(n_clusters=2, random_state=0).fit(X)\n",
      "     |  >>> clustering.row_labels_\n",
      "     |  array([1, 1, 1, 0, 0, 0], dtype=int32)\n",
      "     |  >>> clustering.column_labels_\n",
      "     |  array([0, 1], dtype=int32)\n",
      "     |  >>> clustering\n",
      "     |  SpectralBiclustering(n_clusters=2, random_state=0)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpectralBiclustering\n",
      "     |      BaseSpectral\n",
      "     |      sklearn.base.BiclusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_clusters=3, *, method='bistochastic', n_components=6, n_best=3, svd_method='randomized', n_svd_vecs=None, mini_batch=False, init='k-means++', n_init=10, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSpectral:\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Create a biclustering for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          SpectralBiclustering instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BiclusterMixin:\n",
      "     |  \n",
      "     |  get_indices(self, i)\n",
      "     |      Row and column indices of the `i`'th bicluster.\n",
      "     |      \n",
      "     |      Only works if ``rows_`` and ``columns_`` attributes exist.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      i : int\n",
      "     |          The index of the cluster.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      row_ind : ndarray, dtype=np.intp\n",
      "     |          Indices of rows in the dataset that belong to the bicluster.\n",
      "     |      col_ind : ndarray, dtype=np.intp\n",
      "     |          Indices of columns in the dataset that belong to the bicluster.\n",
      "     |  \n",
      "     |  get_shape(self, i)\n",
      "     |      Shape of the `i`'th bicluster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      i : int\n",
      "     |          The index of the cluster.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_rows : int\n",
      "     |          Number of rows in the bicluster.\n",
      "     |      \n",
      "     |      n_cols : int\n",
      "     |          Number of columns in the bicluster.\n",
      "     |  \n",
      "     |  get_submatrix(self, i, data)\n",
      "     |      Return the submatrix corresponding to bicluster `i`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      i : int\n",
      "     |          The index of the cluster.\n",
      "     |      data : array-like of shape (n_samples, n_features)\n",
      "     |          The data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      submatrix : ndarray of shape (n_rows, n_cols)\n",
      "     |          The submatrix corresponding to bicluster `i`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Works with sparse matrices. Only works if ``rows_`` and\n",
      "     |      ``columns_`` attributes exist.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from sklearn.base.BiclusterMixin:\n",
      "     |  \n",
      "     |  biclusters_\n",
      "     |      Convenient way to get row and column indicators together.\n",
      "     |      \n",
      "     |      Returns the ``rows_`` and ``columns_`` members.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BiclusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SpectralClustering(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "     |  SpectralClustering(n_clusters=8, *, eigen_solver=None, n_components=None, random_state=None, n_init=10, gamma=1.0, affinity='rbf', n_neighbors=10, eigen_tol=0.0, assign_labels='kmeans', degree=3, coef0=1, kernel_params=None, n_jobs=None, verbose=False)\n",
      "     |  \n",
      "     |  Apply clustering to a projection of the normalized Laplacian.\n",
      "     |  \n",
      "     |  In practice Spectral Clustering is very useful when the structure of\n",
      "     |  the individual clusters is highly non-convex, or more generally when\n",
      "     |  a measure of the center and spread of the cluster is not a suitable\n",
      "     |  description of the complete cluster, such as when clusters are\n",
      "     |  nested circles on the 2D plane.\n",
      "     |  \n",
      "     |  If the affinity matrix is the adjacency matrix of a graph, this method\n",
      "     |  can be used to find normalized graph cuts [1]_, [2]_.\n",
      "     |  \n",
      "     |  When calling ``fit``, an affinity matrix is constructed using either\n",
      "     |  a kernel function such the Gaussian (aka RBF) kernel with Euclidean\n",
      "     |  distance ``d(X, X)``::\n",
      "     |  \n",
      "     |          np.exp(-gamma * d(X,X) ** 2)\n",
      "     |  \n",
      "     |  or a k-nearest neighbors connectivity matrix.\n",
      "     |  \n",
      "     |  Alternatively, a user-provided affinity matrix can be specified by\n",
      "     |  setting ``affinity='precomputed'``.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <spectral_clustering>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_clusters : int, default=8\n",
      "     |      The dimension of the projection subspace.\n",
      "     |  \n",
      "     |  eigen_solver : {'arpack', 'lobpcg', 'amg'}, default=None\n",
      "     |      The eigenvalue decomposition strategy to use. AMG requires pyamg\n",
      "     |      to be installed. It can be faster on very large, sparse problems,\n",
      "     |      but may also lead to instabilities. If None, then ``'arpack'`` is\n",
      "     |      used. See [4]_ for more details regarding `'lobpcg'`.\n",
      "     |  \n",
      "     |  n_components : int, default=n_clusters\n",
      "     |      Number of eigenvectors to use for the spectral embedding.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      A pseudo random number generator used for the initialization\n",
      "     |      of the lobpcg eigenvectors decomposition when `eigen_solver ==\n",
      "     |      'amg'`, and for the K-Means initialization. Use an int to make\n",
      "     |      the results deterministic across calls (See\n",
      "     |      :term:`Glossary <random_state>`).\n",
      "     |  \n",
      "     |      .. note::\n",
      "     |          When using `eigen_solver == 'amg'`,\n",
      "     |          it is necessary to also fix the global numpy seed with\n",
      "     |          `np.random.seed(int)` to get deterministic results. See\n",
      "     |          https://github.com/pyamg/pyamg/issues/139 for further\n",
      "     |          information.\n",
      "     |  \n",
      "     |  n_init : int, default=10\n",
      "     |      Number of time the k-means algorithm will be run with different\n",
      "     |      centroid seeds. The final results will be the best output of n_init\n",
      "     |      consecutive runs in terms of inertia. Only used if\n",
      "     |      ``assign_labels='kmeans'``.\n",
      "     |  \n",
      "     |  gamma : float, default=1.0\n",
      "     |      Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.\n",
      "     |      Ignored for ``affinity='nearest_neighbors'``.\n",
      "     |  \n",
      "     |  affinity : str or callable, default='rbf'\n",
      "     |      How to construct the affinity matrix.\n",
      "     |       - 'nearest_neighbors': construct the affinity matrix by computing a\n",
      "     |         graph of nearest neighbors.\n",
      "     |       - 'rbf': construct the affinity matrix using a radial basis function\n",
      "     |         (RBF) kernel.\n",
      "     |       - 'precomputed': interpret ``X`` as a precomputed affinity matrix,\n",
      "     |         where larger values indicate greater similarity between instances.\n",
      "     |       - 'precomputed_nearest_neighbors': interpret ``X`` as a sparse graph\n",
      "     |         of precomputed distances, and construct a binary affinity matrix\n",
      "     |         from the ``n_neighbors`` nearest neighbors of each instance.\n",
      "     |       - one of the kernels supported by\n",
      "     |         :func:`~sklearn.metrics.pairwise_kernels`.\n",
      "     |  \n",
      "     |      Only kernels that produce similarity scores (non-negative values that\n",
      "     |      increase with similarity) should be used. This property is not checked\n",
      "     |      by the clustering algorithm.\n",
      "     |  \n",
      "     |  n_neighbors : int, default=10\n",
      "     |      Number of neighbors to use when constructing the affinity matrix using\n",
      "     |      the nearest neighbors method. Ignored for ``affinity='rbf'``.\n",
      "     |  \n",
      "     |  eigen_tol : float, default=0.0\n",
      "     |      Stopping criterion for eigendecomposition of the Laplacian matrix\n",
      "     |      when ``eigen_solver='arpack'``.\n",
      "     |  \n",
      "     |  assign_labels : {'kmeans', 'discretize'}, default='kmeans'\n",
      "     |      The strategy for assigning labels in the embedding space. There are two\n",
      "     |      ways to assign labels after the Laplacian embedding. k-means is a\n",
      "     |      popular choice, but it can be sensitive to initialization.\n",
      "     |      Discretization is another approach which is less sensitive to random\n",
      "     |      initialization [3]_.\n",
      "     |  \n",
      "     |  degree : float, default=3\n",
      "     |      Degree of the polynomial kernel. Ignored by other kernels.\n",
      "     |  \n",
      "     |  coef0 : float, default=1\n",
      "     |      Zero coefficient for polynomial and sigmoid kernels.\n",
      "     |      Ignored by other kernels.\n",
      "     |  \n",
      "     |  kernel_params : dict of str to any, default=None\n",
      "     |      Parameters (keyword arguments) and values for kernel passed as\n",
      "     |      callable object. Ignored by other kernels.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of parallel jobs to run when `affinity='nearest_neighbors'`\n",
      "     |      or `affinity='precomputed_nearest_neighbors'`. The neighbors search\n",
      "     |      will be done in parallel.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Verbosity mode.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  affinity_matrix_ : array-like of shape (n_samples, n_samples)\n",
      "     |      Affinity matrix used for clustering. Available only after calling\n",
      "     |      ``fit``.\n",
      "     |  \n",
      "     |  labels_ : ndarray of shape (n_samples,)\n",
      "     |      Labels of each point\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.cluster.KMeans : K-Means clustering.\n",
      "     |  sklearn.cluster.DBSCAN : Density-Based Spatial Clustering of\n",
      "     |      Applications with Noise.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  A distance matrix for which 0 indicates identical elements and high values\n",
      "     |  indicate very dissimilar elements can be transformed into an affinity /\n",
      "     |  similarity matrix that is well-suited for the algorithm by\n",
      "     |  applying the Gaussian (aka RBF, heat) kernel::\n",
      "     |  \n",
      "     |      np.exp(- dist_matrix ** 2 / (2. * delta ** 2))\n",
      "     |  \n",
      "     |  where ``delta`` is a free parameter representing the width of the Gaussian\n",
      "     |  kernel.\n",
      "     |  \n",
      "     |  An alternative is to take a symmetric version of the k-nearest neighbors\n",
      "     |  connectivity matrix of the points.\n",
      "     |  \n",
      "     |  If the pyamg package is installed, it is used: this greatly\n",
      "     |  speeds up computation.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] `Normalized cuts and image segmentation, 2000\n",
      "     |         Jianbo Shi, Jitendra Malik\n",
      "     |         <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324>`_\n",
      "     |  \n",
      "     |  .. [2] `A Tutorial on Spectral Clustering, 2007\n",
      "     |         Ulrike von Luxburg\n",
      "     |         <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323>`_\n",
      "     |  \n",
      "     |  .. [3] `Multiclass spectral clustering, 2003\n",
      "     |         Stella X. Yu, Jianbo Shi\n",
      "     |         <https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf>`_\n",
      "     |  \n",
      "     |  .. [4] `Toward the Optimal Preconditioned Eigensolver:\n",
      "     |         Locally Optimal Block Preconditioned Conjugate Gradient Method, 2001.\n",
      "     |         A. V. Knyazev\n",
      "     |         SIAM Journal on Scientific Computing 23, no. 2, pp. 517-541.\n",
      "     |         <https://epubs.siam.org/doi/pdf/10.1137/S1064827500366124>`_\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import SpectralClustering\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 1], [2, 1], [1, 0],\n",
      "     |  ...               [4, 7], [3, 5], [3, 6]])\n",
      "     |  >>> clustering = SpectralClustering(n_clusters=2,\n",
      "     |  ...         assign_labels='discretize',\n",
      "     |  ...         random_state=0).fit(X)\n",
      "     |  >>> clustering.labels_\n",
      "     |  array([1, 1, 1, 0, 0, 0])\n",
      "     |  >>> clustering\n",
      "     |  SpectralClustering(assign_labels='discretize', n_clusters=2,\n",
      "     |      random_state=0)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpectralClustering\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_clusters=8, *, eigen_solver=None, n_components=None, random_state=None, n_init=10, gamma=1.0, affinity='rbf', n_neighbors=10, eigen_tol=0.0, assign_labels='kmeans', degree=3, coef0=1, kernel_params=None, n_jobs=None, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Perform spectral clustering from features, or affinity matrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples)\n",
      "     |          Training instances to cluster, similarities / affinities between\n",
      "     |          instances if ``affinity='precomputed'``, or distances between\n",
      "     |          instances if ``affinity='precomputed_nearest_neighbors``. If a\n",
      "     |          sparse matrix is provided in a format other than ``csr_matrix``,\n",
      "     |          ``csc_matrix``, or ``coo_matrix``, it will be converted into a\n",
      "     |          sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          A fitted instance of the estimator.\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None)\n",
      "     |      Perform spectral clustering on `X` and return cluster labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples)\n",
      "     |          Training instances to cluster, similarities / affinities between\n",
      "     |          instances if ``affinity='precomputed'``, or distances between\n",
      "     |          instances if ``affinity='precomputed_nearest_neighbors``. If a\n",
      "     |          sparse matrix is provided in a format other than ``csr_matrix``,\n",
      "     |          ``csc_matrix``, or ``coo_matrix``, it will be converted into a\n",
      "     |          sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,)\n",
      "     |          Cluster labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SpectralCoclustering(BaseSpectral)\n",
      "     |  SpectralCoclustering(n_clusters=3, *, svd_method='randomized', n_svd_vecs=None, mini_batch=False, init='k-means++', n_init=10, random_state=None)\n",
      "     |  \n",
      "     |  Spectral Co-Clustering algorithm (Dhillon, 2001).\n",
      "     |  \n",
      "     |  Clusters rows and columns of an array `X` to solve the relaxed\n",
      "     |  normalized cut of the bipartite graph created from `X` as follows:\n",
      "     |  the edge between row vertex `i` and column vertex `j` has weight\n",
      "     |  `X[i, j]`.\n",
      "     |  \n",
      "     |  The resulting bicluster structure is block-diagonal, since each\n",
      "     |  row and each column belongs to exactly one bicluster.\n",
      "     |  \n",
      "     |  Supports sparse matrices, as long as they are nonnegative.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <spectral_coclustering>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_clusters : int, default=3\n",
      "     |      The number of biclusters to find.\n",
      "     |  \n",
      "     |  svd_method : {'randomized', 'arpack'}, default='randomized'\n",
      "     |      Selects the algorithm for finding singular vectors. May be\n",
      "     |      'randomized' or 'arpack'. If 'randomized', use\n",
      "     |      :func:`sklearn.utils.extmath.randomized_svd`, which may be faster\n",
      "     |      for large matrices. If 'arpack', use\n",
      "     |      :func:`scipy.sparse.linalg.svds`, which is more accurate, but\n",
      "     |      possibly slower in some cases.\n",
      "     |  \n",
      "     |  n_svd_vecs : int, default=None\n",
      "     |      Number of vectors to use in calculating the SVD. Corresponds\n",
      "     |      to `ncv` when `svd_method=arpack` and `n_oversamples` when\n",
      "     |      `svd_method` is 'randomized`.\n",
      "     |  \n",
      "     |  mini_batch : bool, default=False\n",
      "     |      Whether to use mini-batch k-means, which is faster but may get\n",
      "     |      different results.\n",
      "     |  \n",
      "     |  init : {'k-means++', 'random', or ndarray of shape             (n_clusters, n_features), default='k-means++'\n",
      "     |      Method for initialization of k-means algorithm; defaults to\n",
      "     |      'k-means++'.\n",
      "     |  \n",
      "     |  n_init : int, default=10\n",
      "     |      Number of random initializations that are tried with the\n",
      "     |      k-means algorithm.\n",
      "     |  \n",
      "     |      If mini-batch k-means is used, the best initialization is\n",
      "     |      chosen and the algorithm runs once. Otherwise, the algorithm\n",
      "     |      is run for each initialization and the best solution chosen.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used for randomizing the singular value decomposition and the k-means\n",
      "     |      initialization. Use an int to make the randomness deterministic.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  rows_ : array-like of shape (n_row_clusters, n_rows)\n",
      "     |      Results of the clustering. `rows[i, r]` is True if\n",
      "     |      cluster `i` contains row `r`. Available only after calling ``fit``.\n",
      "     |  \n",
      "     |  columns_ : array-like of shape (n_column_clusters, n_columns)\n",
      "     |      Results of the clustering, like `rows`.\n",
      "     |  \n",
      "     |  row_labels_ : array-like of shape (n_rows,)\n",
      "     |      The bicluster label of each row.\n",
      "     |  \n",
      "     |  column_labels_ : array-like of shape (n_cols,)\n",
      "     |      The bicluster label of each column.\n",
      "     |  \n",
      "     |  biclusters_ : tuple of two ndarrays\n",
      "     |      The tuple contains the `rows_` and `columns_` arrays.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  SpectralBiclustering : Partitions rows and columns under the assumption\n",
      "     |      that the data has an underlying checkerboard structure.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  * Dhillon, Inderjit S, 2001. `Co-clustering documents and words using\n",
      "     |    bipartite spectral graph partitioning\n",
      "     |    <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.3011>`__.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import SpectralCoclustering\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 1], [2, 1], [1, 0],\n",
      "     |  ...               [4, 7], [3, 5], [3, 6]])\n",
      "     |  >>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)\n",
      "     |  >>> clustering.row_labels_ #doctest: +SKIP\n",
      "     |  array([0, 1, 1, 0, 0, 0], dtype=int32)\n",
      "     |  >>> clustering.column_labels_ #doctest: +SKIP\n",
      "     |  array([0, 0], dtype=int32)\n",
      "     |  >>> clustering\n",
      "     |  SpectralCoclustering(n_clusters=2, random_state=0)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpectralCoclustering\n",
      "     |      BaseSpectral\n",
      "     |      sklearn.base.BiclusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_clusters=3, *, svd_method='randomized', n_svd_vecs=None, mini_batch=False, init='k-means++', n_init=10, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSpectral:\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Create a biclustering for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          SpectralBiclustering instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BiclusterMixin:\n",
      "     |  \n",
      "     |  get_indices(self, i)\n",
      "     |      Row and column indices of the `i`'th bicluster.\n",
      "     |      \n",
      "     |      Only works if ``rows_`` and ``columns_`` attributes exist.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      i : int\n",
      "     |          The index of the cluster.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      row_ind : ndarray, dtype=np.intp\n",
      "     |          Indices of rows in the dataset that belong to the bicluster.\n",
      "     |      col_ind : ndarray, dtype=np.intp\n",
      "     |          Indices of columns in the dataset that belong to the bicluster.\n",
      "     |  \n",
      "     |  get_shape(self, i)\n",
      "     |      Shape of the `i`'th bicluster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      i : int\n",
      "     |          The index of the cluster.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_rows : int\n",
      "     |          Number of rows in the bicluster.\n",
      "     |      \n",
      "     |      n_cols : int\n",
      "     |          Number of columns in the bicluster.\n",
      "     |  \n",
      "     |  get_submatrix(self, i, data)\n",
      "     |      Return the submatrix corresponding to bicluster `i`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      i : int\n",
      "     |          The index of the cluster.\n",
      "     |      data : array-like of shape (n_samples, n_features)\n",
      "     |          The data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      submatrix : ndarray of shape (n_rows, n_cols)\n",
      "     |          The submatrix corresponding to bicluster `i`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Works with sparse matrices. Only works if ``rows_`` and\n",
      "     |      ``columns_`` attributes exist.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from sklearn.base.BiclusterMixin:\n",
      "     |  \n",
      "     |  biclusters_\n",
      "     |      Convenient way to get row and column indicators together.\n",
      "     |      \n",
      "     |      Returns the ``rows_`` and ``columns_`` members.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BiclusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "\n",
      "FUNCTIONS\n",
      "    affinity_propagation(S, *, preference=None, convergence_iter=15, max_iter=200, damping=0.5, copy=True, verbose=False, return_n_iter=False, random_state=None)\n",
      "        Perform Affinity Propagation Clustering of data.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <affinity_propagation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        \n",
      "        S : array-like of shape (n_samples, n_samples)\n",
      "            Matrix of similarities between points.\n",
      "        \n",
      "        preference : array-like of shape (n_samples,) or float, default=None\n",
      "            Preferences for each point - points with larger values of\n",
      "            preferences are more likely to be chosen as exemplars. The number of\n",
      "            exemplars, i.e. of clusters, is influenced by the input preferences\n",
      "            value. If the preferences are not passed as arguments, they will be\n",
      "            set to the median of the input similarities (resulting in a moderate\n",
      "            number of clusters). For a smaller amount of clusters, this can be set\n",
      "            to the minimum value of the similarities.\n",
      "        \n",
      "        convergence_iter : int, default=15\n",
      "            Number of iterations with no change in the number\n",
      "            of estimated clusters that stops the convergence.\n",
      "        \n",
      "        max_iter : int, default=200\n",
      "            Maximum number of iterations.\n",
      "        \n",
      "        damping : float, default=0.5\n",
      "            Damping factor between 0.5 and 1.\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            If copy is False, the affinity matrix is modified inplace by the\n",
      "            algorithm, for memory efficiency.\n",
      "        \n",
      "        verbose : bool, default=False\n",
      "            The verbosity level.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether or not to return the number of iterations.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Pseudo-random number generator to control the starting state.\n",
      "            Use an int for reproducible results across function calls.\n",
      "            See the :term:`Glossary <random_state>`.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "                this parameter was previously hardcoded as 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        \n",
      "        cluster_centers_indices : ndarray of shape (n_clusters,)\n",
      "            Index of clusters centers.\n",
      "        \n",
      "        labels : ndarray of shape (n_samples,)\n",
      "            Cluster labels for each point.\n",
      "        \n",
      "        n_iter : int\n",
      "            Number of iterations run. Returned only if `return_n_iter` is\n",
      "            set to True.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n",
      "        <sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n",
      "        \n",
      "        When the algorithm does not converge, it returns an empty array as\n",
      "        ``cluster_center_indices`` and ``-1`` as label for each training sample.\n",
      "        \n",
      "        When all training samples have equal similarities and equal preferences,\n",
      "        the assignment of cluster centers and labels depends on the preference.\n",
      "        If the preference is smaller than the similarities, a single cluster center\n",
      "        and label ``0`` for every sample will be returned. Otherwise, every\n",
      "        training sample becomes its own cluster center and is assigned a unique\n",
      "        label.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n",
      "        Between Data Points\", Science Feb. 2007\n",
      "    \n",
      "    cluster_optics_dbscan(*, reachability, core_distances, ordering, eps)\n",
      "        Perform DBSCAN extraction for an arbitrary epsilon.\n",
      "        \n",
      "        Extracting the clusters runs in linear time. Note that this results in\n",
      "        ``labels_`` which are close to a :class:`~sklearn.cluster.DBSCAN` with\n",
      "        similar settings and ``eps``, only if ``eps`` is close to ``max_eps``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        reachability : array of shape (n_samples,)\n",
      "            Reachability distances calculated by OPTICS (``reachability_``).\n",
      "        \n",
      "        core_distances : array of shape (n_samples,)\n",
      "            Distances at which points become core (``core_distances_``).\n",
      "        \n",
      "        ordering : array of shape (n_samples,)\n",
      "            OPTICS ordered point indices (``ordering_``).\n",
      "        \n",
      "        eps : float\n",
      "            DBSCAN ``eps`` parameter. Must be set to < ``max_eps``. Results\n",
      "            will be close to DBSCAN algorithm if ``eps`` and ``max_eps`` are close\n",
      "            to one another.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        labels_ : array of shape (n_samples,)\n",
      "            The estimated labels.\n",
      "    \n",
      "    cluster_optics_xi(*, reachability, predecessor, ordering, min_samples, min_cluster_size=None, xi=0.05, predecessor_correction=True)\n",
      "        Automatically extract clusters according to the Xi-steep method.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        reachability : ndarray of shape (n_samples,)\n",
      "            Reachability distances calculated by OPTICS (`reachability_`)\n",
      "        \n",
      "        predecessor : ndarray of shape (n_samples,)\n",
      "            Predecessors calculated by OPTICS.\n",
      "        \n",
      "        ordering : ndarray of shape (n_samples,)\n",
      "            OPTICS ordered point indices (`ordering_`)\n",
      "        \n",
      "        min_samples : int > 1 or float between 0 and 1\n",
      "            The same as the min_samples given to OPTICS. Up and down steep regions\n",
      "            can't have more then ``min_samples`` consecutive non-steep points.\n",
      "            Expressed as an absolute number or a fraction of the number of samples\n",
      "            (rounded to be at least 2).\n",
      "        \n",
      "        min_cluster_size : int > 1 or float between 0 and 1, default=None\n",
      "            Minimum number of samples in an OPTICS cluster, expressed as an\n",
      "            absolute number or a fraction of the number of samples (rounded to be\n",
      "            at least 2). If ``None``, the value of ``min_samples`` is used instead.\n",
      "        \n",
      "        xi : float between 0 and 1, default=0.05\n",
      "            Determines the minimum steepness on the reachability plot that\n",
      "            constitutes a cluster boundary. For example, an upwards point in the\n",
      "            reachability plot is defined by the ratio from one point to its\n",
      "            successor being at most 1-xi.\n",
      "        \n",
      "        predecessor_correction : bool, default=True\n",
      "            Correct clusters based on the calculated predecessors.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        labels : ndarray of shape (n_samples,)\n",
      "            The labels assigned to samples. Points which are not included\n",
      "            in any cluster are labeled as -1.\n",
      "        \n",
      "        clusters : ndarray of shape (n_clusters, 2)\n",
      "            The list of clusters in the form of ``[start, end]`` in each row, with\n",
      "            all indices inclusive. The clusters are ordered according to ``(end,\n",
      "            -start)`` (ascending) so that larger clusters encompassing smaller\n",
      "            clusters come after such nested smaller clusters. Since ``labels`` does\n",
      "            not reflect the hierarchy, usually ``len(clusters) >\n",
      "            np.unique(labels)``.\n",
      "    \n",
      "    compute_optics_graph(X, *, min_samples, max_eps, metric, p, metric_params, algorithm, leaf_size, n_jobs)\n",
      "        Compute the OPTICS reachability graph.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <optics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples, n_features), or             (n_samples, n_samples) if metric=’precomputed’.\n",
      "            A feature array, or array of distances between samples if\n",
      "            metric='precomputed'\n",
      "        \n",
      "        min_samples : int > 1 or float between 0 and 1\n",
      "            The number of samples in a neighborhood for a point to be considered\n",
      "            as a core point. Expressed as an absolute number or a fraction of the\n",
      "            number of samples (rounded to be at least 2).\n",
      "        \n",
      "        max_eps : float, default=np.inf\n",
      "            The maximum distance between two samples for one to be considered as\n",
      "            in the neighborhood of the other. Default value of ``np.inf`` will\n",
      "            identify clusters across all scales; reducing ``max_eps`` will result\n",
      "            in shorter run times.\n",
      "        \n",
      "        metric : str or callable, default='minkowski'\n",
      "            Metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string. If metric is\n",
      "            \"precomputed\", X is assumed to be a distance matrix and must be square.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        p : int, default=2\n",
      "            Parameter for the Minkowski metric from\n",
      "            :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\n",
      "            equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "            (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "        \n",
      "        metric_params : dict, default=None\n",
      "            Additional keyword arguments for the metric function.\n",
      "        \n",
      "        algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "            Algorithm used to compute the nearest neighbors:\n",
      "        \n",
      "            - 'ball_tree' will use :class:`BallTree`\n",
      "            - 'kd_tree' will use :class:`KDTree`\n",
      "            - 'brute' will use a brute-force search.\n",
      "            - 'auto' will attempt to decide the most appropriate algorithm\n",
      "              based on the values passed to :meth:`fit` method. (default)\n",
      "        \n",
      "            Note: fitting on sparse input will override the setting of\n",
      "            this parameter, using brute force.\n",
      "        \n",
      "        leaf_size : int, default=30\n",
      "            Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n",
      "            affect the speed of the construction and query, as well as the memory\n",
      "            required to store the tree. The optimal value depends on the\n",
      "            nature of the problem.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of parallel jobs to run for neighbors search.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ordering_ : array of shape (n_samples,)\n",
      "            The cluster ordered list of sample indices.\n",
      "        \n",
      "        core_distances_ : array of shape (n_samples,)\n",
      "            Distance at which each sample becomes a core point, indexed by object\n",
      "            order. Points which will never be core have a distance of inf. Use\n",
      "            ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n",
      "        \n",
      "        reachability_ : array of shape (n_samples,)\n",
      "            Reachability distances per sample, indexed by object order. Use\n",
      "            ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n",
      "        \n",
      "        predecessor_ : array of shape (n_samples,)\n",
      "            Point that a sample was reached from, indexed by object order.\n",
      "            Seed points have a predecessor of -1.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n",
      "           and Jörg Sander. \"OPTICS: ordering points to identify the clustering\n",
      "           structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n",
      "    \n",
      "    dbscan(X, eps=0.5, *, min_samples=5, metric='minkowski', metric_params=None, algorithm='auto', leaf_size=30, p=2, sample_weight=None, n_jobs=None)\n",
      "        Perform DBSCAN clustering from vector array or distance matrix.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <dbscan>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse (CSR) matrix} of shape (n_samples, n_features) or             (n_samples, n_samples)\n",
      "            A feature array, or array of distances between samples if\n",
      "            ``metric='precomputed'``.\n",
      "        \n",
      "        eps : float, default=0.5\n",
      "            The maximum distance between two samples for one to be considered\n",
      "            as in the neighborhood of the other. This is not a maximum bound\n",
      "            on the distances of points within a cluster. This is the most\n",
      "            important DBSCAN parameter to choose appropriately for your data set\n",
      "            and distance function.\n",
      "        \n",
      "        min_samples : int, default=5\n",
      "            The number of samples (or total weight) in a neighborhood for a point\n",
      "            to be considered as a core point. This includes the point itself.\n",
      "        \n",
      "        metric : str or callable, default='minkowski'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string or callable, it must be one of\n",
      "            the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n",
      "            its metric parameter.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "            must be square during fit.\n",
      "            X may be a :term:`sparse graph <sparse graph>`,\n",
      "            in which case only \"nonzero\" elements may be considered neighbors.\n",
      "        \n",
      "        metric_params : dict, default=None\n",
      "            Additional keyword arguments for the metric function.\n",
      "        \n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "        algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "            The algorithm to be used by the NearestNeighbors module\n",
      "            to compute pointwise distances and find nearest neighbors.\n",
      "            See NearestNeighbors module documentation for details.\n",
      "        \n",
      "        leaf_size : int, default=30\n",
      "            Leaf size passed to BallTree or cKDTree. This can affect the speed\n",
      "            of the construction and query, as well as the memory required\n",
      "            to store the tree. The optimal value depends\n",
      "            on the nature of the problem.\n",
      "        \n",
      "        p : float, default=2\n",
      "            The power of the Minkowski metric to be used to calculate distance\n",
      "            between points.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Weight of each sample, such that a sample with a weight of at least\n",
      "            ``min_samples`` is by itself a core sample; a sample with negative\n",
      "            weight may inhibit its eps-neighbor from being core.\n",
      "            Note that weights are absolute, and default to 1.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of parallel jobs to run for neighbors search. ``None`` means\n",
      "            1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means\n",
      "            using all processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "            If precomputed distance are used, parallel execution is not available\n",
      "            and thus n_jobs will have no effect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        core_samples : ndarray of shape (n_core_samples,)\n",
      "            Indices of core samples.\n",
      "        \n",
      "        labels : ndarray of shape (n_samples,)\n",
      "            Cluster labels for each point.  Noisy samples are given the label -1.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        DBSCAN : An estimator interface for this clustering algorithm.\n",
      "        OPTICS : A similar estimator interface clustering at multiple values of\n",
      "            eps. Our implementation is optimized for memory usage.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For an example, see :ref:`examples/cluster/plot_dbscan.py\n",
      "        <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n",
      "        \n",
      "        This implementation bulk-computes all neighborhood queries, which increases\n",
      "        the memory complexity to O(n.d) where d is the average number of neighbors,\n",
      "        while original DBSCAN had memory complexity O(n). It may attract a higher\n",
      "        memory complexity when querying these nearest neighborhoods, depending\n",
      "        on the ``algorithm``.\n",
      "        \n",
      "        One way to avoid the query complexity is to pre-compute sparse\n",
      "        neighborhoods in chunks using\n",
      "        :func:`NearestNeighbors.radius_neighbors_graph\n",
      "        <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n",
      "        ``mode='distance'``, then using ``metric='precomputed'`` here.\n",
      "        \n",
      "        Another way to reduce memory and computation time is to remove\n",
      "        (near-)duplicate points and use ``sample_weight`` instead.\n",
      "        \n",
      "        :func:`cluster.optics <sklearn.cluster.optics>` provides a similar\n",
      "        clustering with lower memory usage.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Ester, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\n",
      "        Algorithm for Discovering Clusters in Large Spatial Databases with Noise\".\n",
      "        In: Proceedings of the 2nd International Conference on Knowledge Discovery\n",
      "        and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n",
      "        \n",
      "        Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n",
      "        DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\n",
      "        ACM Transactions on Database Systems (TODS), 42(3), 19.\n",
      "    \n",
      "    estimate_bandwidth(X, *, quantile=0.3, n_samples=None, random_state=0, n_jobs=None)\n",
      "        Estimate the bandwidth to use with the mean-shift algorithm.\n",
      "        \n",
      "        That this function takes time at least quadratic in n_samples. For large\n",
      "        datasets, it's wise to set that parameter to a small value.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            Input points.\n",
      "        \n",
      "        quantile : float, default=0.3\n",
      "            Should be between [0, 1]\n",
      "            0.5 means that the median of all pairwise distances is used.\n",
      "        \n",
      "        n_samples : int, default=None\n",
      "            The number of samples to use. If not given, all samples are used.\n",
      "        \n",
      "        random_state : int, RandomState instance, default=None\n",
      "            The generator used to randomly select the samples from input points\n",
      "            for bandwidth estimation. Use an int to make the randomness\n",
      "            deterministic.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of parallel jobs to run for neighbors search.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        bandwidth : float\n",
      "            The bandwidth parameter.\n",
      "    \n",
      "    get_bin_seeds(X, bin_size, min_bin_freq=1)\n",
      "        Find seeds for mean_shift.\n",
      "        \n",
      "        Finds seeds by first binning data onto a grid whose lines are\n",
      "        spaced bin_size apart, and then choosing those bins with at least\n",
      "        min_bin_freq points.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        \n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            Input points, the same points that will be used in mean_shift.\n",
      "        \n",
      "        bin_size : float\n",
      "            Controls the coarseness of the binning. Smaller values lead\n",
      "            to more seeding (which is computationally more expensive). If you're\n",
      "            not sure how to set this, set it to the value of the bandwidth used\n",
      "            in clustering.mean_shift.\n",
      "        \n",
      "        min_bin_freq : int, default=1\n",
      "            Only bins with at least min_bin_freq will be selected as seeds.\n",
      "            Raising this value decreases the number of seeds found, which\n",
      "            makes mean_shift computationally cheaper.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        bin_seeds : array-like of shape (n_samples, n_features)\n",
      "            Points used as initial kernel positions in clustering.mean_shift.\n",
      "    \n",
      "    k_means(X, n_clusters, *, sample_weight=None, init='k-means++', n_init=10, max_iter=300, verbose=False, tol=0.0001, random_state=None, copy_x=True, algorithm='auto', return_n_iter=False)\n",
      "        Perform K-means clustering algorithm.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <k_means>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The observations to cluster. It must be noted that the data\n",
      "            will be converted to C ordering, which will cause a memory copy\n",
      "            if the given data is not C-contiguous.\n",
      "        \n",
      "        n_clusters : int\n",
      "            The number of clusters to form as well as the number of\n",
      "            centroids to generate.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            The weights for each observation in `X`. If `None`, all observations\n",
      "            are assigned equal weight.\n",
      "        \n",
      "        init : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n",
      "            Method for initialization:\n",
      "        \n",
      "            - `'k-means++'` : selects initial cluster centers for k-mean\n",
      "              clustering in a smart way to speed up convergence. See section\n",
      "              Notes in k_init for more details.\n",
      "            - `'random'`: choose `n_clusters` observations (rows) at random from data\n",
      "              for the initial centroids.\n",
      "            - If an array is passed, it should be of shape `(n_clusters, n_features)`\n",
      "              and gives the initial centers.\n",
      "            - If a callable is passed, it should take arguments `X`, `n_clusters` and a\n",
      "              random state and return an initialization.\n",
      "        \n",
      "        n_init : int, default=10\n",
      "            Number of time the k-means algorithm will be run with different\n",
      "            centroid seeds. The final results will be the best output of\n",
      "            `n_init` consecutive runs in terms of inertia.\n",
      "        \n",
      "        max_iter : int, default=300\n",
      "            Maximum number of iterations of the k-means algorithm to run.\n",
      "        \n",
      "        verbose : bool, default=False\n",
      "            Verbosity mode.\n",
      "        \n",
      "        tol : float, default=1e-4\n",
      "            Relative tolerance with regards to Frobenius norm of the difference\n",
      "            in the cluster centers of two consecutive iterations to declare\n",
      "            convergence.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for centroid initialization. Use\n",
      "            an int to make the randomness deterministic.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        copy_x : bool, default=True\n",
      "            When pre-computing distances it is more numerically accurate to center\n",
      "            the data first. If `copy_x` is True (default), then the original data is\n",
      "            not modified. If False, the original data is modified, and put back\n",
      "            before the function returns, but small numerical differences may be\n",
      "            introduced by subtracting and then adding the data mean. Note that if\n",
      "            the original data is not C-contiguous, a copy will be made even if\n",
      "            `copy_x` is False. If the original data is sparse, but not in CSR format,\n",
      "            a copy will be made even if `copy_x` is False.\n",
      "        \n",
      "        algorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n",
      "            K-means algorithm to use. The classical EM-style algorithm is `\"full\"`.\n",
      "            The `\"elkan\"` variation is more efficient on data with well-defined\n",
      "            clusters, by using the triangle inequality. However it's more memory\n",
      "            intensive due to the allocation of an extra array of shape\n",
      "            `(n_samples, n_clusters)`.\n",
      "        \n",
      "            For now `\"auto\"` (kept for backward compatibility) chooses `\"elkan\"` but it\n",
      "            might change in the future for a better heuristic.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether or not to return the number of iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        centroid : ndarray of shape (n_clusters, n_features)\n",
      "            Centroids found at the last iteration of k-means.\n",
      "        \n",
      "        label : ndarray of shape (n_samples,)\n",
      "            The `label[i]` is the code or index of the centroid the\n",
      "            i'th observation is closest to.\n",
      "        \n",
      "        inertia : float\n",
      "            The final value of the inertia criterion (sum of squared distances to\n",
      "            the closest centroid for all observations in the training set).\n",
      "        \n",
      "        best_n_iter : int\n",
      "            Number of iterations corresponding to the best results.\n",
      "            Returned only if `return_n_iter` is set to True.\n",
      "    \n",
      "    kmeans_plusplus(X, n_clusters, *, x_squared_norms=None, random_state=None, n_local_trials=None)\n",
      "        Init n_clusters seeds according to k-means++\n",
      "        \n",
      "        .. versionadded:: 0.24\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to pick seeds from.\n",
      "        \n",
      "        n_clusters : int\n",
      "            The number of centroids to initialize\n",
      "        \n",
      "        x_squared_norms : array-like of shape (n_samples,), default=None\n",
      "            Squared Euclidean norm of each data point.\n",
      "        \n",
      "        random_state : int or RandomState instance, default=None\n",
      "            Determines random number generation for centroid initialization. Pass\n",
      "            an int for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        n_local_trials : int, default=None\n",
      "            The number of seeding trials for each center (except the first),\n",
      "            of which the one reducing inertia the most is greedily chosen.\n",
      "            Set to None to make the number of trials depend logarithmically\n",
      "            on the number of seeds (2+log(k)).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        centers : ndarray of shape (n_clusters, n_features)\n",
      "            The initial centers for k-means.\n",
      "        \n",
      "        indices : ndarray of shape (n_clusters,)\n",
      "            The index location of the chosen centers in the data array X. For a\n",
      "            given index and center, X[index] = center.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Selects initial cluster centers for k-mean clustering in a smart way\n",
      "        to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n",
      "        \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n",
      "        on Discrete algorithms. 2007\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        >>> from sklearn.cluster import kmeans_plusplus\n",
      "        >>> import numpy as np\n",
      "        >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
      "        ...               [10, 2], [10, 4], [10, 0]])\n",
      "        >>> centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0)\n",
      "        >>> centers\n",
      "        array([[10,  4],\n",
      "               [ 1,  0]])\n",
      "        >>> indices\n",
      "        array([4, 2])\n",
      "    \n",
      "    linkage_tree(X, connectivity=None, n_clusters=None, linkage='complete', affinity='euclidean', return_distance=False)\n",
      "        Linkage agglomerative clustering based on a Feature matrix.\n",
      "        \n",
      "        The inertia matrix uses a Heapq-based representation.\n",
      "        \n",
      "        This is the structured version, that takes into account some topological\n",
      "        structure between samples.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hierarchical_clustering>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            Feature matrix representing `n_samples` samples to be clustered.\n",
      "        \n",
      "        connectivity : sparse matrix, default=None\n",
      "            Connectivity matrix. Defines for each sample the neighboring samples\n",
      "            following a given structure of the data. The matrix is assumed to\n",
      "            be symmetric and only the upper triangular half is used.\n",
      "            Default is `None`, i.e, the Ward algorithm is unstructured.\n",
      "        \n",
      "        n_clusters : int, default=None\n",
      "            Stop early the construction of the tree at `n_clusters`. This is\n",
      "            useful to decrease computation time if the number of clusters is\n",
      "            not small compared to the number of samples. In this case, the\n",
      "            complete tree is not computed, thus the 'children' output is of\n",
      "            limited use, and the 'parents' output should rather be used.\n",
      "            This option is valid only when specifying a connectivity matrix.\n",
      "        \n",
      "        linkage : {\"average\", \"complete\", \"single\"}, default=\"complete\"\n",
      "            Which linkage criteria to use. The linkage criterion determines which\n",
      "            distance to use between sets of observation.\n",
      "                - \"average\" uses the average of the distances of each observation of\n",
      "                  the two sets.\n",
      "                - \"complete\" or maximum linkage uses the maximum distances between\n",
      "                  all observations of the two sets.\n",
      "                - \"single\" uses the minimum of the distances between all\n",
      "                  observations of the two sets.\n",
      "        \n",
      "        affinity : str or callable, default='euclidean'\n",
      "            Which metric to use. Can be 'euclidean', 'manhattan', or any\n",
      "            distance known to paired distance (see metric.pairwise).\n",
      "        \n",
      "        return_distance : bool, default=False\n",
      "            Whether or not to return the distances between the clusters.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        children : ndarray of shape (n_nodes-1, 2)\n",
      "            The children of each non-leaf node. Values less than `n_samples`\n",
      "            correspond to leaves of the tree which are the original samples.\n",
      "            A node `i` greater than or equal to `n_samples` is a non-leaf\n",
      "            node and has children `children_[i - n_samples]`. Alternatively\n",
      "            at the i-th iteration, children[i][0] and children[i][1]\n",
      "            are merged to form node `n_samples + i`.\n",
      "        \n",
      "        n_connected_components : int\n",
      "            The number of connected components in the graph.\n",
      "        \n",
      "        n_leaves : int\n",
      "            The number of leaves in the tree.\n",
      "        \n",
      "        parents : ndarray of shape (n_nodes, ) or None\n",
      "            The parent of each node. Only returned when a connectivity matrix\n",
      "            is specified, elsewhere 'None' is returned.\n",
      "        \n",
      "        distances : ndarray of shape (n_nodes-1,)\n",
      "            Returned when `return_distance` is set to `True`.\n",
      "        \n",
      "            distances[i] refers to the distance between children[i][0] and\n",
      "            children[i][1] when they are merged.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ward_tree : Hierarchical clustering with ward linkage.\n",
      "    \n",
      "    mean_shift(X, *, bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, max_iter=300, n_jobs=None)\n",
      "        Perform mean shift clustering of data using a flat kernel.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_shift>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        \n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            Input data.\n",
      "        \n",
      "        bandwidth : float, default=None\n",
      "            Kernel bandwidth.\n",
      "        \n",
      "            If bandwidth is not given, it is determined using a heuristic based on\n",
      "            the median of all pairwise distances. This will take quadratic time in\n",
      "            the number of samples. The sklearn.cluster.estimate_bandwidth function\n",
      "            can be used to do this more efficiently.\n",
      "        \n",
      "        seeds : array-like of shape (n_seeds, n_features) or None\n",
      "            Point used as initial kernel locations. If None and bin_seeding=False,\n",
      "            each data point is used as a seed. If None and bin_seeding=True,\n",
      "            see bin_seeding.\n",
      "        \n",
      "        bin_seeding : bool, default=False\n",
      "            If true, initial kernel locations are not locations of all\n",
      "            points, but rather the location of the discretized version of\n",
      "            points, where points are binned onto a grid whose coarseness\n",
      "            corresponds to the bandwidth. Setting this option to True will speed\n",
      "            up the algorithm because fewer seeds will be initialized.\n",
      "            Ignored if seeds argument is not None.\n",
      "        \n",
      "        min_bin_freq : int, default=1\n",
      "           To speed up the algorithm, accept only those bins with at least\n",
      "           min_bin_freq points as seeds.\n",
      "        \n",
      "        cluster_all : bool, default=True\n",
      "            If true, then all points are clustered, even those orphans that are\n",
      "            not within any kernel. Orphans are assigned to the nearest kernel.\n",
      "            If false, then orphans are given cluster label -1.\n",
      "        \n",
      "        max_iter : int, default=300\n",
      "            Maximum number of iterations, per seed point before the clustering\n",
      "            operation terminates (for that seed point), if has not converged yet.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by computing\n",
      "            each of the n_init runs in parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               Parallel Execution using *n_jobs*.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        \n",
      "        cluster_centers : ndarray of shape (n_clusters, n_features)\n",
      "            Coordinates of cluster centers.\n",
      "        \n",
      "        labels : ndarray of shape (n_samples,)\n",
      "            Cluster labels for each point.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For an example, see :ref:`examples/cluster/plot_mean_shift.py\n",
      "        <sphx_glr_auto_examples_cluster_plot_mean_shift.py>`.\n",
      "    \n",
      "    spectral_clustering(affinity, *, n_clusters=8, n_components=None, eigen_solver=None, random_state=None, n_init=10, eigen_tol=0.0, assign_labels='kmeans', verbose=False)\n",
      "        Apply clustering to a projection of the normalized Laplacian.\n",
      "        \n",
      "        In practice Spectral Clustering is very useful when the structure of\n",
      "        the individual clusters is highly non-convex or more generally when\n",
      "        a measure of the center and spread of the cluster is not a suitable\n",
      "        description of the complete cluster. For instance, when clusters are\n",
      "        nested circles on the 2D plane.\n",
      "        \n",
      "        If affinity is the adjacency matrix of a graph, this method can be\n",
      "        used to find normalized graph cuts [1]_, [2]_.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <spectral_clustering>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        affinity : {array-like, sparse matrix} of shape (n_samples, n_samples)\n",
      "            The affinity matrix describing the relationship of the samples to\n",
      "            embed. **Must be symmetric**.\n",
      "        \n",
      "            Possible examples:\n",
      "              - adjacency matrix of a graph,\n",
      "              - heat kernel of the pairwise distance matrix of the samples,\n",
      "              - symmetric k-nearest neighbours connectivity matrix of the samples.\n",
      "        \n",
      "        n_clusters : int, default=None\n",
      "            Number of clusters to extract.\n",
      "        \n",
      "        n_components : int, default=n_clusters\n",
      "            Number of eigenvectors to use for the spectral embedding\n",
      "        \n",
      "        eigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}\n",
      "            The eigenvalue decomposition strategy to use. AMG requires pyamg\n",
      "            to be installed. It can be faster on very large, sparse problems,\n",
      "            but may also lead to instabilities. If None, then ``'arpack'`` is\n",
      "            used. See [4]_ for more details regarding `'lobpcg'`.\n",
      "        \n",
      "        random_state : int, RandomState instance, default=None\n",
      "            A pseudo random number generator used for the initialization\n",
      "            of the lobpcg eigenvectors decomposition when `eigen_solver ==\n",
      "            'amg'`, and for the K-Means initialization. Use an int to make\n",
      "            the results deterministic across calls (See\n",
      "            :term:`Glossary <random_state>`).\n",
      "        \n",
      "            .. note::\n",
      "                When using `eigen_solver == 'amg'`,\n",
      "                it is necessary to also fix the global numpy seed with\n",
      "                `np.random.seed(int)` to get deterministic results. See\n",
      "                https://github.com/pyamg/pyamg/issues/139 for further\n",
      "                information.\n",
      "        \n",
      "        n_init : int, default=10\n",
      "            Number of time the k-means algorithm will be run with different\n",
      "            centroid seeds. The final results will be the best output of n_init\n",
      "            consecutive runs in terms of inertia. Only used if\n",
      "            ``assign_labels='kmeans'``.\n",
      "        \n",
      "        eigen_tol : float, default=0.0\n",
      "            Stopping criterion for eigendecomposition of the Laplacian matrix\n",
      "            when using arpack eigen_solver.\n",
      "        \n",
      "        assign_labels : {'kmeans', 'discretize'}, default='kmeans'\n",
      "            The strategy to use to assign labels in the embedding\n",
      "            space.  There are two ways to assign labels after the Laplacian\n",
      "            embedding.  k-means can be applied and is a popular choice. But it can\n",
      "            also be sensitive to initialization. Discretization is another\n",
      "            approach which is less sensitive to random initialization [3]_.\n",
      "        \n",
      "        verbose : bool, default=False\n",
      "            Verbosity mode.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        labels : array of integers, shape: n_samples\n",
      "            The labels of the clusters.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Normalized cuts and image segmentation, 2000\n",
      "               Jianbo Shi, Jitendra Malik\n",
      "               <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324>`_\n",
      "        \n",
      "        .. [2] `A Tutorial on Spectral Clustering, 2007\n",
      "               Ulrike von Luxburg\n",
      "               <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323>`_\n",
      "        \n",
      "        .. [3] `Multiclass spectral clustering, 2003\n",
      "               Stella X. Yu, Jianbo Shi\n",
      "               <https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf>`_\n",
      "        \n",
      "        .. [4] `Toward the Optimal Preconditioned Eigensolver:\n",
      "               Locally Optimal Block Preconditioned Conjugate Gradient Method, 2001.\n",
      "               A. V. Knyazev\n",
      "               SIAM Journal on Scientific Computing 23, no. 2, pp. 517-541.\n",
      "               <https://epubs.siam.org/doi/pdf/10.1137/S1064827500366124>`_\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The graph should contain only one connect component, elsewhere\n",
      "        the results make little sense.\n",
      "        \n",
      "        This algorithm solves the normalized cut for k=2: it is a\n",
      "        normalized spectral clustering.\n",
      "    \n",
      "    ward_tree(X, *, connectivity=None, n_clusters=None, return_distance=False)\n",
      "        Ward clustering based on a Feature matrix.\n",
      "        \n",
      "        Recursively merges the pair of clusters that minimally increases\n",
      "        within-cluster variance.\n",
      "        \n",
      "        The inertia matrix uses a Heapq-based representation.\n",
      "        \n",
      "        This is the structured version, that takes into account some topological\n",
      "        structure between samples.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hierarchical_clustering>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            Feature matrix representing `n_samples` samples to be clustered.\n",
      "        \n",
      "        connectivity : sparse matrix, default=None\n",
      "            Connectivity matrix. Defines for each sample the neighboring samples\n",
      "            following a given structure of the data. The matrix is assumed to\n",
      "            be symmetric and only the upper triangular half is used.\n",
      "            Default is None, i.e, the Ward algorithm is unstructured.\n",
      "        \n",
      "        n_clusters : int, default=None\n",
      "            `n_clusters` should be less than `n_samples`.  Stop early the\n",
      "            construction of the tree at `n_clusters.` This is useful to decrease\n",
      "            computation time if the number of clusters is not small compared to the\n",
      "            number of samples. In this case, the complete tree is not computed, thus\n",
      "            the 'children' output is of limited use, and the 'parents' output should\n",
      "            rather be used. This option is valid only when specifying a connectivity\n",
      "            matrix.\n",
      "        \n",
      "        return_distance : bool, default=False\n",
      "            If `True`, return the distance between the clusters.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        children : ndarray of shape (n_nodes-1, 2)\n",
      "            The children of each non-leaf node. Values less than `n_samples`\n",
      "            correspond to leaves of the tree which are the original samples.\n",
      "            A node `i` greater than or equal to `n_samples` is a non-leaf\n",
      "            node and has children `children_[i - n_samples]`. Alternatively\n",
      "            at the i-th iteration, children[i][0] and children[i][1]\n",
      "            are merged to form node `n_samples + i`.\n",
      "        \n",
      "        n_connected_components : int\n",
      "            The number of connected components in the graph.\n",
      "        \n",
      "        n_leaves : int\n",
      "            The number of leaves in the tree.\n",
      "        \n",
      "        parents : ndarray of shape (n_nodes,) or None\n",
      "            The parent of each node. Only returned when a connectivity matrix\n",
      "            is specified, elsewhere 'None' is returned.\n",
      "        \n",
      "        distances : ndarray of shape (n_nodes-1,)\n",
      "            Only returned if `return_distance` is set to `True` (for compatibility).\n",
      "            The distances between the centers of the nodes. `distances[i]`\n",
      "            corresponds to a weighted Euclidean distance between\n",
      "            the nodes `children[i, 1]` and `children[i, 2]`. If the nodes refer to\n",
      "            leaves of the tree, then `distances[i]` is their unweighted Euclidean\n",
      "            distance. Distances are updated in the following way\n",
      "            (from scipy.hierarchy.linkage):\n",
      "        \n",
      "            The new entry :math:`d(u,v)` is computed as follows,\n",
      "        \n",
      "            .. math::\n",
      "        \n",
      "               d(u,v) = \\sqrt{\\frac{|v|+|s|}\n",
      "                                   {T}d(v,s)^2\n",
      "                            + \\frac{|v|+|t|}\n",
      "                                   {T}d(v,t)^2\n",
      "                            - \\frac{|v|}\n",
      "                                   {T}d(s,t)^2}\n",
      "        \n",
      "            where :math:`u` is the newly joined cluster consisting of\n",
      "            clusters :math:`s` and :math:`t`, :math:`v` is an unused\n",
      "            cluster in the forest, :math:`T=|v|+|s|+|t|`, and\n",
      "            :math:`|*|` is the cardinality of its argument. This is also\n",
      "            known as the incremental algorithm.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['AffinityPropagation', 'AgglomerativeClustering', 'Birch', ...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8f2a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
