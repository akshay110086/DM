{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc9d797",
   "metadata": {},
   "source": [
    "# Question 5.\n",
    "Use Naive bayes, K-nearest, and Decision tree classification algorithms and build classifiers.\n",
    "Divide the data set into training and test set. Compare the accuracy of the different classifiers\n",
    "under the following situations:\n",
    "5.1 a) Training set = 75% Test set = 25% b) Training set = 66.6% (2/3rd of total), Test set =\n",
    "33.3%\n",
    "5.2 Training set is chosen by i) hold out method ii) Random subsampling iii) Cross-Validation.\n",
    "Compare the accuracy of the classifiers obtained.\n",
    "5.3 Data is scaled to standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c5143d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.neighbors\n",
    "import sklearn.tree\n",
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1a557ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=sklearn.datasets.load_iris() \n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(data.data, data.target, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6a65781",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=sklearn.tree.DecisionTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4f124f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=sklearn.neighbors.KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e165c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3=sklearn.naive_bayes.GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b67c28aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train,y_train)\n",
    "model2.fit(X_train,y_train)\n",
    "model3.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cbbdf14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre_1=model1.predict(X_test)\n",
    "y_pre_2=model2.predict(X_test)\n",
    "y_pre_3=model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26a78a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(y_test, y_pre_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61c288fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(y_test, y_pre_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d617ba37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(y_test, y_pre_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "45cba98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19,  0,  0],\n",
       "       [ 0, 14,  1],\n",
       "       [ 0,  1, 15]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(y_test, y_pre_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7be1d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98 0.94 0.98]\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.model_selection.cross_val_score(model1, data.data, data.target, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4c2474f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98 0.96 0.98]\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.model_selection.cross_val_score(model2, data.data, data.target, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f931af1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92 0.94 0.96]\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.model_selection.cross_val_score(model3, data.data, data.target, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e558204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62e63ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.utils in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.utils - The :mod:`sklearn.utils` module includes various utilities.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _arpack\n",
      "    _cython_blas\n",
      "    _encode\n",
      "    _estimator_html_repr\n",
      "    _fast_dict\n",
      "    _joblib\n",
      "    _logistic_sigmoid\n",
      "    _mask\n",
      "    _mocking\n",
      "    _openmp_helpers\n",
      "    _pprint\n",
      "    _random\n",
      "    _readonly_array_wrapper\n",
      "    _seq_dataset\n",
      "    _show_versions\n",
      "    _tags\n",
      "    _testing\n",
      "    _typedefs\n",
      "    _weight_vector\n",
      "    arrayfuncs\n",
      "    class_weight\n",
      "    deprecation\n",
      "    estimator_checks\n",
      "    extmath\n",
      "    fixes\n",
      "    graph\n",
      "    metaestimators\n",
      "    multiclass\n",
      "    murmurhash\n",
      "    optimize\n",
      "    random\n",
      "    setup\n",
      "    sparsefuncs\n",
      "    sparsefuncs_fast\n",
      "    stats\n",
      "    tests (package)\n",
      "    validation\n",
      "\n",
      "CLASSES\n",
      "    builtins.UserWarning(builtins.Warning)\n",
      "        sklearn.exceptions.DataConversionWarning\n",
      "    builtins.object\n",
      "        joblib.parallel.parallel_backend\n",
      "        sklearn.utils.deprecation.deprecated\n",
      "    \n",
      "    class DataConversionWarning(builtins.UserWarning)\n",
      "     |  Warning used to notify implicit data conversions happening in the code.\n",
      "     |  \n",
      "     |  This warning occurs when some input data needs to be converted or\n",
      "     |  interpreted in a way that may not match the user's expectations.\n",
      "     |  \n",
      "     |  For example, this warning may occur when the user\n",
      "     |      - passes an integer array to a function which expects float input and\n",
      "     |        will convert the input\n",
      "     |      - requests a non-copying operation, but a copy is required to meet the\n",
      "     |        implementation's data-type expectations;\n",
      "     |      - passes an input whose shape can be interpreted ambiguously.\n",
      "     |  \n",
      "     |  .. versionchanged:: 0.18\n",
      "     |     Moved from sklearn.utils.validation.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DataConversionWarning\n",
      "     |      builtins.UserWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.UserWarning:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.UserWarning:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class deprecated(builtins.object)\n",
      "     |  deprecated(extra='')\n",
      "     |  \n",
      "     |  Decorator to mark a function or class as deprecated.\n",
      "     |  \n",
      "     |  Issue a warning when the function is called/the class is instantiated and\n",
      "     |  adds a warning to the docstring.\n",
      "     |  \n",
      "     |  The optional extra argument will be appended to the deprecation message\n",
      "     |  and the docstring. Note: to use this with the default value for extra, put\n",
      "     |  in an empty of parentheses:\n",
      "     |  \n",
      "     |  >>> from sklearn.utils import deprecated\n",
      "     |  >>> deprecated()\n",
      "     |  <sklearn.utils.deprecation.deprecated object at ...>\n",
      "     |  \n",
      "     |  >>> @deprecated()\n",
      "     |  ... def some_function(): pass\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  extra : str, default=''\n",
      "     |        To be added to the deprecation messages.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, obj)\n",
      "     |      Call method\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      obj : object\n",
      "     |  \n",
      "     |  __init__(self, extra='')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class parallel_backend(builtins.object)\n",
      "     |  parallel_backend(backend, n_jobs=-1, inner_max_num_threads=None, **backend_params)\n",
      "     |  \n",
      "     |  Change the default backend used by Parallel inside a with block.\n",
      "     |  \n",
      "     |  If ``backend`` is a string it must match a previously registered\n",
      "     |  implementation using the ``register_parallel_backend`` function.\n",
      "     |  \n",
      "     |  By default the following backends are available:\n",
      "     |  \n",
      "     |  - 'loky': single-host, process-based parallelism (used by default),\n",
      "     |  - 'threading': single-host, thread-based parallelism,\n",
      "     |  - 'multiprocessing': legacy single-host, process-based parallelism.\n",
      "     |  \n",
      "     |  'loky' is recommended to run functions that manipulate Python objects.\n",
      "     |  'threading' is a low-overhead alternative that is most efficient for\n",
      "     |  functions that release the Global Interpreter Lock: e.g. I/O-bound code or\n",
      "     |  CPU-bound code in a few calls to native code that explicitly releases the\n",
      "     |  GIL.\n",
      "     |  \n",
      "     |  In addition, if the `dask` and `distributed` Python packages are installed,\n",
      "     |  it is possible to use the 'dask' backend for better scheduling of nested\n",
      "     |  parallel calls without over-subscription and potentially distribute\n",
      "     |  parallel calls over a networked cluster of several hosts.\n",
      "     |  \n",
      "     |  It is also possible to use the distributed 'ray' backend for distributing\n",
      "     |  the workload to a cluster of nodes. To use the 'ray' joblib backend add\n",
      "     |  the following lines::\n",
      "     |  \n",
      "     |   >>> from ray.util.joblib import register_ray  # doctest: +SKIP\n",
      "     |   >>> register_ray()  # doctest: +SKIP\n",
      "     |   >>> with parallel_backend(\"ray\"):  # doctest: +SKIP\n",
      "     |   ...     print(Parallel()(delayed(neg)(i + 1) for i in range(5)))\n",
      "     |   [-1, -2, -3, -4, -5]\n",
      "     |  \n",
      "     |  Alternatively the backend can be passed directly as an instance.\n",
      "     |  \n",
      "     |  By default all available workers will be used (``n_jobs=-1``) unless the\n",
      "     |  caller passes an explicit value for the ``n_jobs`` parameter.\n",
      "     |  \n",
      "     |  This is an alternative to passing a ``backend='backend_name'`` argument to\n",
      "     |  the ``Parallel`` class constructor. It is particularly useful when calling\n",
      "     |  into library code that uses joblib internally but does not expose the\n",
      "     |  backend argument in its own API.\n",
      "     |  \n",
      "     |  >>> from operator import neg\n",
      "     |  >>> with parallel_backend('threading'):\n",
      "     |  ...     print(Parallel()(delayed(neg)(i + 1) for i in range(5)))\n",
      "     |  ...\n",
      "     |  [-1, -2, -3, -4, -5]\n",
      "     |  \n",
      "     |  Warning: this function is experimental and subject to change in a future\n",
      "     |  version of joblib.\n",
      "     |  \n",
      "     |  Joblib also tries to limit the oversubscription by limiting the number of\n",
      "     |  threads usable in some third-party library threadpools like OpenBLAS, MKL\n",
      "     |  or OpenMP. The default limit in each worker is set to\n",
      "     |  ``max(cpu_count() // effective_n_jobs, 1)`` but this limit can be\n",
      "     |  overwritten with the ``inner_max_num_threads`` argument which will be used\n",
      "     |  to set this limit in the child processes.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.10\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __enter__(self)\n",
      "     |  \n",
      "     |  __exit__(self, type, value, traceback)\n",
      "     |  \n",
      "     |  __init__(self, backend, n_jobs=-1, inner_max_num_threads=None, **backend_params)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  unregister(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    all_estimators(type_filter=None)\n",
      "        Get a list of all estimators from sklearn.\n",
      "        \n",
      "        This function crawls the module and gets all classes that inherit\n",
      "        from BaseEstimator. Classes that are defined in test-modules are not\n",
      "        included.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        type_filter : {\"classifier\", \"regressor\", \"cluster\", \"transformer\"}             or list of such str, default=None\n",
      "            Which kind of estimators should be returned. If None, no filter is\n",
      "            applied and all estimators are returned.  Possible values are\n",
      "            'classifier', 'regressor', 'cluster' and 'transformer' to get\n",
      "            estimators only of these specific types, or a list of these to\n",
      "            get the estimators that fit at least one of the types.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        estimators : list of tuples\n",
      "            List of (name, class), where ``name`` is the class name as string\n",
      "            and ``class`` is the actual type of the class.\n",
      "    \n",
      "    as_float_array(X, *, copy=True, force_all_finite=True)\n",
      "        Convert an array-like to an array of floats.\n",
      "        \n",
      "        The new dtype will be np.float32 or np.float64, depending on the original\n",
      "        type. The function can create a copy or modify the argument depending\n",
      "        on the argument copy.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}\n",
      "            The input data.\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            If True, a copy of X will be created. If False, a copy may still be\n",
      "            returned if X's dtype is not a floating point type.\n",
      "        \n",
      "        force_all_finite : bool or 'allow-nan', default=True\n",
      "            Whether to raise an error on np.inf, np.nan, pd.NA in X. The\n",
      "            possibilities are:\n",
      "        \n",
      "            - True: Force all values of X to be finite.\n",
      "            - False: accepts np.inf, np.nan, pd.NA in X.\n",
      "            - 'allow-nan': accepts only np.nan and pd.NA values in X. Values cannot\n",
      "              be infinite.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "               ``force_all_finite`` accepts the string ``'allow-nan'``.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Accepts `pd.NA` and converts it into `np.nan`\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        XT : {ndarray, sparse matrix}\n",
      "            An array of type float.\n",
      "    \n",
      "    assert_all_finite(X, *, allow_nan=False)\n",
      "        Throw a ValueError if X contains NaN or infinity.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {ndarray, sparse matrix}\n",
      "        \n",
      "        allow_nan : bool, default=False\n",
      "    \n",
      "    check_X_y(X, y, accept_sparse=False, *, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, estimator=None)\n",
      "        Input validation for standard estimators.\n",
      "        \n",
      "        Checks X and y for consistent length, enforces X to be 2D and y 1D. By\n",
      "        default, X is checked to be non-empty and containing only finite values.\n",
      "        Standard input checks are also applied to y, such as checking that y\n",
      "        does not have np.nan or np.inf targets. For multi-label y, set\n",
      "        multi_output=True to allow 2D and sparse y. If the dtype of X is\n",
      "        object, attempt converting to float, raising on failure.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {ndarray, list, sparse matrix}\n",
      "            Input data.\n",
      "        \n",
      "        y : {ndarray, list, sparse matrix}\n",
      "            Labels.\n",
      "        \n",
      "        accept_sparse : str, bool or list of str, default=False\n",
      "            String[s] representing allowed sparse matrix formats, such as 'csc',\n",
      "            'csr', etc. If the input is sparse but not in the allowed format,\n",
      "            it will be converted to the first listed format. True allows the input\n",
      "            to be any format. False means that a sparse matrix input will\n",
      "            raise an error.\n",
      "        \n",
      "        accept_large_sparse : bool, default=True\n",
      "            If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\n",
      "            accept_sparse, accept_large_sparse will cause it to be accepted only\n",
      "            if its indices are stored with a 32-bit dtype.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        dtype : 'numeric', type, list of type or None, default='numeric'\n",
      "            Data type of result. If None, the dtype of the input is preserved.\n",
      "            If \"numeric\", dtype is preserved unless array.dtype is object.\n",
      "            If dtype is a list of types, conversion on the first type is only\n",
      "            performed if the dtype of the input is not in the list.\n",
      "        \n",
      "        order : {'F', 'C'}, default=None\n",
      "            Whether an array will be forced to be fortran or c-style.\n",
      "        \n",
      "        copy : bool, default=False\n",
      "            Whether a forced copy will be triggered. If copy=False, a copy might\n",
      "            be triggered by a conversion.\n",
      "        \n",
      "        force_all_finite : bool or 'allow-nan', default=True\n",
      "            Whether to raise an error on np.inf, np.nan, pd.NA in X. This parameter\n",
      "            does not influence whether y can have np.inf, np.nan, pd.NA values.\n",
      "            The possibilities are:\n",
      "        \n",
      "            - True: Force all values of X to be finite.\n",
      "            - False: accepts np.inf, np.nan, pd.NA in X.\n",
      "            - 'allow-nan': accepts only np.nan or pd.NA values in X. Values cannot\n",
      "              be infinite.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "               ``force_all_finite`` accepts the string ``'allow-nan'``.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Accepts `pd.NA` and converts it into `np.nan`\n",
      "        \n",
      "        ensure_2d : bool, default=True\n",
      "            Whether to raise a value error if X is not 2D.\n",
      "        \n",
      "        allow_nd : bool, default=False\n",
      "            Whether to allow X.ndim > 2.\n",
      "        \n",
      "        multi_output : bool, default=False\n",
      "            Whether to allow 2D y (array or sparse matrix). If false, y will be\n",
      "            validated as a vector. y cannot have np.nan or np.inf values if\n",
      "            multi_output=True.\n",
      "        \n",
      "        ensure_min_samples : int, default=1\n",
      "            Make sure that X has a minimum number of samples in its first\n",
      "            axis (rows for a 2D array).\n",
      "        \n",
      "        ensure_min_features : int, default=1\n",
      "            Make sure that the 2D array has some minimum number of features\n",
      "            (columns). The default value of 1 rejects empty datasets.\n",
      "            This check is only enforced when X has effectively 2 dimensions or\n",
      "            is originally 1D and ``ensure_2d`` is True. Setting to 0 disables\n",
      "            this check.\n",
      "        \n",
      "        y_numeric : bool, default=False\n",
      "            Whether to ensure that y has a numeric type. If dtype of y is object,\n",
      "            it is converted to float64. Should only be used for regression\n",
      "            algorithms.\n",
      "        \n",
      "        estimator : str or estimator instance, default=None\n",
      "            If passed, include the name of the estimator in warning messages.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X_converted : object\n",
      "            The converted and validated X.\n",
      "        \n",
      "        y_converted : object\n",
      "            The converted and validated y.\n",
      "    \n",
      "    check_array(array, accept_sparse=False, *, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, estimator=None)\n",
      "        Input validation on an array, list, sparse matrix or similar.\n",
      "        \n",
      "        By default, the input is checked to be a non-empty 2D array containing\n",
      "        only finite values. If the dtype of the array is object, attempt\n",
      "        converting to float, raising on failure.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        array : object\n",
      "            Input object to check / convert.\n",
      "        \n",
      "        accept_sparse : str, bool or list/tuple of str, default=False\n",
      "            String[s] representing allowed sparse matrix formats, such as 'csc',\n",
      "            'csr', etc. If the input is sparse but not in the allowed format,\n",
      "            it will be converted to the first listed format. True allows the input\n",
      "            to be any format. False means that a sparse matrix input will\n",
      "            raise an error.\n",
      "        \n",
      "        accept_large_sparse : bool, default=True\n",
      "            If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\n",
      "            accept_sparse, accept_large_sparse=False will cause it to be accepted\n",
      "            only if its indices are stored with a 32-bit dtype.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        dtype : 'numeric', type, list of type or None, default='numeric'\n",
      "            Data type of result. If None, the dtype of the input is preserved.\n",
      "            If \"numeric\", dtype is preserved unless array.dtype is object.\n",
      "            If dtype is a list of types, conversion on the first type is only\n",
      "            performed if the dtype of the input is not in the list.\n",
      "        \n",
      "        order : {'F', 'C'} or None, default=None\n",
      "            Whether an array will be forced to be fortran or c-style.\n",
      "            When order is None (default), then if copy=False, nothing is ensured\n",
      "            about the memory layout of the output array; otherwise (copy=True)\n",
      "            the memory layout of the returned array is kept as close as possible\n",
      "            to the original array.\n",
      "        \n",
      "        copy : bool, default=False\n",
      "            Whether a forced copy will be triggered. If copy=False, a copy might\n",
      "            be triggered by a conversion.\n",
      "        \n",
      "        force_all_finite : bool or 'allow-nan', default=True\n",
      "            Whether to raise an error on np.inf, np.nan, pd.NA in array. The\n",
      "            possibilities are:\n",
      "        \n",
      "            - True: Force all values of array to be finite.\n",
      "            - False: accepts np.inf, np.nan, pd.NA in array.\n",
      "            - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n",
      "              cannot be infinite.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "               ``force_all_finite`` accepts the string ``'allow-nan'``.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Accepts `pd.NA` and converts it into `np.nan`\n",
      "        \n",
      "        ensure_2d : bool, default=True\n",
      "            Whether to raise a value error if array is not 2D.\n",
      "        \n",
      "        allow_nd : bool, default=False\n",
      "            Whether to allow array.ndim > 2.\n",
      "        \n",
      "        ensure_min_samples : int, default=1\n",
      "            Make sure that the array has a minimum number of samples in its first\n",
      "            axis (rows for a 2D array). Setting to 0 disables this check.\n",
      "        \n",
      "        ensure_min_features : int, default=1\n",
      "            Make sure that the 2D array has some minimum number of features\n",
      "            (columns). The default value of 1 rejects empty datasets.\n",
      "            This check is only enforced when the input data has effectively 2\n",
      "            dimensions or is originally 1D and ``ensure_2d`` is True. Setting to 0\n",
      "            disables this check.\n",
      "        \n",
      "        estimator : str or estimator instance, default=None\n",
      "            If passed, include the name of the estimator in warning messages.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        array_converted : object\n",
      "            The converted and validated array.\n",
      "    \n",
      "    check_consistent_length(*arrays)\n",
      "        Check that all arrays have consistent first dimensions.\n",
      "        \n",
      "        Checks whether all objects in arrays have the same shape or length.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        *arrays : list or tuple of input objects.\n",
      "            Objects that will be checked for consistent length.\n",
      "    \n",
      "    check_matplotlib_support(caller_name)\n",
      "        Raise ImportError with detailed error message if mpl is not installed.\n",
      "        \n",
      "        Plot utilities like any of the Display's plotting functions should lazily import\n",
      "        matplotlib and call this helper before any computation.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        caller_name : str\n",
      "            The name of the caller that requires matplotlib.\n",
      "    \n",
      "    check_random_state(seed)\n",
      "        Turn seed into a np.random.RandomState instance\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        seed : None, int or instance of RandomState\n",
      "            If seed is None, return the RandomState singleton used by np.random.\n",
      "            If seed is an int, return a new RandomState instance seeded with seed.\n",
      "            If seed is already a RandomState instance, return it.\n",
      "            Otherwise raise ValueError.\n",
      "    \n",
      "    check_scalar(x, name, target_type, *, min_val=None, max_val=None, include_boundaries='both')\n",
      "        Validate scalar parameters type and value.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : object\n",
      "            The scalar parameter to validate.\n",
      "        \n",
      "        name : str\n",
      "            The name of the parameter to be printed in error messages.\n",
      "        \n",
      "        target_type : type or tuple\n",
      "            Acceptable data types for the parameter.\n",
      "        \n",
      "        min_val : float or int, default=None\n",
      "            The minimum valid value the parameter can take. If None (default) it\n",
      "            is implied that the parameter does not have a lower bound.\n",
      "        \n",
      "        max_val : float or int, default=None\n",
      "            The maximum valid value the parameter can take. If None (default) it\n",
      "            is implied that the parameter does not have an upper bound.\n",
      "        \n",
      "        include_boundaries : {\"left\", \"right\", \"both\", \"neither\"}, default=\"both\"\n",
      "            Whether the interval defined by `min_val` and `max_val` should include\n",
      "            the boundaries. Possible choices are:\n",
      "        \n",
      "            - `\"left\"`: only `min_val` is included in the valid interval;\n",
      "            - `\"right\"`: only `max_val` is included in the valid interval;\n",
      "            - `\"both\"`: `min_val` and `max_val` are included in the valid interval;\n",
      "            - `\"neither\"`: neither `min_val` nor `max_val` are included in the\n",
      "              valid interval.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : numbers.Number\n",
      "            The validated number.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        TypeError\n",
      "            If the parameter's type does not match the desired type.\n",
      "        \n",
      "        ValueError\n",
      "            If the parameter's value violates the given bounds.\n",
      "    \n",
      "    check_symmetric(array, *, tol=1e-10, raise_warning=True, raise_exception=False)\n",
      "        Make sure that array is 2D, square and symmetric.\n",
      "        \n",
      "        If the array is not symmetric, then a symmetrized version is returned.\n",
      "        Optionally, a warning or exception is raised if the matrix is not\n",
      "        symmetric.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        array : {ndarray, sparse matrix}\n",
      "            Input object to check / convert. Must be two-dimensional and square,\n",
      "            otherwise a ValueError will be raised.\n",
      "        \n",
      "        tol : float, default=1e-10\n",
      "            Absolute tolerance for equivalence of arrays. Default = 1E-10.\n",
      "        \n",
      "        raise_warning : bool, default=True\n",
      "            If True then raise a warning if conversion is required.\n",
      "        \n",
      "        raise_exception : bool, default=False\n",
      "            If True then raise an exception if array is not symmetric.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        array_sym : {ndarray, sparse matrix}\n",
      "            Symmetrized version of the input array, i.e. the average of array\n",
      "            and array.transpose(). If sparse, then duplicate entries are first\n",
      "            summed and zeros are eliminated.\n",
      "    \n",
      "    column_or_1d(y, *, warn=False)\n",
      "        Ravel column or 1d numpy array, else raises an error.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : array-like\n",
      "           Input data.\n",
      "        \n",
      "        warn : bool, default=False\n",
      "           To control display of warnings.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y : ndarray\n",
      "           Output data.\n",
      "        \n",
      "        Raises\n",
      "        -------\n",
      "        ValueError\n",
      "            If `y` is not a 1D array or a 2D array with a single row or column.\n",
      "    \n",
      "    compute_class_weight(class_weight, *, classes, y)\n",
      "        Estimate class weights for unbalanced datasets.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        class_weight : dict, 'balanced' or None\n",
      "            If 'balanced', class weights will be given by\n",
      "            ``n_samples / (n_classes * np.bincount(y))``.\n",
      "            If a dictionary is given, keys are classes and values\n",
      "            are corresponding class weights.\n",
      "            If None is given, the class weights will be uniform.\n",
      "        \n",
      "        classes : ndarray\n",
      "            Array of the classes occurring in the data, as given by\n",
      "            ``np.unique(y_org)`` with ``y_org`` the original class labels.\n",
      "        \n",
      "        y : array-like of shape (n_samples,)\n",
      "            Array of original class labels per sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        class_weight_vect : ndarray of shape (n_classes,)\n",
      "            Array with class_weight_vect[i] the weight for i-th class.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        The \"balanced\" heuristic is inspired by\n",
      "        Logistic Regression in Rare Events Data, King, Zen, 2001.\n",
      "    \n",
      "    compute_sample_weight(class_weight, y, *, indices=None)\n",
      "        Estimate sample weights by class for unbalanced datasets.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        class_weight : dict, list of dicts, \"balanced\", or None\n",
      "            Weights associated with classes in the form ``{class_label: weight}``.\n",
      "            If not given, all classes are supposed to have weight one. For\n",
      "            multi-output problems, a list of dicts can be provided in the same\n",
      "            order as the columns of y.\n",
      "        \n",
      "            Note that for multioutput (including multilabel) weights should be\n",
      "            defined for each class of every column in its own dict. For example,\n",
      "            for four-class multilabel classification weights should be\n",
      "            [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      "            [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      "        \n",
      "            The \"balanced\" mode uses the values of y to automatically adjust\n",
      "            weights inversely proportional to class frequencies in the input data:\n",
      "            ``n_samples / (n_classes * np.bincount(y))``.\n",
      "        \n",
      "            For multi-output, the weights of each column of y will be multiplied.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Array of original class labels per sample.\n",
      "        \n",
      "        indices : array-like of shape (n_subsample,), default=None\n",
      "            Array of indices to be used in a subsample. Can be of length less than\n",
      "            n_samples in the case of a subsample, or equal to n_samples in the\n",
      "            case of a bootstrap subsample with repeated indices. If None, the\n",
      "            sample weight will be calculated over the full sample. Only \"balanced\"\n",
      "            is supported for class_weight if this is provided.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sample_weight_vect : ndarray of shape (n_samples,)\n",
      "            Array with sample weights as applied to the original y.\n",
      "    \n",
      "    estimator_html_repr(estimator)\n",
      "        Build a HTML representation of an estimator.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <visualizing_composite_estimators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object\n",
      "            The estimator to visualize.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        html: str\n",
      "            HTML representation of estimator.\n",
      "    \n",
      "    indexable(*iterables)\n",
      "        Make arrays indexable for cross-validation.\n",
      "        \n",
      "        Checks consistent length, passes through None, and ensures that everything\n",
      "        can be indexed by converting sparse matrices to csr and converting\n",
      "        non-interable objects to arrays.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        *iterables : {lists, dataframes, ndarrays, sparse matrices}\n",
      "            List of objects to ensure sliceability.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        result : list of {ndarray, sparse matrix, dataframe} or None\n",
      "            Returns a list containing indexable arrays (i.e. NumPy array,\n",
      "            sparse matrix, or dataframe) or `None`.\n",
      "    \n",
      "    indices_to_mask(indices, mask_length)\n",
      "        Convert list of indices to boolean mask.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        indices : list-like\n",
      "            List of integers treated as indices.\n",
      "        mask_length : int\n",
      "            Length of boolean mask to be generated.\n",
      "            This parameter must be greater than max(indices).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mask : 1d boolean nd-array\n",
      "            Boolean array that is True where indices are present, else False.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.utils import indices_to_mask\n",
      "        >>> indices = [1, 2 , 3, 4]\n",
      "        >>> indices_to_mask(indices, 5)\n",
      "        array([False,  True,  True,  True,  True])\n",
      "    \n",
      "    murmurhash3_32(...)\n",
      "        Compute the 32bit murmurhash3 of key at seed.\n",
      "        \n",
      "        The underlying implementation is MurmurHash3_x86_32 generating low\n",
      "        latency 32bits hash suitable for implementing lookup tables, Bloom\n",
      "        filters, count min sketch or feature hashing.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        key : np.int32, bytes, unicode or ndarray of dtype=np.int32\n",
      "            The physical object to hash.\n",
      "        \n",
      "        seed : int, default=0\n",
      "            Integer seed for the hashing algorithm.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            True: the results is casted to an unsigned int\n",
      "              from 0 to 2 ** 32 - 1\n",
      "            False: the results is casted to a signed int\n",
      "              from -(2 ** 31) to 2 ** 31 - 1\n",
      "    \n",
      "    register_parallel_backend(name, factory, make_default=False)\n",
      "        Register a new Parallel backend factory.\n",
      "        \n",
      "        The new backend can then be selected by passing its name as the backend\n",
      "        argument to the Parallel class. Moreover, the default backend can be\n",
      "        overwritten globally by setting make_default=True.\n",
      "        \n",
      "        The factory can be any callable that takes no argument and return an\n",
      "        instance of ``ParallelBackendBase``.\n",
      "        \n",
      "        Warning: this function is experimental and subject to change in a future\n",
      "        version of joblib.\n",
      "        \n",
      "        .. versionadded:: 0.10\n",
      "    \n",
      "    resample(*arrays, replace=True, n_samples=None, random_state=None, stratify=None)\n",
      "        Resample arrays or sparse matrices in a consistent way.\n",
      "        \n",
      "        The default strategy implements one step of the bootstrapping\n",
      "        procedure.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        *arrays : sequence of array-like of shape (n_samples,) or             (n_samples, n_outputs)\n",
      "            Indexable data-structures can be arrays, lists, dataframes or scipy\n",
      "            sparse matrices with consistent first dimension.\n",
      "        \n",
      "        replace : bool, default=True\n",
      "            Implements resampling with replacement. If False, this will implement\n",
      "            (sliced) random permutations.\n",
      "        \n",
      "        n_samples : int, default=None\n",
      "            Number of samples to generate. If left to None this is\n",
      "            automatically set to the first dimension of the arrays.\n",
      "            If replace is False it should not be larger than the length of\n",
      "            arrays.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for shuffling\n",
      "            the data.\n",
      "            Pass an int for reproducible results across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        stratify : array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None\n",
      "            If not None, data is split in a stratified fashion, using this as\n",
      "            the class labels.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        resampled_arrays : sequence of array-like of shape (n_samples,) or             (n_samples, n_outputs)\n",
      "            Sequence of resampled copies of the collections. The original arrays\n",
      "            are not impacted.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        It is possible to mix sparse and dense arrays in the same run::\n",
      "        \n",
      "          >>> import numpy as np\n",
      "          >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n",
      "          >>> y = np.array([0, 1, 2])\n",
      "        \n",
      "          >>> from scipy.sparse import coo_matrix\n",
      "          >>> X_sparse = coo_matrix(X)\n",
      "        \n",
      "          >>> from sklearn.utils import resample\n",
      "          >>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0)\n",
      "          >>> X\n",
      "          array([[1., 0.],\n",
      "                 [2., 1.],\n",
      "                 [1., 0.]])\n",
      "        \n",
      "          >>> X_sparse\n",
      "          <3x2 sparse matrix of type '<... 'numpy.float64'>'\n",
      "              with 4 stored elements in Compressed Sparse Row format>\n",
      "        \n",
      "          >>> X_sparse.toarray()\n",
      "          array([[1., 0.],\n",
      "                 [2., 1.],\n",
      "                 [1., 0.]])\n",
      "        \n",
      "          >>> y\n",
      "          array([0, 1, 0])\n",
      "        \n",
      "          >>> resample(y, n_samples=2, random_state=0)\n",
      "          array([0, 1])\n",
      "        \n",
      "        Example using stratification::\n",
      "        \n",
      "          >>> y = [0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "          >>> resample(y, n_samples=5, replace=False, stratify=y,\n",
      "          ...          random_state=0)\n",
      "          [1, 1, 1, 0, 1]\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        shuffle\n",
      "    \n",
      "    shuffle(*arrays, random_state=None, n_samples=None)\n",
      "        Shuffle arrays or sparse matrices in a consistent way.\n",
      "        \n",
      "        This is a convenience alias to ``resample(*arrays, replace=False)`` to do\n",
      "        random permutations of the collections.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        *arrays : sequence of indexable data-structures\n",
      "            Indexable data-structures can be arrays, lists, dataframes or scipy\n",
      "            sparse matrices with consistent first dimension.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for shuffling\n",
      "            the data.\n",
      "            Pass an int for reproducible results across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        n_samples : int, default=None\n",
      "            Number of samples to generate. If left to None this is\n",
      "            automatically set to the first dimension of the arrays.  It should\n",
      "            not be larger than the length of arrays.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        shuffled_arrays : sequence of indexable data-structures\n",
      "            Sequence of shuffled copies of the collections. The original arrays\n",
      "            are not impacted.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        It is possible to mix sparse and dense arrays in the same run::\n",
      "        \n",
      "          >>> import numpy as np\n",
      "          >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n",
      "          >>> y = np.array([0, 1, 2])\n",
      "        \n",
      "          >>> from scipy.sparse import coo_matrix\n",
      "          >>> X_sparse = coo_matrix(X)\n",
      "        \n",
      "          >>> from sklearn.utils import shuffle\n",
      "          >>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)\n",
      "          >>> X\n",
      "          array([[0., 0.],\n",
      "                 [2., 1.],\n",
      "                 [1., 0.]])\n",
      "        \n",
      "          >>> X_sparse\n",
      "          <3x2 sparse matrix of type '<... 'numpy.float64'>'\n",
      "              with 3 stored elements in Compressed Sparse Row format>\n",
      "        \n",
      "          >>> X_sparse.toarray()\n",
      "          array([[0., 0.],\n",
      "                 [2., 1.],\n",
      "                 [1., 0.]])\n",
      "        \n",
      "          >>> y\n",
      "          array([2, 1, 0])\n",
      "        \n",
      "          >>> shuffle(y, n_samples=2, random_state=0)\n",
      "          array([0, 1])\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        resample\n",
      "\n",
      "DATA\n",
      "    __all__ = ['murmurhash3_32', 'as_float_array', 'assert_all_finite', 'c...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ea6577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.metrics in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.metrics\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.metrics` module includes score functions, performance metrics\n",
      "    and pairwise metrics and distance computations.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _base\n",
      "    _classification\n",
      "    _dist_metrics\n",
      "    _pairwise_fast\n",
      "    _plot (package)\n",
      "    _ranking\n",
      "    _regression\n",
      "    _scorer\n",
      "    cluster (package)\n",
      "    pairwise\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        sklearn.metrics._dist_metrics.DistanceMetric\n",
      "        sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay\n",
      "        sklearn.metrics._plot.det_curve.DetCurveDisplay\n",
      "        sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay\n",
      "        sklearn.metrics._plot.roc_curve.RocCurveDisplay\n",
      "    \n",
      "    class ConfusionMatrixDisplay(builtins.object)\n",
      "     |  ConfusionMatrixDisplay(confusion_matrix, *, display_labels=None)\n",
      "     |  \n",
      "     |  Confusion Matrix visualization.\n",
      "     |  \n",
      "     |  It is recommend to use\n",
      "     |  :func:`~sklearn.metrics.ConfusionMatrixDisplay.from_estimator` or\n",
      "     |  :func:`~sklearn.metrics.ConfusionMatrixDisplay.from_predictions` to\n",
      "     |  create a :class:`ConfusionMatrixDisplay`. All parameters are stored as\n",
      "     |  attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  confusion_matrix : ndarray of shape (n_classes, n_classes)\n",
      "     |      Confusion matrix.\n",
      "     |  \n",
      "     |  display_labels : ndarray of shape (n_classes,), default=None\n",
      "     |      Display labels for plot. If None, display labels are set from 0 to\n",
      "     |      `n_classes - 1`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  im_ : matplotlib AxesImage\n",
      "     |      Image representing the confusion matrix.\n",
      "     |  \n",
      "     |  text_ : ndarray of shape (n_classes, n_classes), dtype=matplotlib Text,             or None\n",
      "     |      Array of matplotlib axes. `None` if `include_values` is false.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with confusion matrix.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the confusion matrix.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  confusion_matrix : Compute Confusion Matrix to evaluate the accuracy of a\n",
      "     |      classification.\n",
      "     |  ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\n",
      "     |      given an estimator, the data, and the label.\n",
      "     |  ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\n",
      "     |      given the true and predicted labels.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> X, y = make_classification(random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
      "     |  ...                                                     random_state=0)\n",
      "     |  >>> clf = SVC(random_state=0)\n",
      "     |  >>> clf.fit(X_train, y_train)\n",
      "     |  SVC(random_state=0)\n",
      "     |  >>> predictions = clf.predict(X_test)\n",
      "     |  >>> cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n",
      "     |  >>> disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
      "     |  ...                               display_labels=clf.classes_)\n",
      "     |  >>> disp.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, confusion_matrix, *, display_labels=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, *, include_values=True, cmap='viridis', xticks_rotation='horizontal', values_format=None, ax=None, colorbar=True)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      include_values : bool, default=True\n",
      "     |          Includes values in confusion matrix.\n",
      "     |      \n",
      "     |      cmap : str or matplotlib Colormap, default='viridis'\n",
      "     |          Colormap recognized by matplotlib.\n",
      "     |      \n",
      "     |      xticks_rotation : {'vertical', 'horizontal'} or float,                          default='horizontal'\n",
      "     |          Rotation of xtick labels.\n",
      "     |      \n",
      "     |      values_format : str, default=None\n",
      "     |          Format specification for values in confusion matrix. If `None`,\n",
      "     |          the format specification is 'd' or '.2g' whichever is shorter.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      colorbar : bool, default=True\n",
      "     |          Whether or not to add a colorbar to the plot.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, labels=None, sample_weight=None, normalize=None, display_labels=None, include_values=True, xticks_rotation='horizontal', values_format=None, cmap='viridis', ax=None, colorbar=True) from builtins.type\n",
      "     |      Plot Confusion Matrix given an estimator and some data.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a classifier.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      labels : array-like of shape (n_classes,), default=None\n",
      "     |          List of labels to index the confusion matrix. This may be used to\n",
      "     |          reorder or select a subset of labels. If `None` is given, those\n",
      "     |          that appear at least once in `y_true` or `y_pred` are used in\n",
      "     |          sorted order.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      normalize : {'true', 'pred', 'all'}, default=None\n",
      "     |          Either to normalize the counts display in the matrix:\n",
      "     |      \n",
      "     |          - if `'true'`, the confusion matrix is normalized over the true\n",
      "     |            conditions (e.g. rows);\n",
      "     |          - if `'pred'`, the confusion matrix is normalized over the\n",
      "     |            predicted conditions (e.g. columns);\n",
      "     |          - if `'all'`, the confusion matrix is normalized by the total\n",
      "     |            number of samples;\n",
      "     |          - if `None` (default), the confusion matrix will not be normalized.\n",
      "     |      \n",
      "     |      display_labels : array-like of shape (n_classes,), default=None\n",
      "     |          Target names used for plotting. By default, `labels` will be used\n",
      "     |          if it is defined, otherwise the unique labels of `y_true` and\n",
      "     |          `y_pred` will be used.\n",
      "     |      \n",
      "     |      include_values : bool, default=True\n",
      "     |          Includes values in confusion matrix.\n",
      "     |      \n",
      "     |      xticks_rotation : {'vertical', 'horizontal'} or float,                 default='horizontal'\n",
      "     |          Rotation of xtick labels.\n",
      "     |      \n",
      "     |      values_format : str, default=None\n",
      "     |          Format specification for values in confusion matrix. If `None`, the\n",
      "     |          format specification is 'd' or '.2g' whichever is shorter.\n",
      "     |      \n",
      "     |      cmap : str or matplotlib Colormap, default='viridis'\n",
      "     |          Colormap recognized by matplotlib.\n",
      "     |      \n",
      "     |      ax : matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      colorbar : bool, default=True\n",
      "     |          Whether or not to add a colorbar to the plot.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\n",
      "     |          given the true and predicted labels.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import ConfusionMatrixDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...         X, y, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0)\n",
      "     |      >>> clf.fit(X_train, y_train)\n",
      "     |      SVC(random_state=0)\n",
      "     |      >>> ConfusionMatrixDisplay.from_estimator(\n",
      "     |      ...     clf, X_test, y_test)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None, display_labels=None, include_values=True, xticks_rotation='horizontal', values_format=None, cmap='viridis', ax=None, colorbar=True) from builtins.type\n",
      "     |      Plot Confusion Matrix given true and predicted labels.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.24\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True labels.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          The predicted labels given by the method `predict` of an\n",
      "     |          classifier.\n",
      "     |      \n",
      "     |      labels : array-like of shape (n_classes,), default=None\n",
      "     |          List of labels to index the confusion matrix. This may be used to\n",
      "     |          reorder or select a subset of labels. If `None` is given, those\n",
      "     |          that appear at least once in `y_true` or `y_pred` are used in\n",
      "     |          sorted order.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      normalize : {'true', 'pred', 'all'}, default=None\n",
      "     |          Either to normalize the counts display in the matrix:\n",
      "     |      \n",
      "     |          - if `'true'`, the confusion matrix is normalized over the true\n",
      "     |            conditions (e.g. rows);\n",
      "     |          - if `'pred'`, the confusion matrix is normalized over the\n",
      "     |            predicted conditions (e.g. columns);\n",
      "     |          - if `'all'`, the confusion matrix is normalized by the total\n",
      "     |            number of samples;\n",
      "     |          - if `None` (default), the confusion matrix will not be normalized.\n",
      "     |      \n",
      "     |      display_labels : array-like of shape (n_classes,), default=None\n",
      "     |          Target names used for plotting. By default, `labels` will be used\n",
      "     |          if it is defined, otherwise the unique labels of `y_true` and\n",
      "     |          `y_pred` will be used.\n",
      "     |      \n",
      "     |      include_values : bool, default=True\n",
      "     |          Includes values in confusion matrix.\n",
      "     |      \n",
      "     |      xticks_rotation : {'vertical', 'horizontal'} or float,                 default='horizontal'\n",
      "     |          Rotation of xtick labels.\n",
      "     |      \n",
      "     |      values_format : str, default=None\n",
      "     |          Format specification for values in confusion matrix. If `None`, the\n",
      "     |          format specification is 'd' or '.2g' whichever is shorter.\n",
      "     |      \n",
      "     |      cmap : str or matplotlib Colormap, default='viridis'\n",
      "     |          Colormap recognized by matplotlib.\n",
      "     |      \n",
      "     |      ax : matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      colorbar : bool, default=True\n",
      "     |          Whether or not to add a colorbar to the plot.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\n",
      "     |          given an estimator, the data, and the label.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import ConfusionMatrixDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...         X, y, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0)\n",
      "     |      >>> clf.fit(X_train, y_train)\n",
      "     |      SVC(random_state=0)\n",
      "     |      >>> y_pred = clf.predict(X_test)\n",
      "     |      >>> ConfusionMatrixDisplay.from_predictions(\n",
      "     |      ...    y_test, y_pred)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DetCurveDisplay(builtins.object)\n",
      "     |  DetCurveDisplay(*, fpr, fnr, estimator_name=None, pos_label=None)\n",
      "     |  \n",
      "     |  DET curve visualization.\n",
      "     |  \n",
      "     |  It is recommend to use :func:`~sklearn.metrics.DetCurveDisplay.from_estimator`\n",
      "     |  or :func:`~sklearn.metrics.DetCurveDisplay.from_predictions` to create a\n",
      "     |  visualizer. All parameters are stored as attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fpr : ndarray\n",
      "     |      False positive rate.\n",
      "     |  \n",
      "     |  fnr : ndarray\n",
      "     |      False negative rate.\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, the estimator name is not shown.\n",
      "     |  \n",
      "     |  pos_label : str or int, default=None\n",
      "     |      The label of the positive class.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      DET Curve.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with DET Curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  det_curve : Compute error rates for different probability thresholds.\n",
      "     |  DetCurveDisplay.from_estimator : Plot DET curve given an estimator and\n",
      "     |      some data.\n",
      "     |  DetCurveDisplay.from_predictions : Plot DET curve given the true and\n",
      "     |      predicted labels.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.metrics import det_curve, DetCurveDisplay\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> X, y = make_classification(n_samples=1000, random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |  ...     X, y, test_size=0.4, random_state=0)\n",
      "     |  >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |  >>> y_pred = clf.decision_function(X_test)\n",
      "     |  >>> fpr, fnr, _ = det_curve(y_test, y_pred)\n",
      "     |  >>> display = DetCurveDisplay(\n",
      "     |  ...     fpr=fpr, fnr=fnr, estimator_name=\"SVC\"\n",
      "     |  ... )\n",
      "     |  >>> display.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fpr, fnr, estimator_name=None, pos_label=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, **kwargs)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of DET curve for labeling. If `None`, use `estimator_name` if\n",
      "     |          it is not `None`, otherwise no labeling is shown.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Additional keywords arguments passed to matplotlib `plot` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.plot.DetCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, sample_weight=None, response_method='auto', pos_label=None, name=None, ax=None, **kwargs) from builtins.type\n",
      "     |      Plot DET curve given an estimator and data.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a classifier.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      response_method : {'predict_proba', 'decision_function', 'auto'}                 default='auto'\n",
      "     |          Specifies whether to use :term:`predict_proba` or\n",
      "     |          :term:`decision_function` as the predicted target response. If set\n",
      "     |          to 'auto', :term:`predict_proba` is tried first and if it does not\n",
      "     |          exist :term:`decision_function` is tried next.\n",
      "     |      \n",
      "     |      pos_label : str or int, default=None\n",
      "     |          The label of the positive class. When `pos_label=None`, if `y_true`\n",
      "     |          is in {-1, 1} or {0, 1}, `pos_label` is set to 1, otherwise an\n",
      "     |          error will be raised.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of DET curve for labeling. If `None`, use the name of the\n",
      "     |          estimator.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Additional keywords arguments passed to matplotlib `plot` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.DetCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      det_curve : Compute error rates for different probability thresholds.\n",
      "     |      DetCurveDisplay.from_predictions : Plot DET curve given the true and\n",
      "     |          predicted labels.\n",
      "     |      plot_roc_curve : Plot Receiver operating characteristic (ROC) curve.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import DetCurveDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(n_samples=1000, random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...     X, y, test_size=0.4, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |      >>> DetCurveDisplay.from_estimator(\n",
      "     |      ...    clf, X_test, y_test)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, sample_weight=None, pos_label=None, name=None, ax=None, **kwargs) from builtins.type\n",
      "     |      Plot DET curve given the true and\n",
      "     |      predicted labels.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True labels.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          Target scores, can either be probability estimates of the positive\n",
      "     |          class, confidence values, or non-thresholded measure of decisions\n",
      "     |          (as returned by `decision_function` on some classifiers).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      pos_label : str or int, default=None\n",
      "     |          The label of the positive class. When `pos_label=None`, if `y_true`\n",
      "     |          is in {-1, 1} or {0, 1}, `pos_label` is set to 1, otherwise an\n",
      "     |          error will be raised.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of DET curve for labeling. If `None`, name will be set to\n",
      "     |          `\"Classifier\"`.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Additional keywords arguments passed to matplotlib `plot` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.DetCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      det_curve : Compute error rates for different probability thresholds.\n",
      "     |      DetCurveDisplay.from_estimator : Plot DET curve given an estimator and\n",
      "     |          some data.\n",
      "     |      plot_roc_curve : Plot Receiver operating characteristic (ROC) curve.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import DetCurveDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(n_samples=1000, random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...     X, y, test_size=0.4, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |      >>> y_pred = clf.decision_function(X_test)\n",
      "     |      >>> DetCurveDisplay.from_predictions(\n",
      "     |      ...    y_test, y_pred)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DistanceMetric(builtins.object)\n",
      "     |  DistanceMetric class\n",
      "     |  \n",
      "     |  This class provides a uniform interface to fast distance metric\n",
      "     |  functions.  The various metrics can be accessed via the :meth:`get_metric`\n",
      "     |  class method and the metric string identifier (see below).\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.metrics import DistanceMetric\n",
      "     |  >>> dist = DistanceMetric.get_metric('euclidean')\n",
      "     |  >>> X = [[0, 1, 2],\n",
      "     |           [3, 4, 5]]\n",
      "     |  >>> dist.pairwise(X)\n",
      "     |  array([[ 0.        ,  5.19615242],\n",
      "     |         [ 5.19615242,  0.        ]])\n",
      "     |  \n",
      "     |  Available Metrics\n",
      "     |  \n",
      "     |  The following lists the string metric identifiers and the associated\n",
      "     |  distance metric classes:\n",
      "     |  \n",
      "     |  **Metrics intended for real-valued vector spaces:**\n",
      "     |  \n",
      "     |  ==============  ====================  ========  ===============================\n",
      "     |  identifier      class name            args      distance function\n",
      "     |  --------------  --------------------  --------  -------------------------------\n",
      "     |  \"euclidean\"     EuclideanDistance     -         ``sqrt(sum((x - y)^2))``\n",
      "     |  \"manhattan\"     ManhattanDistance     -         ``sum(|x - y|)``\n",
      "     |  \"chebyshev\"     ChebyshevDistance     -         ``max(|x - y|)``\n",
      "     |  \"minkowski\"     MinkowskiDistance     p, w      ``sum(w * |x - y|^p)^(1/p)``\n",
      "     |  \"wminkowski\"    WMinkowskiDistance    p, w      ``sum(|w * (x - y)|^p)^(1/p)``\n",
      "     |  \"seuclidean\"    SEuclideanDistance    V         ``sqrt(sum((x - y)^2 / V))``\n",
      "     |  \"mahalanobis\"   MahalanobisDistance   V or VI   ``sqrt((x - y)' V^-1 (x - y))``\n",
      "     |  ==============  ====================  ========  ===============================\n",
      "     |  \n",
      "     |  Note that \"minkowski\" with a non-None `w` parameter actually calls\n",
      "     |  `WMinkowskiDistance` with `w=w ** (1/p)` in order to be consistent with the\n",
      "     |  parametrization of scipy 1.8 and later.\n",
      "     |  \n",
      "     |  **Metrics intended for two-dimensional vector spaces:**  Note that the haversine\n",
      "     |  distance metric requires data in the form of [latitude, longitude] and both\n",
      "     |  inputs and outputs are in units of radians.\n",
      "     |  \n",
      "     |  ============  ==================  ===============================================================\n",
      "     |  identifier    class name          distance function\n",
      "     |  ------------  ------------------  ---------------------------------------------------------------\n",
      "     |  \"haversine\"   HaversineDistance   ``2 arcsin(sqrt(sin^2(0.5*dx) + cos(x1)cos(x2)sin^2(0.5*dy)))``\n",
      "     |  ============  ==================  ===============================================================\n",
      "     |  \n",
      "     |  \n",
      "     |  **Metrics intended for integer-valued vector spaces:**  Though intended\n",
      "     |  for integer-valued vectors, these are also valid metrics in the case of\n",
      "     |  real-valued vectors.\n",
      "     |  \n",
      "     |  =============  ====================  ========================================\n",
      "     |  identifier     class name            distance function\n",
      "     |  -------------  --------------------  ----------------------------------------\n",
      "     |  \"hamming\"      HammingDistance       ``N_unequal(x, y) / N_tot``\n",
      "     |  \"canberra\"     CanberraDistance      ``sum(|x - y| / (|x| + |y|))``\n",
      "     |  \"braycurtis\"   BrayCurtisDistance    ``sum(|x - y|) / (sum(|x|) + sum(|y|))``\n",
      "     |  =============  ====================  ========================================\n",
      "     |  \n",
      "     |  **Metrics intended for boolean-valued vector spaces:**  Any nonzero entry\n",
      "     |  is evaluated to \"True\".  In the listings below, the following\n",
      "     |  abbreviations are used:\n",
      "     |  \n",
      "     |   - N  : number of dimensions\n",
      "     |   - NTT : number of dims in which both values are True\n",
      "     |   - NTF : number of dims in which the first value is True, second is False\n",
      "     |   - NFT : number of dims in which the first value is False, second is True\n",
      "     |   - NFF : number of dims in which both values are False\n",
      "     |   - NNEQ : number of non-equal dimensions, NNEQ = NTF + NFT\n",
      "     |   - NNZ : number of nonzero dimensions, NNZ = NTF + NFT + NTT\n",
      "     |  \n",
      "     |  =================  =======================  ===============================\n",
      "     |  identifier         class name               distance function\n",
      "     |  -----------------  -----------------------  -------------------------------\n",
      "     |  \"jaccard\"          JaccardDistance          NNEQ / NNZ\n",
      "     |  \"matching\"         MatchingDistance         NNEQ / N\n",
      "     |  \"dice\"             DiceDistance             NNEQ / (NTT + NNZ)\n",
      "     |  \"kulsinski\"        KulsinskiDistance        (NNEQ + N - NTT) / (NNEQ + N)\n",
      "     |  \"rogerstanimoto\"   RogersTanimotoDistance   2 * NNEQ / (N + NNEQ)\n",
      "     |  \"russellrao\"       RussellRaoDistance       (N - NTT) / N\n",
      "     |  \"sokalmichener\"    SokalMichenerDistance    2 * NNEQ / (N + NNEQ)\n",
      "     |  \"sokalsneath\"      SokalSneathDistance      NNEQ / (NNEQ + 0.5 * NTT)\n",
      "     |  =================  =======================  ===============================\n",
      "     |  \n",
      "     |  **User-defined distance:**\n",
      "     |  \n",
      "     |  ===========    ===============    =======\n",
      "     |  identifier     class name         args\n",
      "     |  -----------    ---------------    -------\n",
      "     |  \"pyfunc\"       PyFuncDistance     func\n",
      "     |  ===========    ===============    =======\n",
      "     |  \n",
      "     |  Here ``func`` is a function which takes two one-dimensional numpy\n",
      "     |  arrays, and returns a distance.  Note that in order to be used within\n",
      "     |  the BallTree, the distance must be a true metric:\n",
      "     |  i.e. it must satisfy the following properties\n",
      "     |  \n",
      "     |  1) Non-negativity: d(x, y) >= 0\n",
      "     |  2) Identity: d(x, y) = 0 if and only if x == y\n",
      "     |  3) Symmetry: d(x, y) = d(y, x)\n",
      "     |  4) Triangle Inequality: d(x, y) + d(y, z) >= d(x, z)\n",
      "     |  \n",
      "     |  Because of the Python object overhead involved in calling the python\n",
      "     |  function, this will be fairly slow, but it will have the same\n",
      "     |  scaling as other distances.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getstate__(...)\n",
      "     |      get state for pickling\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      reduce method used for pickling\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |      set state for pickling\n",
      "     |  \n",
      "     |  dist_to_rdist(...)\n",
      "     |      Convert the true distance to the rank-preserving surrogate distance.\n",
      "     |      \n",
      "     |      The surrogate distance is any measure that yields the same rank as the\n",
      "     |      distance, but is more efficient to compute. For example, for the\n",
      "     |      Euclidean metric, the surrogate distance is the squared-euclidean distance.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dist : double\n",
      "     |          True distance.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          Surrogate distance.\n",
      "     |  \n",
      "     |  pairwise(...)\n",
      "     |      Compute the pairwise distances between X and Y\n",
      "     |      \n",
      "     |      This is a convenience routine for the sake of testing.  For many\n",
      "     |      metrics, the utilities in scipy.spatial.distance.cdist and\n",
      "     |      scipy.spatial.distance.pdist will be faster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like\n",
      "     |          Array of shape (Nx, D), representing Nx points in D dimensions.\n",
      "     |      Y : array-like (optional)\n",
      "     |          Array of shape (Ny, D), representing Ny points in D dimensions.\n",
      "     |          If not specified, then Y=X.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dist : ndarray\n",
      "     |          The shape (Nx, Ny) array of pairwise distances between points in\n",
      "     |          X and Y.\n",
      "     |  \n",
      "     |  rdist_to_dist(...)\n",
      "     |      Convert the rank-preserving surrogate distance to the distance.\n",
      "     |      \n",
      "     |      The surrogate distance is any measure that yields the same rank as the\n",
      "     |      distance, but is more efficient to compute. For example, for the\n",
      "     |      Euclidean metric, the surrogate distance is the squared-euclidean distance.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      rdist : double\n",
      "     |          Surrogate distance.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          True distance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  get_metric(...) from builtins.type\n",
      "     |      Get the given distance metric from the string identifier.\n",
      "     |      \n",
      "     |      See the docstring of DistanceMetric for a list of available metrics.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      metric : str or class name\n",
      "     |          The distance metric to use\n",
      "     |      **kwargs\n",
      "     |          additional arguments will be passed to the requested metric\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "    \n",
      "    class PrecisionRecallDisplay(builtins.object)\n",
      "     |  PrecisionRecallDisplay(precision, recall, *, average_precision=None, estimator_name=None, pos_label=None)\n",
      "     |  \n",
      "     |  Precision Recall visualization.\n",
      "     |  \n",
      "     |  It is recommend to use\n",
      "     |  :func:`~sklearn.metrics.PrecisionRecallDisplay.from_estimator` or\n",
      "     |  :func:`~sklearn.metrics.PrecisionRecallDisplay.from_predictions` to create\n",
      "     |  a :class:`~sklearn.metrics.PredictionRecallDisplay`. All parameters are\n",
      "     |  stored as attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  -----------\n",
      "     |  precision : ndarray\n",
      "     |      Precision values.\n",
      "     |  \n",
      "     |  recall : ndarray\n",
      "     |      Recall values.\n",
      "     |  \n",
      "     |  average_precision : float, default=None\n",
      "     |      Average precision. If None, the average precision is not shown.\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, then the estimator name is not shown.\n",
      "     |  \n",
      "     |  pos_label : str or int, default=None\n",
      "     |      The class considered as the positive class. If None, the class will not\n",
      "     |      be shown in the legend.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      Precision recall curve.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with precision recall curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  precision_recall_curve : Compute precision-recall pairs for different\n",
      "     |      probability thresholds.\n",
      "     |  PrecisionRecallDisplay.from_estimator : Plot Precision Recall Curve given\n",
      "     |      a binary classifier.\n",
      "     |  PrecisionRecallDisplay.from_predictions : Plot Precision Recall Curve\n",
      "     |      using predictions from a binary classifier.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.metrics import (precision_recall_curve,\n",
      "     |  ...                              PrecisionRecallDisplay)\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> X, y = make_classification(random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
      "     |  ...                                                     random_state=0)\n",
      "     |  >>> clf = SVC(random_state=0)\n",
      "     |  >>> clf.fit(X_train, y_train)\n",
      "     |  SVC(random_state=0)\n",
      "     |  >>> predictions = clf.predict(X_test)\n",
      "     |  >>> precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
      "     |  >>> disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
      "     |  >>> disp.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, precision, recall, *, average_precision=None, estimator_name=None, pos_label=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, **kwargs)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Extra keyword arguments will be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : Matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of precision recall curve for labeling. If `None`, use\n",
      "     |          `estimator_name` if not `None`, otherwise no labeling is shown.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, sample_weight=None, pos_label=None, response_method='auto', name=None, ax=None, **kwargs) from builtins.type\n",
      "     |      Plot precision-recall curve given an estimator and some data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a classifier.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      pos_label : str or int, default=None\n",
      "     |          The class considered as the positive class when computing the\n",
      "     |          precision and recall metrics. By default, `estimators.classes_[1]`\n",
      "     |          is considered as the positive class.\n",
      "     |      \n",
      "     |      response_method : {'predict_proba', 'decision_function', 'auto'},             default='auto'\n",
      "     |          Specifies whether to use :term:`predict_proba` or\n",
      "     |          :term:`decision_function` as the target response. If set to 'auto',\n",
      "     |          :term:`predict_proba` is tried first and if it does not exist\n",
      "     |          :term:`decision_function` is tried next.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name for labeling curve. If `None`, no name is used.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      PrecisionRecallDisplay.from_predictions : Plot precision-recall curve\n",
      "     |          using estimated probabilities or output of decision function.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import PrecisionRecallDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.linear_model import LogisticRegression\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...         X, y, random_state=0)\n",
      "     |      >>> clf = LogisticRegression()\n",
      "     |      >>> clf.fit(X_train, y_train)\n",
      "     |      LogisticRegression()\n",
      "     |      >>> PrecisionRecallDisplay.from_estimator(\n",
      "     |      ...    clf, X_test, y_test)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, sample_weight=None, pos_label=None, name=None, ax=None, **kwargs) from builtins.type\n",
      "     |      Plot precision-recall curve given binary class predictions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True binary labels.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          Estimated probabilities or output of decision function.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      pos_label : str or int, default=None\n",
      "     |          The class considered as the positive class when computing the\n",
      "     |          precision and recall metrics.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name for labeling curve. If `None`, name will be set to\n",
      "     |          `\"Classifier\"`.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      PrecisionRecallDisplay.from_estimator : Plot precision-recall curve\n",
      "     |          using an estimator.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import PrecisionRecallDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.linear_model import LogisticRegression\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...         X, y, random_state=0)\n",
      "     |      >>> clf = LogisticRegression()\n",
      "     |      >>> clf.fit(X_train, y_train)\n",
      "     |      LogisticRegression()\n",
      "     |      >>> y_pred = clf.predict_proba(X_test)[:, 1]\n",
      "     |      >>> PrecisionRecallDisplay.from_predictions(\n",
      "     |      ...    y_test, y_pred)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class RocCurveDisplay(builtins.object)\n",
      "     |  RocCurveDisplay(*, fpr, tpr, roc_auc=None, estimator_name=None, pos_label=None)\n",
      "     |  \n",
      "     |  ROC Curve visualization.\n",
      "     |  \n",
      "     |  It is recommend to use\n",
      "     |  :func:`~sklearn.metrics.RocCurveDisplay.from_estimator` or\n",
      "     |  :func:`~sklearn.metrics.RocCurveDisplay.from_predictions` to create\n",
      "     |  a :class:`~sklearn.metrics.RocCurveDisplay`. All parameters are\n",
      "     |  stored as attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fpr : ndarray\n",
      "     |      False positive rate.\n",
      "     |  \n",
      "     |  tpr : ndarray\n",
      "     |      True positive rate.\n",
      "     |  \n",
      "     |  roc_auc : float, default=None\n",
      "     |      Area under ROC curve. If None, the roc_auc score is not shown.\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, the estimator name is not shown.\n",
      "     |  \n",
      "     |  pos_label : str or int, default=None\n",
      "     |      The class considered as the positive class when computing the roc auc\n",
      "     |      metrics. By default, `estimators.classes_[1]` is considered\n",
      "     |      as the positive class.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      ROC Curve.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with ROC Curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "     |  RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n",
      "     |      (ROC) curve given an estimator and some data.\n",
      "     |  RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n",
      "     |      (ROC) curve given the true and predicted values.\n",
      "     |  roc_auc_score : Compute the area under the ROC curve.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn import metrics\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "     |  >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
      "     |  >>> roc_auc = metrics.auc(fpr, tpr)\n",
      "     |  >>> display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
      "     |  ...                                   estimator_name='example estimator')\n",
      "     |  >>> display.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fpr, tpr, roc_auc=None, estimator_name=None, pos_label=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, **kwargs)\n",
      "     |      Plot visualization\n",
      "     |      \n",
      "     |      Extra keyword arguments will be passed to matplotlib's ``plot``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of ROC Curve for labeling. If `None`, use `estimator_name` if\n",
      "     |          not `None`, otherwise no labeling is shown.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.plot.RocCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, sample_weight=None, drop_intermediate=True, response_method='auto', pos_label=None, name=None, ax=None, **kwargs) from builtins.type\n",
      "     |      Create a ROC Curve display from an estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a classifier.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      drop_intermediate : bool, default=True\n",
      "     |          Whether to drop some suboptimal thresholds which would not appear\n",
      "     |          on a plotted ROC curve. This is useful in order to create lighter\n",
      "     |          ROC curves.\n",
      "     |      \n",
      "     |      response_method : {'predict_proba', 'decision_function', 'auto'}                 default='auto'\n",
      "     |          Specifies whether to use :term:`predict_proba` or\n",
      "     |          :term:`decision_function` as the target response. If set to 'auto',\n",
      "     |          :term:`predict_proba` is tried first and if it does not exist\n",
      "     |          :term:`decision_function` is tried next.\n",
      "     |      \n",
      "     |      pos_label : str or int, default=None\n",
      "     |          The class considered as the positive class when computing the roc auc\n",
      "     |          metrics. By default, `estimators.classes_[1]` is considered\n",
      "     |          as the positive class.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of ROC Curve for labeling. If `None`, use the name of the\n",
      "     |          estimator.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.plot.RocCurveDisplay`\n",
      "     |          The ROC Curve display.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "     |      RocCurveDisplay.from_predictions : ROC Curve visualization given the\n",
      "     |          probabilities of scores of a classifier.\n",
      "     |      roc_auc_score : Compute the area under the ROC curve.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import RocCurveDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...     X, y, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |      >>> RocCurveDisplay.from_estimator(\n",
      "     |      ...    clf, X_test, y_test)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, sample_weight=None, drop_intermediate=True, pos_label=None, name=None, ax=None, **kwargs) from builtins.type\n",
      "     |      Plot ROC curve given the true and predicted values.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True labels.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          Target scores, can either be probability estimates of the positive\n",
      "     |          class, confidence values, or non-thresholded measure of decisions\n",
      "     |          (as returned by decision_function on some classifiers).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      drop_intermediate : bool, default=True\n",
      "     |          Whether to drop some suboptimal thresholds which would not appear\n",
      "     |          on a plotted ROC curve. This is useful in order to create lighter\n",
      "     |          ROC curves.\n",
      "     |      \n",
      "     |      pos_label : str or int, default=None\n",
      "     |          The label of the positive class. When `pos_label=None`, if `y_true`\n",
      "     |          is in {-1, 1} or {0, 1}, `pos_label` is set to 1, otherwise an\n",
      "     |          error will be raised.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of ROC curve for labeling. If `None`, name will be set to\n",
      "     |          `\"Classifier\"`.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Additional keywords arguments passed to matplotlib `plot` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.DetCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "     |      RocCurveDisplay.from_estimator : ROC Curve visualization given an\n",
      "     |          estimator and some data.\n",
      "     |      roc_auc_score : Compute the area under the ROC curve.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import RocCurveDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...     X, y, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |      >>> y_pred = clf.decision_function(X_test)\n",
      "     |      >>> RocCurveDisplay.from_predictions(\n",
      "     |      ...    y_test, y_pred)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)\n",
      "        Accuracy classification score.\n",
      "        \n",
      "        In multilabel classification, this function computes subset accuracy:\n",
      "        the set of labels predicted for a sample must *exactly* match the\n",
      "        corresponding set of labels in y_true.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <accuracy_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If ``False``, return the number of correctly classified samples.\n",
      "            Otherwise, return the fraction of correctly classified samples.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            If ``normalize == True``, return the fraction of correctly\n",
      "            classified samples (float), else returns the number of correctly\n",
      "            classified samples (int).\n",
      "        \n",
      "            The best performance is 1 with ``normalize == True`` and the number\n",
      "            of samples with ``normalize == False``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        balanced_accuracy_score : Compute the balanced accuracy to deal with\n",
      "            imbalanced datasets.\n",
      "        jaccard_score : Compute the Jaccard similarity coefficient score.\n",
      "        hamming_loss : Compute the average Hamming loss or Hamming distance between\n",
      "            two sets of samples.\n",
      "        zero_one_loss : Compute the Zero-one classification loss. By default, the\n",
      "            function will return the percentage of imperfectly predicted subsets.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In binary classification, this function is equal to the `jaccard_score`\n",
      "        function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import accuracy_score\n",
      "        >>> y_pred = [0, 2, 1, 3]\n",
      "        >>> y_true = [0, 1, 2, 3]\n",
      "        >>> accuracy_score(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> accuracy_score(y_true, y_pred, normalize=False)\n",
      "        2\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "    \n",
      "    adjusted_mutual_info_score(labels_true, labels_pred, *, average_method='arithmetic')\n",
      "        Adjusted Mutual Information between two clusterings.\n",
      "        \n",
      "        Adjusted Mutual Information (AMI) is an adjustment of the Mutual\n",
      "        Information (MI) score to account for chance. It accounts for the fact that\n",
      "        the MI is generally higher for two clusterings with a larger number of\n",
      "        clusters, regardless of whether there is actually more information shared.\n",
      "        For two clusterings :math:`U` and :math:`V`, the AMI is given as::\n",
      "        \n",
      "            AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching :math:`U` (``label_true``)\n",
      "        with :math:`V` (``labels_pred``) will return the same score value. This can\n",
      "        be useful to measure the agreement of two independent label assignments\n",
      "        strategies on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Be mindful that this function is an order of magnitude slower than other\n",
      "        metrics, such as the Adjusted Rand Index.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets, called :math:`U` in\n",
      "            the above formula.\n",
      "        \n",
      "        labels_pred : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets, called :math:`V` in\n",
      "            the above formula.\n",
      "        \n",
      "        average_method : str, default='arithmetic'\n",
      "            How to compute the normalizer in the denominator. Possible options\n",
      "            are 'min', 'geometric', 'arithmetic', and 'max'.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "               The default value of ``average_method`` changed from 'max' to\n",
      "               'arithmetic'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ami: float (upperlimited by 1.0)\n",
      "           The AMI returns a value of 1 when the two partitions are identical\n",
      "           (ie perfectly matched). Random partitions (independent labellings) have\n",
      "           an expected AMI around 0 on average hence can be negative. The value is\n",
      "           in adjusted nats (based on the natural logarithm).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_rand_score : Adjusted Rand Index.\n",
      "        mutual_info_score : Mutual Information (not adjusted for chance).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the AMI is null::\n",
      "        \n",
      "          >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          ... # doctest: +SKIP\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for\n",
      "           Clusterings Comparison: Variants, Properties, Normalization and\n",
      "           Correction for Chance, JMLR\n",
      "           <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Adjusted Mutual Information\n",
      "           <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_\n",
      "    \n",
      "    adjusted_rand_score(labels_true, labels_pred)\n",
      "        Rand index adjusted for chance.\n",
      "        \n",
      "        The Rand Index computes a similarity measure between two clusterings\n",
      "        by considering all pairs of samples and counting pairs that are\n",
      "        assigned in the same or different clusters in the predicted and\n",
      "        true clusterings.\n",
      "        \n",
      "        The raw RI score is then \"adjusted for chance\" into the ARI score\n",
      "        using the following scheme::\n",
      "        \n",
      "            ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n",
      "        \n",
      "        The adjusted Rand index is thus ensured to have a value close to\n",
      "        0.0 for random labeling independently of the number of clusters and\n",
      "        samples and exactly 1.0 when the clusterings are identical (up to\n",
      "        a permutation).\n",
      "        \n",
      "        ARI is a symmetric measure::\n",
      "        \n",
      "            adjusted_rand_score(a, b) == adjusted_rand_score(b, a)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <adjusted_rand_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            Ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            Cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ARI : float\n",
      "           Similarity score between -1.0 and 1.0. Random labelings have an ARI\n",
      "           close to 0.0. 1.0 stands for perfect match.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have a score of 1 even\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_rand_score\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may not always be pure, hence penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          0.57...\n",
      "        \n",
      "        ARI is symmetric, so labelings that have pure clusters with members\n",
      "        coming from the same classes but unnecessary splits are penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])\n",
      "          0.57...\n",
      "        \n",
      "        If classes members are completely split across different clusters, the\n",
      "        assignment is totally incomplete, hence the ARI is very low::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Hubert1985] L. Hubert and P. Arabie, Comparing Partitions,\n",
      "          Journal of Classification 1985\n",
      "          https://link.springer.com/article/10.1007%2FBF01908075\n",
      "        \n",
      "        .. [Steinley2004] D. Steinley, Properties of the Hubert-Arabie\n",
      "          adjusted Rand index, Psychological Methods 2004\n",
      "        \n",
      "        .. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_mutual_info_score : Adjusted Mutual Information.\n",
      "    \n",
      "    auc(x, y)\n",
      "        Compute Area Under the Curve (AUC) using the trapezoidal rule.\n",
      "        \n",
      "        This is a general function, given points on a curve.  For computing the\n",
      "        area under the ROC-curve, see :func:`roc_auc_score`.  For an alternative\n",
      "        way to summarize a precision-recall curve, see\n",
      "        :func:`average_precision_score`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : ndarray of shape (n,)\n",
      "            x coordinates. These must be either monotonic increasing or monotonic\n",
      "            decreasing.\n",
      "        y : ndarray of shape, (n,)\n",
      "            y coordinates.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        average_precision_score : Compute average precision from prediction scores.\n",
      "        precision_recall_curve : Compute precision-recall pairs for different\n",
      "            probability thresholds.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y = np.array([1, 1, 2, 2])\n",
      "        >>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
      "        >>> metrics.auc(fpr, tpr)\n",
      "        0.75\n",
      "    \n",
      "    average_precision_score(y_true, y_score, *, average='macro', pos_label=1, sample_weight=None)\n",
      "        Compute average precision (AP) from prediction scores.\n",
      "        \n",
      "        AP summarizes a precision-recall curve as the weighted mean of precisions\n",
      "        achieved at each threshold, with the increase in recall from the previous\n",
      "        threshold used as the weight:\n",
      "        \n",
      "        .. math::\n",
      "            \\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\n",
      "        \n",
      "        where :math:`P_n` and :math:`R_n` are the precision and recall at the nth\n",
      "        threshold [1]_. This implementation is not interpolated and is different\n",
      "        from computing the area under the precision-recall curve with the\n",
      "        trapezoidal rule, which uses linear interpolation and can be too\n",
      "        optimistic.\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task\n",
      "        or multilabel classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "            True binary labels or binary label indicators.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by :term:`decision_function` on some classifiers).\n",
      "        \n",
      "        average : {'micro', 'samples', 'weighted', 'macro'} or None,             default='macro'\n",
      "            If ``None``, the scores for each class are returned. Otherwise,\n",
      "            this determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "            Will be ignored when ``y_true`` is binary.\n",
      "        \n",
      "        pos_label : int or str, default=1\n",
      "            The label of the positive class. Only applied to binary ``y_true``.\n",
      "            For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        average_precision : float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        precision_recall_curve : Compute precision-recall pairs for different\n",
      "            probability thresholds.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionchanged:: 0.19\n",
      "          Instead of linearly interpolating between operating points, precisions\n",
      "          are weighted by the change in recall since the last operating point.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Average precision\n",
      "               <https://en.wikipedia.org/w/index.php?title=Information_retrieval&\n",
      "               oldid=793358396#Average_precision>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import average_precision_score\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> average_precision_score(y_true, y_scores)\n",
      "        0.83...\n",
      "    \n",
      "    balanced_accuracy_score(y_true, y_pred, *, sample_weight=None, adjusted=False)\n",
      "        Compute the balanced accuracy.\n",
      "        \n",
      "        The balanced accuracy in binary and multiclass classification problems to\n",
      "        deal with imbalanced datasets. It is defined as the average of recall\n",
      "        obtained on each class.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0 when ``adjusted=False``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        adjusted : bool, default=False\n",
      "            When true, the result is adjusted for chance, so that random\n",
      "            performance would score 0, while keeping perfect performance at a score\n",
      "            of 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        balanced_accuracy : float\n",
      "            Balanced accuracy score.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        average_precision_score : Compute average precision (AP) from prediction\n",
      "            scores.\n",
      "        precision_score : Compute the precision score.\n",
      "        recall_score : Compute the recall score.\n",
      "        roc_auc_score : Compute Area Under the Receiver Operating Characteristic\n",
      "            Curve (ROC AUC) from prediction scores.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Some literature promotes alternative definitions of balanced accuracy. Our\n",
      "        definition is equivalent to :func:`accuracy_score` with class-balanced\n",
      "        sample weights, and shares desirable properties with the binary case.\n",
      "        See the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).\n",
      "               The balanced accuracy and its posterior distribution.\n",
      "               Proceedings of the 20th International Conference on Pattern\n",
      "               Recognition, 3121-24.\n",
      "        .. [2] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015).\n",
      "               `Fundamentals of Machine Learning for Predictive Data Analytics:\n",
      "               Algorithms, Worked Examples, and Case Studies\n",
      "               <https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import balanced_accuracy_score\n",
      "        >>> y_true = [0, 1, 0, 0, 1, 0]\n",
      "        >>> y_pred = [0, 1, 0, 0, 0, 1]\n",
      "        >>> balanced_accuracy_score(y_true, y_pred)\n",
      "        0.625\n",
      "    \n",
      "    brier_score_loss(y_true, y_prob, *, sample_weight=None, pos_label=None)\n",
      "        Compute the Brier score loss.\n",
      "        \n",
      "        The smaller the Brier score loss, the better, hence the naming with \"loss\".\n",
      "        The Brier score measures the mean squared difference between the predicted\n",
      "        probability and the actual outcome. The Brier score always\n",
      "        takes on a value between zero and one, since this is the largest\n",
      "        possible difference between a predicted probability (which must be\n",
      "        between zero and one) and the actual outcome (which can take on values\n",
      "        of only 0 and 1). It can be decomposed is the sum of refinement loss and\n",
      "        calibration loss.\n",
      "        \n",
      "        The Brier score is appropriate for binary and categorical outcomes that\n",
      "        can be structured as true or false, but is inappropriate for ordinal\n",
      "        variables which can take on three or more values (this is because the\n",
      "        Brier score assumes that all possible outcomes are equivalently\n",
      "        \"distant\" from one another). Which label is considered to be the positive\n",
      "        label is controlled via the parameter `pos_label`, which defaults to\n",
      "        the greater label unless `y_true` is all 0 or all -1, in which case\n",
      "        `pos_label` defaults to 1.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <brier_score_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array of shape (n_samples,)\n",
      "            True targets.\n",
      "        \n",
      "        y_prob : array of shape (n_samples,)\n",
      "            Probabilities of the positive class.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            Label of the positive class. `pos_label` will be inferred in the\n",
      "            following manner:\n",
      "        \n",
      "            * if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1;\n",
      "            * else if `y_true` contains string, an error will be raised and\n",
      "              `pos_label` should be explicitly specified;\n",
      "            * otherwise, `pos_label` defaults to the greater label,\n",
      "              i.e. `np.unique(y_true)[-1]`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            Brier score loss.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import brier_score_loss\n",
      "        >>> y_true = np.array([0, 1, 1, 0])\n",
      "        >>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n",
      "        >>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n",
      "        >>> brier_score_loss(y_true, y_prob)\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, 1-y_prob, pos_label=0)\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, np.array(y_prob) > 0.5)\n",
      "        0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Brier score\n",
      "                <https://en.wikipedia.org/wiki/Brier_score>`_.\n",
      "    \n",
      "    calinski_harabasz_score(X, labels)\n",
      "        Compute the Calinski and Harabasz score.\n",
      "        \n",
      "        It is also known as the Variance Ratio Criterion.\n",
      "        \n",
      "        The score is defined as ratio between the within-cluster dispersion and\n",
      "        the between-cluster dispersion.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <calinski_harabasz_index>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            A list of ``n_features``-dimensional data points. Each row corresponds\n",
      "            to a single data point.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The resulting Calinski-Harabasz score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `T. Calinski and J. Harabasz, 1974. \"A dendrite method for cluster\n",
      "           analysis\". Communications in Statistics\n",
      "           <https://www.tandfonline.com/doi/abs/10.1080/03610927408827101>`_\n",
      "    \n",
      "    check_scoring(estimator, scoring=None, *, allow_none=False)\n",
      "        Determine scorer from user options.\n",
      "        \n",
      "        A TypeError will be thrown if the estimator cannot be scored.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        scoring : str or callable, default=None\n",
      "            A string (see model evaluation documentation) or\n",
      "            a scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "            If None, the provided estimator object's `score` method is used.\n",
      "        \n",
      "        allow_none : bool, default=False\n",
      "            If no scoring is specified and the estimator has no score function, we\n",
      "            can either return None or raise an exception.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scoring : callable\n",
      "            A scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "    \n",
      "    classification_report(y_true, y_pred, *, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False, zero_division='warn')\n",
      "        Build a text report showing the main classification metrics.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <classification_report>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_labels,), default=None\n",
      "            Optional list of label indices to include in the report.\n",
      "        \n",
      "        target_names : list of str of shape (n_labels,), default=None\n",
      "            Optional display names matching the labels (same order).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        digits : int, default=2\n",
      "            Number of digits for formatting output floating point values.\n",
      "            When ``output_dict`` is ``True``, this will be ignored and the\n",
      "            returned values will not be rounded.\n",
      "        \n",
      "        output_dict : bool, default=False\n",
      "            If True, return output as dict.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division. If set to\n",
      "            \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        report : str or dict\n",
      "            Text summary of the precision, recall, F1 score for each class.\n",
      "            Dictionary returned if output_dict is True. Dictionary has the\n",
      "            following structure::\n",
      "        \n",
      "                {'label 1': {'precision':0.5,\n",
      "                             'recall':1.0,\n",
      "                             'f1-score':0.67,\n",
      "                             'support':1},\n",
      "                 'label 2': { ... },\n",
      "                  ...\n",
      "                }\n",
      "        \n",
      "            The reported averages include macro average (averaging the unweighted\n",
      "            mean per label), weighted average (averaging the support-weighted mean\n",
      "            per label), and sample average (only for multilabel classification).\n",
      "            Micro average (averaging the total true positives, false negatives and\n",
      "            false positives) is only shown for multi-label or multi-class\n",
      "            with a subset of classes, because it corresponds to accuracy\n",
      "            otherwise and would be the same for all metrics.\n",
      "            See also :func:`precision_recall_fscore_support` for more details\n",
      "            on averages.\n",
      "        \n",
      "            Note that in binary classification, recall of the positive class\n",
      "            is also known as \"sensitivity\"; recall of the negative class is\n",
      "            \"specificity\".\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support, confusion_matrix,\n",
      "        multilabel_confusion_matrix\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import classification_report\n",
      "        >>> y_true = [0, 1, 2, 2, 2]\n",
      "        >>> y_pred = [0, 0, 2, 2, 1]\n",
      "        >>> target_names = ['class 0', 'class 1', 'class 2']\n",
      "        >>> print(classification_report(y_true, y_pred, target_names=target_names))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "             class 0       0.50      1.00      0.67         1\n",
      "             class 1       0.00      0.00      0.00         1\n",
      "             class 2       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "            accuracy                           0.60         5\n",
      "           macro avg       0.50      0.56      0.49         5\n",
      "        weighted avg       0.70      0.60      0.61         5\n",
      "        <BLANKLINE>\n",
      "        >>> y_pred = [1, 1, 0]\n",
      "        >>> y_true = [1, 1, 1]\n",
      "        >>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "                   1       1.00      0.67      0.80         3\n",
      "                   2       0.00      0.00      0.00         0\n",
      "                   3       0.00      0.00      0.00         0\n",
      "        <BLANKLINE>\n",
      "           micro avg       1.00      0.67      0.80         3\n",
      "           macro avg       0.33      0.22      0.27         3\n",
      "        weighted avg       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "    \n",
      "    cohen_kappa_score(y1, y2, *, labels=None, weights=None, sample_weight=None)\n",
      "        Cohen's kappa: a statistic that measures inter-annotator agreement.\n",
      "        \n",
      "        This function computes Cohen's kappa [1]_, a score that expresses the level\n",
      "        of agreement between two annotators on a classification problem. It is\n",
      "        defined as\n",
      "        \n",
      "        .. math::\n",
      "            \\kappa = (p_o - p_e) / (1 - p_e)\n",
      "        \n",
      "        where :math:`p_o` is the empirical probability of agreement on the label\n",
      "        assigned to any sample (the observed agreement ratio), and :math:`p_e` is\n",
      "        the expected agreement when both annotators assign labels randomly.\n",
      "        :math:`p_e` is estimated using a per-annotator empirical prior over the\n",
      "        class labels [2]_.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cohen_kappa>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y1 : array of shape (n_samples,)\n",
      "            Labels assigned by the first annotator.\n",
      "        \n",
      "        y2 : array of shape (n_samples,)\n",
      "            Labels assigned by the second annotator. The kappa statistic is\n",
      "            symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            List of labels to index the matrix. This may be used to select a\n",
      "            subset of labels. If `None`, all labels that appear at least once in\n",
      "            ``y1`` or ``y2`` are used.\n",
      "        \n",
      "        weights : {'linear', 'quadratic'}, default=None\n",
      "            Weighting type to calculate the score. `None` means no weighted;\n",
      "            \"linear\" means linear weighted; \"quadratic\" means quadratic weighted.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kappa : float\n",
      "            The kappa statistic, which is a number between -1 and 1. The maximum\n",
      "            value means complete agreement; zero or lower means chance agreement.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Cohen (1960). \"A coefficient of agreement for nominal scales\".\n",
      "               Educational and Psychological Measurement 20(1):37-46.\n",
      "               doi:10.1177/001316446002000104.\n",
      "        .. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for\n",
      "               computational linguistics\". Computational Linguistics 34(4):555-596\n",
      "               <https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2>`_.\n",
      "        .. [3] `Wikipedia entry for the Cohen's kappa\n",
      "                <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_.\n",
      "    \n",
      "    completeness_score(labels_true, labels_pred)\n",
      "        Completeness metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`homogeneity_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        completeness : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        v_measure_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are complete::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import completeness_score\n",
      "          >>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that assign all classes members to the same clusters\n",
      "        are still complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          1.0\n",
      "          >>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          0.999...\n",
      "        \n",
      "        If classes members are split across different clusters, the\n",
      "        assignment cannot be complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          0.0\n",
      "          >>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          0.0\n",
      "    \n",
      "    confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)\n",
      "        Compute confusion matrix to evaluate the accuracy of a classification.\n",
      "        \n",
      "        By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\n",
      "        is equal to the number of observations known to be in group :math:`i` and\n",
      "        predicted to be in group :math:`j`.\n",
      "        \n",
      "        Thus in binary classification, the count of true negatives is\n",
      "        :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n",
      "        :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_classes), default=None\n",
      "            List of labels to index the matrix. This may be used to reorder\n",
      "            or select a subset of labels.\n",
      "            If ``None`` is given, those that appear at least once\n",
      "            in ``y_true`` or ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        normalize : {'true', 'pred', 'all'}, default=None\n",
      "            Normalizes confusion matrix over the true (rows), predicted (columns)\n",
      "            conditions or all the population. If None, confusion matrix will not be\n",
      "            normalized.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : ndarray of shape (n_classes, n_classes)\n",
      "            Confusion matrix whose i-th row and j-th\n",
      "            column entry indicates the number of\n",
      "            samples with true label being i-th class\n",
      "            and predicted label being j-th class.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\n",
      "            given an estimator, the data, and the label.\n",
      "        ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\n",
      "            given the true and predicted labels.\n",
      "        ConfusionMatrixDisplay : Confusion Matrix visualization.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Confusion matrix\n",
      "               <https://en.wikipedia.org/wiki/Confusion_matrix>`_\n",
      "               (Wikipedia and other references may use a different\n",
      "               convention for axes).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import confusion_matrix\n",
      "        >>> y_true = [2, 0, 2, 2, 0, 1]\n",
      "        >>> y_pred = [0, 0, 2, 2, 0, 2]\n",
      "        >>> confusion_matrix(y_true, y_pred)\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        In the binary case, we can extract true positives, etc as follows:\n",
      "        \n",
      "        >>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
      "        >>> (tn, fp, fn, tp)\n",
      "        (0, 2, 1, 1)\n",
      "    \n",
      "    consensus_score(a, b, *, similarity='jaccard')\n",
      "        The similarity of two sets of biclusters.\n",
      "        \n",
      "        Similarity between individual biclusters is computed. Then the\n",
      "        best matching between sets is found using the Hungarian algorithm.\n",
      "        The final score is the sum of similarities divided by the size of\n",
      "        the larger set.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <biclustering>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : (rows, columns)\n",
      "            Tuple of row and column indicators for a set of biclusters.\n",
      "        \n",
      "        b : (rows, columns)\n",
      "            Another set of biclusters like ``a``.\n",
      "        \n",
      "        similarity : 'jaccard' or callable, default='jaccard'\n",
      "            May be the string \"jaccard\" to use the Jaccard coefficient, or\n",
      "            any function that takes four arguments, each of which is a 1d\n",
      "            indicator vector: (a_rows, a_columns, b_rows, b_columns).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        * Hochreiter, Bodenhofer, et. al., 2010. `FABIA: factor analysis\n",
      "          for bicluster acquisition\n",
      "          <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/>`__.\n",
      "    \n",
      "    coverage_error(y_true, y_score, *, sample_weight=None)\n",
      "        Coverage error measure.\n",
      "        \n",
      "        Compute how far we need to go through the ranked scores to cover all\n",
      "        true labels. The best value is equal to the average number\n",
      "        of labels in ``y_true`` per sample.\n",
      "        \n",
      "        Ties in ``y_scores`` are broken by giving maximal rank that would have\n",
      "        been assigned to all tied values.\n",
      "        \n",
      "        Note: Our implementation's score is 1 greater than the one given in\n",
      "        Tsoumakas et al., 2010. This extends it to handle the degenerate case\n",
      "        in which an instance has 0 true labels.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <coverage_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coverage_error : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "    \n",
      "    d2_tweedie_score(y_true, y_pred, *, sample_weight=None, power=0)\n",
      "        D^2 regression score function, percentage of Tweedie deviance explained.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the model can be\n",
      "        arbitrarily worse). A model that always uses the empirical mean of `y_true` as\n",
      "        constant prediction, disregarding the input features, gets a D^2 score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <d2_tweedie_score>`.\n",
      "        \n",
      "        .. versionadded:: 1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        power : float, default=0\n",
      "            Tweedie power parameter. Either power <= 0 or power >= 1.\n",
      "        \n",
      "            The higher `p` the less weight is given to extreme\n",
      "            deviations between true and predicted targets.\n",
      "        \n",
      "            - power < 0: Extreme stable distribution. Requires: y_pred > 0.\n",
      "            - power = 0 : Normal distribution, output corresponds to r2_score.\n",
      "              y_true and y_pred can be any real numbers.\n",
      "            - power = 1 : Poisson distribution. Requires: y_true >= 0 and\n",
      "              y_pred > 0.\n",
      "            - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0\n",
      "              and y_pred > 0.\n",
      "            - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.\n",
      "            - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "            - otherwise : Positive stable distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z : float or ndarray of floats\n",
      "            The D^2 score.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Like R^2, D^2 score may be negative (it need not actually be the square of\n",
      "        a quantity D).\n",
      "        \n",
      "        This metric is not well-defined for single samples and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.\n",
      "               Wainwright. \"Statistical Learning with Sparsity: The Lasso and\n",
      "               Generalizations.\" (2015). https://trevorhastie.github.io\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import d2_tweedie_score\n",
      "        >>> y_true = [0.5, 1, 2.5, 7]\n",
      "        >>> y_pred = [1, 1, 5, 3.5]\n",
      "        >>> d2_tweedie_score(y_true, y_pred)\n",
      "        0.285...\n",
      "        >>> d2_tweedie_score(y_true, y_pred, power=1)\n",
      "        0.487...\n",
      "        >>> d2_tweedie_score(y_true, y_pred, power=2)\n",
      "        0.630...\n",
      "        >>> d2_tweedie_score(y_true, y_true, power=2)\n",
      "        1.0\n",
      "    \n",
      "    davies_bouldin_score(X, labels)\n",
      "        Compute the Davies-Bouldin score.\n",
      "        \n",
      "        The score is defined as the average similarity measure of each cluster with\n",
      "        its most similar cluster, where similarity is the ratio of within-cluster\n",
      "        distances to between-cluster distances. Thus, clusters which are farther\n",
      "        apart and less dispersed will result in a better score.\n",
      "        \n",
      "        The minimum score is zero, with lower values indicating better clustering.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <davies-bouldin_index>`.\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            A list of ``n_features``-dimensional data points. Each row corresponds\n",
      "            to a single data point.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score: float\n",
      "            The resulting Davies-Bouldin score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Davies, David L.; Bouldin, Donald W. (1979).\n",
      "           `\"A Cluster Separation Measure\"\n",
      "           <https://ieeexplore.ieee.org/document/4766909>`__.\n",
      "           IEEE Transactions on Pattern Analysis and Machine Intelligence.\n",
      "           PAMI-1 (2): 224-227\n",
      "    \n",
      "    dcg_score(y_true, y_score, *, k=None, log_base=2, sample_weight=None, ignore_ties=False)\n",
      "        Compute Discounted Cumulative Gain.\n",
      "        \n",
      "        Sum the true scores ranked in the order induced by the predicted scores,\n",
      "        after applying a logarithmic discount.\n",
      "        \n",
      "        This ranking metric yields a high value if true labels are ranked high by\n",
      "        ``y_score``.\n",
      "        \n",
      "        Usually the Normalized Discounted Cumulative Gain (NDCG, computed by\n",
      "        ndcg_score) is preferred.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples, n_labels)\n",
      "            True targets of multilabel classification, or true scores of entities\n",
      "            to be ranked.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates, confidence values,\n",
      "            or non-thresholded measure of decisions (as returned by\n",
      "            \"decision_function\" on some classifiers).\n",
      "        \n",
      "        k : int, default=None\n",
      "            Only consider the highest k scores in the ranking. If None, use all\n",
      "            outputs.\n",
      "        \n",
      "        log_base : float, default=2\n",
      "            Base of the logarithm used for the discount. A low value means a\n",
      "            sharper discount (top results are more important).\n",
      "        \n",
      "        sample_weight : ndarray of shape (n_samples,), default=None\n",
      "            Sample weights. If `None`, all samples are given the same weight.\n",
      "        \n",
      "        ignore_ties : bool, default=False\n",
      "            Assume that there are no ties in y_score (which is likely to be the\n",
      "            case if y_score is continuous) for efficiency gains.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        discounted_cumulative_gain : float\n",
      "            The averaged sample DCG scores.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ndcg_score : The Discounted Cumulative Gain divided by the Ideal Discounted\n",
      "            Cumulative Gain (the DCG obtained for a perfect ranking), in order to\n",
      "            have a score between 0 and 1.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        `Wikipedia entry for Discounted Cumulative Gain\n",
      "        <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_.\n",
      "        \n",
      "        Jarvelin, K., & Kekalainen, J. (2002).\n",
      "        Cumulated gain-based evaluation of IR techniques. ACM Transactions on\n",
      "        Information Systems (TOIS), 20(4), 422-446.\n",
      "        \n",
      "        Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\n",
      "        A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\n",
      "        Annual Conference on Learning Theory (COLT 2013).\n",
      "        \n",
      "        McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n",
      "        performance measures efficiently in the presence of tied scores. In\n",
      "        European conference on information retrieval (pp. 414-421). Springer,\n",
      "        Berlin, Heidelberg.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import dcg_score\n",
      "        >>> # we have groud-truth relevance of some answers to a query:\n",
      "        >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n",
      "        >>> # we predict scores for the answers\n",
      "        >>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n",
      "        >>> dcg_score(true_relevance, scores)\n",
      "        9.49...\n",
      "        >>> # we can set k to truncate the sum; only top k answers contribute\n",
      "        >>> dcg_score(true_relevance, scores, k=2)\n",
      "        5.63...\n",
      "        >>> # now we have some ties in our prediction\n",
      "        >>> scores = np.asarray([[1, 0, 0, 0, 1]])\n",
      "        >>> # by default ties are averaged, so here we get the average true\n",
      "        >>> # relevance of our top predictions: (10 + 5) / 2 = 7.5\n",
      "        >>> dcg_score(true_relevance, scores, k=1)\n",
      "        7.5\n",
      "        >>> # we can choose to ignore ties for faster results, but only\n",
      "        >>> # if we know there aren't ties in our scores, otherwise we get\n",
      "        >>> # wrong results:\n",
      "        >>> dcg_score(true_relevance,\n",
      "        ...           scores, k=1, ignore_ties=True)\n",
      "        5.0\n",
      "    \n",
      "    det_curve(y_true, y_score, pos_label=None, sample_weight=None)\n",
      "        Compute error rates for different probability thresholds.\n",
      "        \n",
      "        .. note::\n",
      "           This metric is used for evaluation of ranking and error tradeoffs of\n",
      "           a binary classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <det_curve>`.\n",
      "        \n",
      "        .. versionadded:: 0.24\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        y_score : ndarray of shape of (n_samples,)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fpr : ndarray of shape (n_thresholds,)\n",
      "            False positive rate (FPR) such that element i is the false positive\n",
      "            rate of predictions with score >= thresholds[i]. This is occasionally\n",
      "            referred to as false acceptance propability or fall-out.\n",
      "        \n",
      "        fnr : ndarray of shape (n_thresholds,)\n",
      "            False negative rate (FNR) such that element i is the false negative\n",
      "            rate of predictions with score >= thresholds[i]. This is occasionally\n",
      "            referred to as false rejection or miss rate.\n",
      "        \n",
      "        thresholds : ndarray of shape (n_thresholds,)\n",
      "            Decreasing score values.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        DetCurveDisplay.from_estimator : Plot DET curve given an estimator and\n",
      "            some data.\n",
      "        DetCurveDisplay.from_predictions : Plot DET curve given the true and\n",
      "            predicted labels.\n",
      "        DetCurveDisplay : DET curve visualization.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        precision_recall_curve : Compute precision-recall curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import det_curve\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, fnr, thresholds = det_curve(y_true, y_scores)\n",
      "        >>> fpr\n",
      "        array([0.5, 0.5, 0. ])\n",
      "        >>> fnr\n",
      "        array([0. , 0.5, 0.5])\n",
      "        >>> thresholds\n",
      "        array([0.35, 0.4 , 0.8 ])\n",
      "    \n",
      "    euclidean_distances(X, Y=None, *, Y_norm_squared=None, squared=False, X_norm_squared=None)\n",
      "        Compute the distance matrix between each pair from a vector array X and Y.\n",
      "        \n",
      "        For efficiency reasons, the euclidean distance between a pair of row\n",
      "        vector x and y is computed as::\n",
      "        \n",
      "            dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))\n",
      "        \n",
      "        This formulation has two advantages over other ways of computing distances.\n",
      "        First, it is computationally efficient when dealing with sparse data.\n",
      "        Second, if one argument varies but the other remains unchanged, then\n",
      "        `dot(x, x)` and/or `dot(y, y)` can be pre-computed.\n",
      "        \n",
      "        However, this is not the most precise way of doing this computation,\n",
      "        because this equation potentially suffers from \"catastrophic cancellation\".\n",
      "        Also, the distance matrix returned by this function may not be exactly\n",
      "        symmetric as required by, e.g., ``scipy.spatial.distance`` functions.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            An array where each row is a sample and each column is a feature.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None\n",
      "            An array where each row is a sample and each column is a feature.\n",
      "            If `None`, method uses `Y=X`.\n",
      "        \n",
      "        Y_norm_squared : array-like of shape (n_samples_Y,) or (n_samples_Y, 1)             or (1, n_samples_Y), default=None\n",
      "            Pre-computed dot-products of vectors in Y (e.g.,\n",
      "            ``(Y**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        squared : bool, default=False\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        X_norm_squared : array-like of shape (n_samples_X,) or (n_samples_X, 1)             or (1, n_samples_X), default=None\n",
      "            Pre-computed dot-products of vectors in X (e.g.,\n",
      "            ``(X**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "            Returns the distances between the row vectors of `X`\n",
      "            and the row vectors of `Y`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        paired_distances : Distances betweens pairs of elements of X and Y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        To achieve a better accuracy, `X_norm_squared`and `Y_norm_squared` may be\n",
      "        unused if they are passed as `np.float32`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import euclidean_distances\n",
      "        >>> X = [[0, 1], [1, 1]]\n",
      "        >>> # distance between rows of X\n",
      "        >>> euclidean_distances(X, X)\n",
      "        array([[0., 1.],\n",
      "               [1., 0.]])\n",
      "        >>> # get distance to origin\n",
      "        >>> euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "    \n",
      "    explained_variance_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Explained variance regression score function.\n",
      "        \n",
      "        Best possible score is 1.0, lower values are worse.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <explained_variance_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average', 'variance_weighted'} or             array-like of shape (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of floats\n",
      "            The explained variance or ndarray if 'multioutput' is 'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import explained_variance_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> explained_variance_score(y_true, y_pred)\n",
      "        0.957...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> explained_variance_score(y_true, y_pred, multioutput='uniform_average')\n",
      "        0.983...\n",
      "    \n",
      "    f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the F1 score, also known as balanced F-score or F-measure.\n",
      "        \n",
      "        The F1 score can be interpreted as a harmonic mean of the precision and\n",
      "        recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
      "        The relative contribution of precision and recall to the F1 score are\n",
      "        equal. The formula for the F1 score is::\n",
      "        \n",
      "            F1 = 2 * (precision * recall) / (precision + recall)\n",
      "        \n",
      "        In the multi-class and multi-label case, this is the average of\n",
      "        the F1 score of each class with weighting depending on the ``average``\n",
      "        parameter.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples','weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when all\n",
      "            predictions and labels are negative. If set to \"warn\", this acts as 0,\n",
      "            but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f1_score : float or array of float, shape = [n_unique_labels]\n",
      "            F1 score of the positive class in binary classification or weighted\n",
      "            average of the F1 scores of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fbeta_score, precision_recall_fscore_support, jaccard_score,\n",
      "        multilabel_confusion_matrix\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import f1_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> f1_score(y_true, y_pred, average='macro')\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average='micro')\n",
      "        0.33...\n",
      "        >>> f1_score(y_true, y_pred, average='weighted')\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average=None)\n",
      "        array([0.8, 0. , 0. ])\n",
      "        >>> y_true = [0, 0, 0, 0, 0, 0]\n",
      "        >>> y_pred = [0, 0, 0, 0, 0, 0]\n",
      "        >>> f1_score(y_true, y_pred, zero_division=1)\n",
      "        1.0...\n",
      "        >>> # multilabel classification\n",
      "        >>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
      "        >>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
      "        >>> f1_score(y_true, y_pred, average=None)\n",
      "        array([0.66666667, 1.        , 0.66666667])\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision is undefined.\n",
      "        When ``true positive + false negative == 0``, recall is undefined.\n",
      "        In such cases, by default the metric will be set to 0, as will f-score,\n",
      "        and ``UndefinedMetricWarning`` will be raised. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "    \n",
      "    fbeta_score(y_true, y_pred, *, beta, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the F-beta score.\n",
      "        \n",
      "        The F-beta score is the weighted harmonic mean of precision and recall,\n",
      "        reaching its optimal value at 1 and its worst value at 0.\n",
      "        \n",
      "        The `beta` parameter determines the weight of recall in the combined\n",
      "        score. ``beta < 1`` lends more weight to precision, while ``beta > 1``\n",
      "        favors recall (``beta -> 0`` considers only precision, ``beta -> +inf``\n",
      "        only recall).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float\n",
      "            Determines the weight of recall in the combined score.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when all\n",
      "            predictions and labels are negative. If set to \"warn\", this acts as 0,\n",
      "            but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            F-beta score of the positive class in binary classification or weighted\n",
      "            average of the F-beta score of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support, multilabel_confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0`` or\n",
      "        ``true positive + false negative == 0``, f-score returns 0 and raises\n",
      "        ``UndefinedMetricWarning``. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).\n",
      "               Modern Information Retrieval. Addison Wesley, pp. 327-328.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import fbeta_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)\n",
      "        0.33...\n",
      "        >>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average=None, beta=0.5)\n",
      "        array([0.71..., 0.        , 0.        ])\n",
      "    \n",
      "    fowlkes_mallows_score(labels_true, labels_pred, *, sparse=False)\n",
      "        Measure the similarity of two clusterings of a set of points.\n",
      "        \n",
      "        .. versionadded:: 0.18\n",
      "        \n",
      "        The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of\n",
      "        the precision and recall::\n",
      "        \n",
      "            FMI = TP / sqrt((TP + FP) * (TP + FN))\n",
      "        \n",
      "        Where ``TP`` is the number of **True Positive** (i.e. the number of pair of\n",
      "        points that belongs in the same clusters in both ``labels_true`` and\n",
      "        ``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the\n",
      "        number of pair of points that belongs in the same clusters in\n",
      "        ``labels_true`` and not in ``labels_pred``) and ``FN`` is the number of\n",
      "        **False Negative** (i.e the number of pair of points that belongs in the\n",
      "        same clusters in ``labels_pred`` and not in ``labels_True``).\n",
      "        \n",
      "        The score ranges from 0 to 1. A high value indicates a good similarity\n",
      "        between two clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <fowlkes_mallows_scores>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = (``n_samples``,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array, shape = (``n_samples``, )\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        sparse : bool, default=False\n",
      "            Compute contingency matrix internally with sparse matrix.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "           The resulting Fowlkes-Mallows score.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import fowlkes_mallows_score\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally random, hence the FMI is null::\n",
      "        \n",
      "          >>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `E. B. Fowkles and C. L. Mallows, 1983. \"A method for comparing two\n",
      "           hierarchical clusterings\". Journal of the American Statistical\n",
      "           Association\n",
      "           <https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Fowlkes-Mallows Index\n",
      "               <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_\n",
      "    \n",
      "    get_scorer(scoring)\n",
      "        Get a scorer from string.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <scoring_parameter>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        scoring : str or callable\n",
      "            Scoring method as string. If callable it is returned as is.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            The scorer.\n",
      "    \n",
      "    hamming_loss(y_true, y_pred, *, sample_weight=None)\n",
      "        Compute the average Hamming loss.\n",
      "        \n",
      "        The Hamming loss is the fraction of labels that are incorrectly predicted.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hamming_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int\n",
      "            Return the average Hamming loss between element of ``y_true`` and\n",
      "            ``y_pred``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score : Compute the accuracy score. By default, the function will\n",
      "            return the fraction of correct predictions divided by the total number\n",
      "            of predictions.\n",
      "        jaccard_score : Compute the Jaccard similarity coefficient score.\n",
      "        zero_one_loss : Compute the Zero-one classification loss. By default, the\n",
      "            function will return the percentage of imperfectly predicted subsets.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multiclass classification, the Hamming loss corresponds to the Hamming\n",
      "        distance between ``y_true`` and ``y_pred`` which is equivalent to the\n",
      "        subset ``zero_one_loss`` function, when `normalize` parameter is set to\n",
      "        True.\n",
      "        \n",
      "        In multilabel classification, the Hamming loss is different from the\n",
      "        subset zero-one loss. The zero-one loss considers the entire set of labels\n",
      "        for a given sample incorrect if it does not entirely match the true set of\n",
      "        labels. Hamming loss is more forgiving in that it penalizes only the\n",
      "        individual labels.\n",
      "        \n",
      "        The Hamming loss is upperbounded by the subset zero-one loss, when\n",
      "        `normalize` parameter is set to True. It is always between 0 and 1,\n",
      "        lower being better.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:\n",
      "               An Overview. International Journal of Data Warehousing & Mining,\n",
      "               3(3), 1-13, July-September 2007.\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Hamming distance\n",
      "               <https://en.wikipedia.org/wiki/Hamming_distance>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import hamming_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> hamming_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n",
      "        0.75\n",
      "    \n",
      "    hinge_loss(y_true, pred_decision, *, labels=None, sample_weight=None)\n",
      "        Average hinge loss (non-regularized).\n",
      "        \n",
      "        In binary class case, assuming labels in y_true are encoded with +1 and -1,\n",
      "        when a prediction mistake is made, ``margin = y_true * pred_decision`` is\n",
      "        always negative (since the signs disagree), implying ``1 - margin`` is\n",
      "        always greater than 1.  The cumulated hinge loss is therefore an upper\n",
      "        bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        In multiclass case, the function expects that either all the labels are\n",
      "        included in y_true or an optional labels argument is provided which\n",
      "        contains all the labels. The multilabel margin is calculated according\n",
      "        to Crammer-Singer's method. As in the binary case, the cumulated hinge loss\n",
      "        is an upper bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hinge_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array of shape (n_samples,)\n",
      "            True target, consisting of integers of two values. The positive label\n",
      "            must be greater than the negative label.\n",
      "        \n",
      "        pred_decision : array of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Predicted decisions, as output by decision_function (floats).\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            Contains all the labels for the problem. Used in multiclass hinge loss.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Hinge loss\n",
      "               <https://en.wikipedia.org/wiki/Hinge_loss>`_.\n",
      "        \n",
      "        .. [2] Koby Crammer, Yoram Singer. On the Algorithmic\n",
      "               Implementation of Multiclass Kernel-based Vector\n",
      "               Machines. Journal of Machine Learning Research 2,\n",
      "               (2001), 265-292.\n",
      "        \n",
      "        .. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models\n",
      "               by Robert C. Moore, John DeNero\n",
      "               <http://www.ttic.edu/sigml/symposium2011/papers/\n",
      "               Moore+DeNero_Regularization.pdf>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import svm\n",
      "        >>> from sklearn.metrics import hinge_loss\n",
      "        >>> X = [[0], [1]]\n",
      "        >>> y = [-1, 1]\n",
      "        >>> est = svm.LinearSVC(random_state=0)\n",
      "        >>> est.fit(X, y)\n",
      "        LinearSVC(random_state=0)\n",
      "        >>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n",
      "        >>> pred_decision\n",
      "        array([-2.18...,  2.36...,  0.09...])\n",
      "        >>> hinge_loss([-1, 1, 1], pred_decision)\n",
      "        0.30...\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> X = np.array([[0], [1], [2], [3]])\n",
      "        >>> Y = np.array([0, 1, 2, 3])\n",
      "        >>> labels = np.array([0, 1, 2, 3])\n",
      "        >>> est = svm.LinearSVC()\n",
      "        >>> est.fit(X, Y)\n",
      "        LinearSVC()\n",
      "        >>> pred_decision = est.decision_function([[-1], [2], [3]])\n",
      "        >>> y_true = [0, 2, 3]\n",
      "        >>> hinge_loss(y_true, pred_decision, labels=labels)\n",
      "        0.56...\n",
      "    \n",
      "    homogeneity_completeness_v_measure(labels_true, labels_pred, *, beta=1.0)\n",
      "        Compute the homogeneity and completeness and V-Measure scores at once.\n",
      "        \n",
      "        Those metrics are based on normalized conditional entropy measures of\n",
      "        the clustering labeling to evaluate given the knowledge of a Ground\n",
      "        Truth class labels of the same samples.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        Both scores have positive values between 0.0 and 1.0, larger values\n",
      "        being desirable.\n",
      "        \n",
      "        Those 3 metrics are independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score values in any way.\n",
      "        \n",
      "        V-Measure is furthermore symmetric: swapping ``labels_true`` and\n",
      "        ``label_pred`` will give the same score. This does not hold for\n",
      "        homogeneity and completeness. V-Measure is identical to\n",
      "        :func:`normalized_mutual_info_score` with the arithmetic averaging\n",
      "        method.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n",
      "            If ``beta`` is greater than 1, ``completeness`` is weighted more\n",
      "            strongly in the calculation. If ``beta`` is less than 1,\n",
      "            ``homogeneity`` is weighted more strongly.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling\n",
      "        \n",
      "        completeness : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        v_measure : float\n",
      "            harmonic mean of the first two\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        completeness_score\n",
      "        v_measure_score\n",
      "    \n",
      "    homogeneity_score(labels_true, labels_pred)\n",
      "        Homogeneity metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`completeness_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        completeness_score\n",
      "        v_measure_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are homogeneous::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import homogeneity_score\n",
      "          >>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that further split classes into more clusters can be\n",
      "        perfectly homogeneous::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          1.000000\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          1.000000\n",
      "        \n",
      "        Clusters that include samples from different classes do not make for an\n",
      "        homogeneous labeling::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          0.0...\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          0.0...\n",
      "    \n",
      "    jaccard_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Jaccard similarity coefficient score.\n",
      "        \n",
      "        The Jaccard index [1], or Jaccard similarity coefficient, defined as\n",
      "        the size of the intersection divided by the size of the union of two label\n",
      "        sets, is used to compare set of predicted labels for a sample to the\n",
      "        corresponding set of labels in ``y_true``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <jaccard_similarity_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted',             'binary'} or None, default='binary'\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", {0.0, 1.0}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when there\n",
      "            there are no negative values in predictions and labels. If set to\n",
      "            \"warn\", this acts like 0, but a warning is also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float (if average is not None) or array of floats, shape =            [n_unique_labels]\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score, f1_score, multilabel_confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        :func:`jaccard_score` may be a poor metric if there are no\n",
      "        positives for some samples or classes. Jaccard is undefined if there are\n",
      "        no true or predicted labels, and our implementation will return a score\n",
      "        of 0 with a warning.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Jaccard index\n",
      "               <https://en.wikipedia.org/wiki/Jaccard_index>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import jaccard_score\n",
      "        >>> y_true = np.array([[0, 1, 1],\n",
      "        ...                    [1, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 1, 1],\n",
      "        ...                    [1, 0, 0]])\n",
      "        \n",
      "        In the binary case:\n",
      "        \n",
      "        >>> jaccard_score(y_true[0], y_pred[0])\n",
      "        0.6666...\n",
      "        \n",
      "        In the multilabel case:\n",
      "        \n",
      "        >>> jaccard_score(y_true, y_pred, average='samples')\n",
      "        0.5833...\n",
      "        >>> jaccard_score(y_true, y_pred, average='macro')\n",
      "        0.6666...\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 0.5, 1. ])\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> y_pred = [0, 2, 1, 2]\n",
      "        >>> y_true = [0, 1, 2, 2]\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        array([1. , 0. , 0.33...])\n",
      "    \n",
      "    label_ranking_average_precision_score(y_true, y_score, *, sample_weight=None)\n",
      "        Compute ranking-based average precision.\n",
      "        \n",
      "        Label ranking average precision (LRAP) is the average over each ground\n",
      "        truth label assigned to each sample, of the ratio of true vs. total\n",
      "        labels with lower score.\n",
      "        \n",
      "        This metric is used in multilabel ranking problem, where the goal\n",
      "        is to give better rank to the labels associated to each sample.\n",
      "        \n",
      "        The obtained score is always strictly greater than 0 and\n",
      "        the best value is 1.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_average_precision>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {ndarray, sparse matrix} of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import label_ranking_average_precision_score\n",
      "        >>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
      "        >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
      "        >>> label_ranking_average_precision_score(y_true, y_score)\n",
      "        0.416...\n",
      "    \n",
      "    label_ranking_loss(y_true, y_score, *, sample_weight=None)\n",
      "        Compute Ranking loss measure.\n",
      "        \n",
      "        Compute the average number of label pairs that are incorrectly ordered\n",
      "        given y_score weighted by the size of the label set and the number of\n",
      "        labels not in the label set.\n",
      "        \n",
      "        This is similar to the error set size, but weighted by the number of\n",
      "        relevant and irrelevant labels. The best performance is achieved with\n",
      "        a ranking loss of zero.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_loss>`.\n",
      "        \n",
      "        .. versionadded:: 0.17\n",
      "           A function *label_ranking_loss*\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {ndarray, sparse matrix} of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "    \n",
      "    log_loss(y_true, y_pred, *, eps=1e-15, normalize=True, sample_weight=None, labels=None)\n",
      "        Log loss, aka logistic loss or cross-entropy loss.\n",
      "        \n",
      "        This is the loss function used in (multinomial) logistic regression\n",
      "        and extensions of it such as neural networks, defined as the negative\n",
      "        log-likelihood of a logistic model that returns ``y_pred`` probabilities\n",
      "        for its training data ``y_true``.\n",
      "        The log loss is only defined for two or more labels.\n",
      "        For a single sample with true label :math:`y \\in \\{0,1\\}` and\n",
      "        a probability estimate :math:`p = \\operatorname{Pr}(y = 1)`, the log\n",
      "        loss is:\n",
      "        \n",
      "        .. math::\n",
      "            L_{\\log}(y, p) = -(y \\log (p) + (1 - y) \\log (1 - p))\n",
      "        \n",
      "        Read more in the :ref:`User Guide <log_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like or label indicator matrix\n",
      "            Ground truth (correct) labels for n_samples samples.\n",
      "        \n",
      "        y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)\n",
      "            Predicted probabilities, as returned by a classifier's\n",
      "            predict_proba method. If ``y_pred.shape = (n_samples,)``\n",
      "            the probabilities provided are assumed to be that of the\n",
      "            positive class. The labels in ``y_pred`` are assumed to be\n",
      "            ordered alphabetically, as done by\n",
      "            :class:`preprocessing.LabelBinarizer`.\n",
      "        \n",
      "        eps : float, default=1e-15\n",
      "            Log loss is undefined for p=0 or p=1, so probabilities are\n",
      "            clipped to max(eps, min(1 - eps, p)).\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If true, return the mean loss per sample.\n",
      "            Otherwise, return the sum of the per-sample losses.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            If not provided, labels will be inferred from y_true. If ``labels``\n",
      "            is ``None`` and ``y_pred`` has shape (n_samples,) the labels are\n",
      "            assumed to be binary and are inferred from ``y_true``.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The logarithm used is the natural logarithm (base-e).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import log_loss\n",
      "        >>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],\n",
      "        ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n",
      "        0.21616...\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,\n",
      "        p. 209.\n",
      "    \n",
      "    make_scorer(score_func, *, greater_is_better=True, needs_proba=False, needs_threshold=False, **kwargs)\n",
      "        Make a scorer from a performance metric or loss function.\n",
      "        \n",
      "        This factory function wraps scoring functions for use in\n",
      "        :class:`~sklearn.model_selection.GridSearchCV` and\n",
      "        :func:`~sklearn.model_selection.cross_val_score`.\n",
      "        It takes a score function, such as :func:`~sklearn.metrics.accuracy_score`,\n",
      "        :func:`~sklearn.metrics.mean_squared_error`,\n",
      "        :func:`~sklearn.metrics.adjusted_rand_index` or\n",
      "        :func:`~sklearn.metrics.average_precision`\n",
      "        and returns a callable that scores an estimator's output.\n",
      "        The signature of the call is `(estimator, X, y)` where `estimator`\n",
      "        is the model to be evaluated, `X` is the data and `y` is the\n",
      "        ground truth labeling (or `None` in the case of unsupervised models).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <scoring>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        score_func : callable\n",
      "            Score function (or loss function) with signature\n",
      "            ``score_func(y, y_pred, **kwargs)``.\n",
      "        \n",
      "        greater_is_better : bool, default=True\n",
      "            Whether score_func is a score function (default), meaning high is good,\n",
      "            or a loss function, meaning low is good. In the latter case, the\n",
      "            scorer object will sign-flip the outcome of the score_func.\n",
      "        \n",
      "        needs_proba : bool, default=False\n",
      "            Whether score_func requires predict_proba to get probability estimates\n",
      "            out of a classifier.\n",
      "        \n",
      "            If True, for binary `y_true`, the score function is supposed to accept\n",
      "            a 1D `y_pred` (i.e., probability of the positive class, shape\n",
      "            `(n_samples,)`).\n",
      "        \n",
      "        needs_threshold : bool, default=False\n",
      "            Whether score_func takes a continuous decision certainty.\n",
      "            This only works for binary classification using estimators that\n",
      "            have either a decision_function or predict_proba method.\n",
      "        \n",
      "            If True, for binary `y_true`, the score function is supposed to accept\n",
      "            a 1D `y_pred` (i.e., probability of the positive class or the decision\n",
      "            function, shape `(n_samples,)`).\n",
      "        \n",
      "            For example ``average_precision`` or the area under the roc curve\n",
      "            can not be computed using discrete predictions alone.\n",
      "        \n",
      "        **kwargs : additional arguments\n",
      "            Additional parameters to be passed to score_func.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            Callable object that returns a scalar score; greater is better.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import fbeta_score, make_scorer\n",
      "        >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
      "        >>> ftwo_scorer\n",
      "        make_scorer(fbeta_score, beta=2)\n",
      "        >>> from sklearn.model_selection import GridSearchCV\n",
      "        >>> from sklearn.svm import LinearSVC\n",
      "        >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n",
      "        ...                     scoring=ftwo_scorer)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If `needs_proba=False` and `needs_threshold=False`, the score\n",
      "        function is supposed to accept the output of :term:`predict`. If\n",
      "        `needs_proba=True`, the score function is supposed to accept the\n",
      "        output of :term:`predict_proba` (For binary `y_true`, the score function is\n",
      "        supposed to accept probability of the positive class). If\n",
      "        `needs_threshold=True`, the score function is supposed to accept the\n",
      "        output of :term:`decision_function` or :term:`predict_proba` when\n",
      "        :term:`decision_function` is not present.\n",
      "    \n",
      "    matthews_corrcoef(y_true, y_pred, *, sample_weight=None)\n",
      "        Compute the Matthews correlation coefficient (MCC).\n",
      "        \n",
      "        The Matthews correlation coefficient is used in machine learning as a\n",
      "        measure of the quality of binary and multiclass classifications. It takes\n",
      "        into account true and false positives and negatives and is generally\n",
      "        regarded as a balanced measure which can be used even if the classes are of\n",
      "        very different sizes. The MCC is in essence a correlation coefficient value\n",
      "        between -1 and +1. A coefficient of +1 represents a perfect prediction, 0\n",
      "        an average random prediction and -1 an inverse prediction.  The statistic\n",
      "        is also known as the phi coefficient. [source: Wikipedia]\n",
      "        \n",
      "        Binary and multiclass labels are supported.  Only in the binary case does\n",
      "        this relate to information about true and false positives and negatives.\n",
      "        See references below.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <matthews_corrcoef>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array, shape = [n_samples]\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mcc : float\n",
      "            The Matthews correlation coefficient (+1 represents a perfect\n",
      "            prediction, 0 an average random prediction and -1 and inverse\n",
      "            prediction).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the\n",
      "           accuracy of prediction algorithms for classification: an overview\n",
      "           <https://doi.org/10.1093/bioinformatics/16.5.412>`_.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Matthews Correlation Coefficient\n",
      "           <https://en.wikipedia.org/wiki/Matthews_correlation_coefficient>`_.\n",
      "        \n",
      "        .. [3] `Gorodkin, (2004). Comparing two K-category assignments by a\n",
      "            K-category correlation coefficient\n",
      "            <https://www.sciencedirect.com/science/article/pii/S1476927104000799>`_.\n",
      "        \n",
      "        .. [4] `Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN\n",
      "            Error Measures in MultiClass Prediction\n",
      "            <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import matthews_corrcoef\n",
      "        >>> y_true = [+1, +1, +1, -1]\n",
      "        >>> y_pred = [+1, -1, +1, +1]\n",
      "        >>> matthews_corrcoef(y_true, y_pred)\n",
      "        -0.33...\n",
      "    \n",
      "    max_error(y_true, y_pred)\n",
      "        The max_error metric calculates the maximum residual error.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <max_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        max_error : float\n",
      "            A positive floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import max_error\n",
      "        >>> y_true = [3, 2, 7, 1]\n",
      "        >>> y_pred = [4, 2, 7, 1]\n",
      "        >>> max_error(y_true, y_pred)\n",
      "        1\n",
      "    \n",
      "    mean_absolute_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean absolute error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            MAE output is non-negative floating point. The best value is 0.0.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.75\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.5, 1. ])\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.85...\n",
      "    \n",
      "    mean_absolute_percentage_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean absolute percentage error (MAPE) regression loss.\n",
      "        \n",
      "        Note here that the output is not a percentage in the range [0, 100]\n",
      "        and a value of 100 does not mean 100% but 1e2. Furthermore, the output\n",
      "        can be arbitrarily high when `y_true` is small (which is specific to the\n",
      "        metric) or when `abs(y_true - y_pred)` is large (which is common for most\n",
      "        regression metrics). Read more in the\n",
      "        :ref:`User Guide <mean_absolute_percentage_error>`.\n",
      "        \n",
      "        .. versionadded:: 0.24\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "            If input is list then the shape must be (n_outputs,).\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute percentage error\n",
      "            is returned for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            MAPE output is non-negative floating point. The best value is 0.0.\n",
      "            But note that bad predictions can lead to arbitrarily large\n",
      "            MAPE values, especially if some `y_true` values are very close to zero.\n",
      "            Note that we return a large value instead of `inf` when `y_true` is zero.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_absolute_percentage_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred)\n",
      "        0.3273...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred)\n",
      "        0.5515...\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.6198...\n",
      "        >>> # the value when some element of the y_true is zero is arbitrarily high because\n",
      "        >>> # of the division by epsilon\n",
      "        >>> y_true = [1., 0., 2.4, 7.]\n",
      "        >>> y_pred = [1.2, 0.1, 2.4, 8.]\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred)\n",
      "        112589990684262.48\n",
      "    \n",
      "    mean_gamma_deviance(y_true, y_pred, *, sample_weight=None)\n",
      "        Mean Gamma deviance regression loss.\n",
      "        \n",
      "        Gamma deviance is equivalent to the Tweedie deviance with\n",
      "        the power parameter `power=2`. It is invariant to scaling of\n",
      "        the target variable, and measures relative errors.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values. Requires y_true > 0.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values. Requires y_pred > 0.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_gamma_deviance\n",
      "        >>> y_true = [2, 0.5, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_gamma_deviance(y_true, y_pred)\n",
      "        1.0568...\n",
      "    \n",
      "    mean_pinball_loss(y_true, y_pred, *, sample_weight=None, alpha=0.5, multioutput='uniform_average')\n",
      "        Pinball loss for quantile regression.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <pinball_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        alpha: float, slope of the pinball loss, default=0.5,\n",
      "            this loss is equivalent to :ref:`mean_absolute_error` when `alpha=0.5`,\n",
      "            `alpha=0.95` is minimized by estimators of the 95th percentile.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            The pinball loss output is a non-negative floating point. The best\n",
      "            value is 0.0.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_pinball_loss\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.1)\n",
      "        0.03...\n",
      "        >>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.1)\n",
      "        0.3...\n",
      "        >>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.9)\n",
      "        0.3...\n",
      "        >>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.9)\n",
      "        0.03...\n",
      "        >>> mean_pinball_loss(y_true, y_true, alpha=0.1)\n",
      "        0.0\n",
      "        >>> mean_pinball_loss(y_true, y_true, alpha=0.9)\n",
      "        0.0\n",
      "    \n",
      "    mean_poisson_deviance(y_true, y_pred, *, sample_weight=None)\n",
      "        Mean Poisson deviance regression loss.\n",
      "        \n",
      "        Poisson deviance is equivalent to the Tweedie deviance with\n",
      "        the power parameter `power=1`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values. Requires y_true >= 0.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values. Requires y_pred > 0.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_poisson_deviance\n",
      "        >>> y_true = [2, 0, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_poisson_deviance(y_true, y_pred)\n",
      "        1.4260...\n",
      "    \n",
      "    mean_squared_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', squared=True)\n",
      "        Mean squared error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        squared : bool, default=True\n",
      "            If True returns MSE value, if False returns RMSE value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_squared_error(y_true, y_pred)\n",
      "        0.375\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_squared_error(y_true, y_pred, squared=False)\n",
      "        0.612...\n",
      "        >>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
      "        >>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
      "        >>> mean_squared_error(y_true, y_pred)\n",
      "        0.708...\n",
      "        >>> mean_squared_error(y_true, y_pred, squared=False)\n",
      "        0.822...\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.41666667, 1.        ])\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.825...\n",
      "    \n",
      "    mean_squared_log_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', squared=True)\n",
      "        Mean squared logarithmic error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_log_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "        \n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors when the input is of multioutput\n",
      "                format.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        squared : bool, default=True\n",
      "            If True returns MSLE (mean squared log error) value.\n",
      "            If False returns RMSLE (root mean squared log error) value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_log_error\n",
      "        >>> y_true = [3, 5, 2.5, 7]\n",
      "        >>> y_pred = [2.5, 5, 4, 8]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)\n",
      "        0.039...\n",
      "        >>> mean_squared_log_error(y_true, y_pred, squared=False)\n",
      "        0.199...\n",
      "        >>> y_true = [[0.5, 1], [1, 2], [7, 6]]\n",
      "        >>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)\n",
      "        0.044...\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.00462428, 0.08377444])\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.060...\n",
      "    \n",
      "    mean_tweedie_deviance(y_true, y_pred, *, sample_weight=None, power=0)\n",
      "        Mean Tweedie deviance regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        power : float, default=0\n",
      "            Tweedie power parameter. Either power <= 0 or power >= 1.\n",
      "        \n",
      "            The higher `p` the less weight is given to extreme\n",
      "            deviations between true and predicted targets.\n",
      "        \n",
      "            - power < 0: Extreme stable distribution. Requires: y_pred > 0.\n",
      "            - power = 0 : Normal distribution, output corresponds to\n",
      "              mean_squared_error. y_true and y_pred can be any real numbers.\n",
      "            - power = 1 : Poisson distribution. Requires: y_true >= 0 and\n",
      "              y_pred > 0.\n",
      "            - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0\n",
      "              and y_pred > 0.\n",
      "            - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.\n",
      "            - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "            - otherwise : Positive stable distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_tweedie_deviance\n",
      "        >>> y_true = [2, 0, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_tweedie_deviance(y_true, y_pred, power=1)\n",
      "        1.4260...\n",
      "    \n",
      "    median_absolute_error(y_true, y_pred, *, multioutput='uniform_average', sample_weight=None)\n",
      "        Median absolute error regression loss.\n",
      "        \n",
      "        Median absolute error output is non-negative floating point. The best value\n",
      "        is 0.0. Read more in the :ref:`User Guide <median_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values. Array-like value defines\n",
      "            weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import median_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> median_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> median_absolute_error(y_true, y_pred)\n",
      "        0.75\n",
      "        >>> median_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.5, 1. ])\n",
      "        >>> median_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.85\n",
      "    \n",
      "    multilabel_confusion_matrix(y_true, y_pred, *, sample_weight=None, labels=None, samplewise=False)\n",
      "        Compute a confusion matrix for each class or sample.\n",
      "        \n",
      "        .. versionadded:: 0.21\n",
      "        \n",
      "        Compute class-wise (default) or sample-wise (samplewise=True) multilabel\n",
      "        confusion matrix to evaluate the accuracy of a classification, and output\n",
      "        confusion matrices for each class or sample.\n",
      "        \n",
      "        In multilabel confusion matrix :math:`MCM`, the count of true negatives\n",
      "        is :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`,\n",
      "        true positives is :math:`MCM_{:,1,1}` and false positives is\n",
      "        :math:`MCM_{:,0,1}`.\n",
      "        \n",
      "        Multiclass data will be treated as if binarized under a one-vs-rest\n",
      "        transformation. Returned confusion matrices will be in the order of\n",
      "        sorted unique labels in the union of (y_true, y_pred).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <multilabel_confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            A list of classes or column indices to select some (or to force\n",
      "            inclusion of classes absent from the data).\n",
      "        \n",
      "        samplewise : bool, default=False\n",
      "            In the multilabel case, this calculates a confusion matrix per sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        multi_confusion : ndarray of shape (n_outputs, 2, 2)\n",
      "            A 2x2 confusion matrix corresponding to each output in the input.\n",
      "            When calculating class-wise multi_confusion (default), then\n",
      "            n_outputs = n_labels; when calculating sample-wise multi_confusion\n",
      "            (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,\n",
      "            the results will be returned in the order specified in ``labels``,\n",
      "            otherwise the results will be returned in sorted order by default.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        confusion_matrix : Compute confusion matrix to evaluate the accuracy of a\n",
      "            classifier.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The `multilabel_confusion_matrix` calculates class-wise or sample-wise\n",
      "        multilabel confusion matrices, and in multiclass tasks, labels are\n",
      "        binarized under a one-vs-rest way; while\n",
      "        :func:`~sklearn.metrics.confusion_matrix` calculates one confusion matrix\n",
      "        for confusion between every two classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Multilabel-indicator case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import multilabel_confusion_matrix\n",
      "        >>> y_true = np.array([[1, 0, 1],\n",
      "        ...                    [0, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 0, 0],\n",
      "        ...                    [0, 1, 1]])\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred)\n",
      "        array([[[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[0, 1],\n",
      "                [1, 0]]])\n",
      "        \n",
      "        Multiclass case:\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred,\n",
      "        ...                             labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[[3, 1],\n",
      "                [0, 2]],\n",
      "        <BLANKLINE>\n",
      "               [[5, 0],\n",
      "                [1, 0]],\n",
      "        <BLANKLINE>\n",
      "               [[2, 1],\n",
      "                [1, 2]]])\n",
      "    \n",
      "    mutual_info_score(labels_true, labels_pred, *, contingency=None)\n",
      "        Mutual Information between two clusterings.\n",
      "        \n",
      "        The Mutual Information is a measure of the similarity between two labels\n",
      "        of the same data. Where :math:`|U_i|` is the number of the samples\n",
      "        in cluster :math:`U_i` and :math:`|V_j|` is the number of the\n",
      "        samples in cluster :math:`V_j`, the Mutual Information\n",
      "        between clusterings :math:`U` and :math:`V` is given as:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}\n",
      "            \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching :math:`U` (i.e\n",
      "        ``label_true``) with :math:`V` (i.e. ``label_pred``) will return the\n",
      "        same score value. This can be useful to measure the agreement of two\n",
      "        independent label assignments strategies on the same dataset when the\n",
      "        real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets, called :math:`U` in\n",
      "            the above formula.\n",
      "        \n",
      "        labels_pred : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets, called :math:`V` in\n",
      "            the above formula.\n",
      "        \n",
      "        contingency : {ndarray, sparse matrix} of shape             (n_classes_true, n_classes_pred), default=None\n",
      "            A contingency matrix given by the :func:`contingency_matrix` function.\n",
      "            If value is ``None``, it will be computed, otherwise the given value is\n",
      "            used, with ``labels_true`` and ``labels_pred`` ignored.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mi : float\n",
      "           Mutual information, a non-negative value, measured in nats using the\n",
      "           natural logarithm.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The logarithm used is the natural logarithm (base-e).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_mutual_info_score : Adjusted against chance Mutual Information.\n",
      "        normalized_mutual_info_score : Normalized Mutual Information.\n",
      "    \n",
      "    nan_euclidean_distances(X, Y=None, *, squared=False, missing_values=nan, copy=True)\n",
      "        Calculate the euclidean distances in the presence of missing values.\n",
      "        \n",
      "        Compute the euclidean distance between each pair of samples in X and Y,\n",
      "        where Y=X is assumed if Y=None. When calculating the distance between a\n",
      "        pair of samples, this formulation ignores feature coordinates with a\n",
      "        missing value in either sample and scales up the weight of the remaining\n",
      "        coordinates:\n",
      "        \n",
      "            dist(x,y) = sqrt(weight * sq. distance from present coordinates)\n",
      "            where,\n",
      "            weight = Total # of coordinates / # of present coordinates\n",
      "        \n",
      "        For example, the distance between ``[3, na, na, 6]`` and ``[1, na, 4, 5]``\n",
      "        is:\n",
      "        \n",
      "            .. math::\n",
      "                \\sqrt{\\frac{4}{2}((3-1)^2 + (6-5)^2)}\n",
      "        \n",
      "        If all the coordinates are missing or if there are no common present\n",
      "        coordinates then NaN is returned for that pair.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        .. versionadded:: 0.22\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape=(n_samples_X, n_features)\n",
      "        \n",
      "        Y : array-like of shape=(n_samples_Y, n_features), default=None\n",
      "        \n",
      "        squared : bool, default=False\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        missing_values : np.nan or int, default=np.nan\n",
      "            Representation of missing value.\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            Make and use a deep copy of X and Y (if Y exists).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        paired_distances : Distances between pairs of elements of X and Y.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import nan_euclidean_distances\n",
      "        >>> nan = float(\"NaN\")\n",
      "        >>> X = [[0, 1], [1, nan]]\n",
      "        >>> nan_euclidean_distances(X, X) # distance between rows of X\n",
      "        array([[0.        , 1.41421356],\n",
      "               [1.41421356, 0.        ]])\n",
      "        \n",
      "        >>> # get distance to origin\n",
      "        >>> nan_euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * John K. Dixon, \"Pattern Recognition with Partly Missing Data\",\n",
      "          IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue:\n",
      "          10, pp. 617 - 621, Oct. 1979.\n",
      "          http://ieeexplore.ieee.org/abstract/document/4310090/\n",
      "    \n",
      "    ndcg_score(y_true, y_score, *, k=None, sample_weight=None, ignore_ties=False)\n",
      "        Compute Normalized Discounted Cumulative Gain.\n",
      "        \n",
      "        Sum the true scores ranked in the order induced by the predicted scores,\n",
      "        after applying a logarithmic discount. Then divide by the best possible\n",
      "        score (Ideal DCG, obtained for a perfect ranking) to obtain a score between\n",
      "        0 and 1.\n",
      "        \n",
      "        This ranking metric yields a high value if true labels are ranked high by\n",
      "        ``y_score``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples, n_labels)\n",
      "            True targets of multilabel classification, or true scores of entities\n",
      "            to be ranked.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates, confidence values,\n",
      "            or non-thresholded measure of decisions (as returned by\n",
      "            \"decision_function\" on some classifiers).\n",
      "        \n",
      "        k : int, default=None\n",
      "            Only consider the highest k scores in the ranking. If `None`, use all\n",
      "            outputs.\n",
      "        \n",
      "        sample_weight : ndarray of shape (n_samples,), default=None\n",
      "            Sample weights. If `None`, all samples are given the same weight.\n",
      "        \n",
      "        ignore_ties : bool, default=False\n",
      "            Assume that there are no ties in y_score (which is likely to be the\n",
      "            case if y_score is continuous) for efficiency gains.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        normalized_discounted_cumulative_gain : float in [0., 1.]\n",
      "            The averaged NDCG scores for all samples.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        dcg_score : Discounted Cumulative Gain (not normalized).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        `Wikipedia entry for Discounted Cumulative Gain\n",
      "        <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_\n",
      "        \n",
      "        Jarvelin, K., & Kekalainen, J. (2002).\n",
      "        Cumulated gain-based evaluation of IR techniques. ACM Transactions on\n",
      "        Information Systems (TOIS), 20(4), 422-446.\n",
      "        \n",
      "        Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\n",
      "        A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\n",
      "        Annual Conference on Learning Theory (COLT 2013)\n",
      "        \n",
      "        McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n",
      "        performance measures efficiently in the presence of tied scores. In\n",
      "        European conference on information retrieval (pp. 414-421). Springer,\n",
      "        Berlin, Heidelberg.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import ndcg_score\n",
      "        >>> # we have groud-truth relevance of some answers to a query:\n",
      "        >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n",
      "        >>> # we predict some scores (relevance) for the answers\n",
      "        >>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n",
      "        >>> ndcg_score(true_relevance, scores)\n",
      "        0.69...\n",
      "        >>> scores = np.asarray([[.05, 1.1, 1., .5, .0]])\n",
      "        >>> ndcg_score(true_relevance, scores)\n",
      "        0.49...\n",
      "        >>> # we can set k to truncate the sum; only top k answers contribute.\n",
      "        >>> ndcg_score(true_relevance, scores, k=4)\n",
      "        0.35...\n",
      "        >>> # the normalization takes k into account so a perfect answer\n",
      "        >>> # would still get 1.0\n",
      "        >>> ndcg_score(true_relevance, true_relevance, k=4)\n",
      "        1.0\n",
      "        >>> # now we have some ties in our prediction\n",
      "        >>> scores = np.asarray([[1, 0, 0, 0, 1]])\n",
      "        >>> # by default ties are averaged, so here we get the average (normalized)\n",
      "        >>> # true relevance of our top predictions: (10 / 10 + 5 / 10) / 2 = .75\n",
      "        >>> ndcg_score(true_relevance, scores, k=1)\n",
      "        0.75\n",
      "        >>> # we can choose to ignore ties for faster results, but only\n",
      "        >>> # if we know there aren't ties in our scores, otherwise we get\n",
      "        >>> # wrong results:\n",
      "        >>> ndcg_score(true_relevance,\n",
      "        ...           scores, k=1, ignore_ties=True)\n",
      "        0.5\n",
      "    \n",
      "    normalized_mutual_info_score(labels_true, labels_pred, *, average_method='arithmetic')\n",
      "        Normalized Mutual Information between two clusterings.\n",
      "        \n",
      "        Normalized Mutual Information (NMI) is a normalization of the Mutual\n",
      "        Information (MI) score to scale the results between 0 (no mutual\n",
      "        information) and 1 (perfect correlation). In this function, mutual\n",
      "        information is normalized by some generalized mean of ``H(labels_true)``\n",
      "        and ``H(labels_pred))``, defined by the `average_method`.\n",
      "        \n",
      "        This measure is not adjusted for chance. Therefore\n",
      "        :func:`adjusted_mutual_info_score` might be preferred.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        average_method : str, default='arithmetic'\n",
      "            How to compute the normalizer in the denominator. Possible options\n",
      "            are 'min', 'geometric', 'arithmetic', and 'max'.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "               The default value of ``average_method`` changed from 'geometric' to\n",
      "               'arithmetic'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        nmi : float\n",
      "           Score between 0.0 and 1.0 in normalized nats (based on the natural\n",
      "           logarithm). 1.0 stands for perfectly complete labeling.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        v_measure_score : V-Measure (NMI with arithmetic mean option).\n",
      "        adjusted_rand_score : Adjusted Rand Index.\n",
      "        adjusted_mutual_info_score : Adjusted Mutual Information (adjusted\n",
      "            against chance).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import normalized_mutual_info_score\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the NMI is null::\n",
      "        \n",
      "          >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          ... # doctest: +SKIP\n",
      "          0.0\n",
      "    \n",
      "    pair_confusion_matrix(labels_true, labels_pred)\n",
      "        Pair confusion matrix arising from two clusterings.\n",
      "        \n",
      "        The pair confusion matrix :math:`C` computes a 2 by 2 similarity matrix\n",
      "        between two clusterings by considering all pairs of samples and counting\n",
      "        pairs that are assigned into the same or into different clusters under\n",
      "        the true and predicted clusterings.\n",
      "        \n",
      "        Considering a pair of samples that is clustered together a positive pair,\n",
      "        then as in binary classification the count of true negatives is\n",
      "        :math:`C_{00}`, false negatives is :math:`C_{10}`, true positives is\n",
      "        :math:`C_{11}` and false positives is :math:`C_{01}`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <pair_confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=integral\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=integral\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : ndarray of shape (2, 2), dtype=np.int64\n",
      "            The contingency matrix.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rand_score: Rand Score\n",
      "        adjusted_rand_score: Adjusted Rand Score\n",
      "        adjusted_mutual_info_score: Adjusted Mutual Information\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have all non-zero entries on the\n",
      "        diagonal regardless of actual label values:\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import pair_confusion_matrix\n",
      "          >>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          array([[8, 0],\n",
      "                 [0, 4]]...\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may be not always pure, hence penalized, and\n",
      "        have some off-diagonal non-zero entries:\n",
      "        \n",
      "          >>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          array([[8, 2],\n",
      "                 [0, 2]]...\n",
      "        \n",
      "        Note that the matrix is not symmetric.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. L. Hubert and P. Arabie, Comparing Partitions, Journal of\n",
      "          Classification 1985\n",
      "          https://link.springer.com/article/10.1007%2FBF01908075\n",
      "    \n",
      "    pairwise_distances(X, Y=None, metric='euclidean', *, n_jobs=None, force_all_finite=True, **kwds)\n",
      "        Compute the distance matrix from a vector array X and optional Y.\n",
      "        \n",
      "        This method takes either a vector array or a distance matrix, and returns\n",
      "        a distance matrix. If the input is a vector array, the distances are\n",
      "        computed. If the input is a distances matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a distance matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        distance between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "        \n",
      "        - From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "          'manhattan']. These metrics support sparse matrix\n",
      "          inputs.\n",
      "          ['nan_euclidean'] but it does not yet support sparse matrices.\n",
      "        \n",
      "        - From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',\n",
      "          'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n",
      "          'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n",
      "          See the documentation for scipy.spatial.distance for details on these\n",
      "          metrics. These metrics do not support sparse matrix inputs.\n",
      "        \n",
      "        Note that in the case of 'cityblock', 'cosine' and 'euclidean' (which are\n",
      "        valid scipy.spatial.distance metrics), the scikit-learn implementation\n",
      "        will be used, which is faster and has support for sparse matrices (except\n",
      "        for 'cityblock'). For a verbose description of the metrics from\n",
      "        scikit-learn, see the __doc__ of the sklearn.pairwise.distance_metrics\n",
      "        function.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "            The shape of the array should be (n_samples_X, n_samples_X) if\n",
      "            metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      "            a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        force_all_finite : bool or 'allow-nan', default=True\n",
      "            Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored\n",
      "            for a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``. The\n",
      "            possibilities are:\n",
      "        \n",
      "            - True: Force all values of array to be finite.\n",
      "            - False: accepts np.inf, np.nan, pd.NA in array.\n",
      "            - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n",
      "              cannot be infinite.\n",
      "        \n",
      "            .. versionadded:: 0.22\n",
      "               ``force_all_finite`` accepts the string ``'allow-nan'``.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Accepts `pd.NA` and converts it into `np.nan`.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        D : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)\n",
      "            A distance matrix D such that D_{i, j} is the distance between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then D_{i, j} is the distance between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pairwise_distances_chunked : Performs the same calculation as this\n",
      "            function, but returns a generator of chunks of the distance matrix, in\n",
      "            order to limit memory usage.\n",
      "        paired_distances : Computes the distances between corresponding elements\n",
      "            of two arrays.\n",
      "    \n",
      "    pairwise_distances_argmin(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance).\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        This function works with dense 2D arrays only.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_X, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : array-like of shape (n_samples_Y, n_features)\n",
      "            Arrays containing points.\n",
      "        \n",
      "        axis : int, default=1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : str or callable, default=\"euclidean\"\n",
      "            Metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        metric_kwargs : dict, default=None\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : numpy.ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        sklearn.metrics.pairwise_distances\n",
      "        sklearn.metrics.pairwise_distances_argmin_min\n",
      "    \n",
      "    pairwise_distances_argmin_min(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance). The minimal distances are\n",
      "        also returned.\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            (pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),\n",
      "             pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        axis : int, default=1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            Metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        metric_kwargs : dict, default=None\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        distances : ndarray\n",
      "            distances[i] is the distance between the i-th row in X and the\n",
      "            argmin[i]-th row in Y.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        sklearn.metrics.pairwise_distances\n",
      "        sklearn.metrics.pairwise_distances_argmin\n",
      "    \n",
      "    pairwise_distances_chunked(X, Y=None, *, reduce_func=None, metric='euclidean', n_jobs=None, working_memory=None, **kwds)\n",
      "        Generate a distance matrix chunk by chunk with optional reduction.\n",
      "        \n",
      "        In cases where not all of a pairwise distance matrix needs to be stored at\n",
      "        once, this is used to calculate pairwise distances in\n",
      "        ``working_memory``-sized chunks.  If ``reduce_func`` is given, it is run\n",
      "        on each chunk and its return values are concatenated into lists, arrays\n",
      "        or sparse matrices.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "            The shape the array should be (n_samples_X, n_samples_X) if\n",
      "            metric='precomputed' and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        reduce_func : callable, default=None\n",
      "            The function which is applied on each chunk of the distance matrix,\n",
      "            reducing it to needed values.  ``reduce_func(D_chunk, start)``\n",
      "            is called repeatedly, where ``D_chunk`` is a contiguous vertical\n",
      "            slice of the pairwise distance matrix, starting at row ``start``.\n",
      "            It should return one of: None; an array, a list, or a sparse matrix\n",
      "            of length ``D_chunk.shape[0]``; or a tuple of such objects. Returning\n",
      "            None is useful for in-place operations, rather than reductions.\n",
      "        \n",
      "            If None, pairwise_distances_chunked returns a generator of vertical\n",
      "            chunks of the distance matrix.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      "            a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            The sought maximum memory for temporary distance matrix chunks.\n",
      "            When None (default), the value of\n",
      "            ``sklearn.get_config()['working_memory']`` is used.\n",
      "        \n",
      "        `**kwds` : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Yields\n",
      "        ------\n",
      "        D_chunk : {ndarray, sparse matrix}\n",
      "            A contiguous slice of distance matrix, optionally processed by\n",
      "            ``reduce_func``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Without reduce_func:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import pairwise_distances_chunked\n",
      "        >>> X = np.random.RandomState(0).rand(5, 3)\n",
      "        >>> D_chunk = next(pairwise_distances_chunked(X))\n",
      "        >>> D_chunk\n",
      "        array([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],\n",
      "               [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],\n",
      "               [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],\n",
      "               [0.19..., 0.41..., 0.44..., 0.  ..., 0.51...],\n",
      "               [0.57..., 0.76..., 0.90..., 0.51..., 0.  ...]])\n",
      "        \n",
      "        Retrieve all neighbors and average distance within radius r:\n",
      "        \n",
      "        >>> r = .2\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r) for d in D_chunk]\n",
      "        ...     avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n",
      "        ...     return neigh, avg_dist\n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n",
      "        >>> neigh, avg_dist = next(gen)\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]\n",
      "        >>> avg_dist\n",
      "        array([0.039..., 0.        , 0.        , 0.039..., 0.        ])\n",
      "        \n",
      "        Where r is defined per sample, we need to make use of ``start``:\n",
      "        \n",
      "        >>> r = [.2, .4, .4, .3, .1]\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r[i])\n",
      "        ...              for i, d in enumerate(D_chunk, start)]\n",
      "        ...     return neigh\n",
      "        >>> neigh = next(pairwise_distances_chunked(X, reduce_func=reduce_func))\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]\n",
      "        \n",
      "        Force row-by-row generation by reducing ``working_memory``:\n",
      "        \n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func,\n",
      "        ...                                  working_memory=0)\n",
      "        >>> next(gen)\n",
      "        [array([0, 3])]\n",
      "        >>> next(gen)\n",
      "        [array([0, 1])]\n",
      "    \n",
      "    pairwise_kernels(X, Y=None, metric='linear', *, filter_params=False, n_jobs=None, **kwds)\n",
      "        Compute the kernel between arrays X and optional array Y.\n",
      "        \n",
      "        This method takes either a vector array or a kernel matrix, and returns\n",
      "        a kernel matrix. If the input is a vector array, the kernels are\n",
      "        computed. If the input is a kernel matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a kernel matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        kernel between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "            ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',\n",
      "            'laplacian', 'sigmoid', 'cosine']\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise kernels between samples, or a feature array.\n",
      "            The shape of the array should be (n_samples_X, n_samples_X) if\n",
      "            metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            A second feature array only if X has shape (n_samples_X, n_features).\n",
      "        \n",
      "        metric : str or callable, default=\"linear\"\n",
      "            The metric to use when calculating kernel between instances in a\n",
      "            feature array. If metric is a string, it must be one of the metrics\n",
      "            in pairwise.PAIRWISE_KERNEL_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a kernel matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two rows from X as input and return the corresponding\n",
      "            kernel value as a single number. This means that callables from\n",
      "            :mod:`sklearn.metrics.pairwise` are not allowed, as they operate on\n",
      "            matrices, not single samples. Use the string identifying the kernel\n",
      "            instead.\n",
      "        \n",
      "        filter_params : bool, default=False\n",
      "            Whether to filter invalid parameters or not.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the kernel function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)\n",
      "            A kernel matrix K such that K_{i, j} is the kernel between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then K_{i, j} is the kernel between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If metric is 'precomputed', Y is ignored and X is returned.\n",
      "    \n",
      "    plot_confusion_matrix(estimator, X, y_true, *, labels=None, sample_weight=None, normalize=None, display_labels=None, include_values=True, xticks_rotation='horizontal', values_format=None, cmap='viridis', ax=None, colorbar=True)\n",
      "        DEPRECATED: Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "        \n",
      "        Plot Confusion Matrix.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "        \n",
      "        .. deprecated:: 1.0\n",
      "           `plot_confusion_matrix` is deprecated in 1.0 and will be removed in\n",
      "           1.2. Use one of the following class methods:\n",
      "           :func:`~sklearn.metrics.ConfusionMatrixDisplay.from_predictions` or\n",
      "           :func:`~sklearn.metrics.ConfusionMatrixDisplay.from_estimator`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator instance\n",
      "            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "            in which the last estimator is a classifier.\n",
      "        \n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Input values.\n",
      "        \n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Target values.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            List of labels to index the matrix. This may be used to reorder or\n",
      "            select a subset of labels. If `None` is given, those that appear at\n",
      "            least once in `y_true` or `y_pred` are used in sorted order.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        normalize : {'true', 'pred', 'all'}, default=None\n",
      "            Either to normalize the counts display in the matrix:\n",
      "        \n",
      "                - if `'true'`, the confusion matrix is normalized over the true\n",
      "                  conditions (e.g. rows);\n",
      "                - if `'pred'`, the confusion matrix is normalized over the\n",
      "                  predicted conditions (e.g. columns);\n",
      "                - if `'all'`, the confusion matrix is normalized by the total\n",
      "                  number of samples;\n",
      "                - if `None` (default), the confusion matrix will not be normalized.\n",
      "        \n",
      "        display_labels : array-like of shape (n_classes,), default=None\n",
      "            Target names used for plotting. By default, `labels` will be used if\n",
      "            it is defined, otherwise the unique labels of `y_true` and `y_pred`\n",
      "            will be used.\n",
      "        \n",
      "        include_values : bool, default=True\n",
      "            Includes values in confusion matrix.\n",
      "        \n",
      "        xticks_rotation : {'vertical', 'horizontal'} or float,                         default='horizontal'\n",
      "            Rotation of xtick labels.\n",
      "        \n",
      "        values_format : str, default=None\n",
      "            Format specification for values in confusion matrix. If `None`,\n",
      "            the format specification is 'd' or '.2g' whichever is shorter.\n",
      "        \n",
      "        cmap : str or matplotlib Colormap, default='viridis'\n",
      "            Colormap recognized by matplotlib.\n",
      "        \n",
      "        ax : matplotlib Axes, default=None\n",
      "            Axes object to plot on. If `None`, a new figure and axes is\n",
      "            created.\n",
      "        \n",
      "        colorbar : bool, default=True\n",
      "            Whether or not to add a colorbar to the plot.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        display : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        confusion_matrix : Compute Confusion Matrix to evaluate the accuracy of a\n",
      "            classification.\n",
      "        ConfusionMatrixDisplay : Confusion Matrix visualization.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from sklearn.datasets import make_classification\n",
      "        >>> from sklearn.metrics import plot_confusion_matrix\n",
      "        >>> from sklearn.model_selection import train_test_split\n",
      "        >>> from sklearn.svm import SVC\n",
      "        >>> X, y = make_classification(random_state=0)\n",
      "        >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "        ...         X, y, random_state=0)\n",
      "        >>> clf = SVC(random_state=0)\n",
      "        >>> clf.fit(X_train, y_train)\n",
      "        SVC(random_state=0)\n",
      "        >>> plot_confusion_matrix(clf, X_test, y_test)  # doctest: +SKIP\n",
      "        >>> plt.show()\n",
      "    \n",
      "    plot_det_curve(estimator, X, y, *, sample_weight=None, response_method='auto', name=None, ax=None, pos_label=None, **kwargs)\n",
      "        DEPRECATED: Function plot_det_curve is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: DetCurveDisplay.from_predictions or DetCurveDisplay.from_estimator.\n",
      "        \n",
      "        Plot detection error tradeoff (DET) curve.\n",
      "        \n",
      "        Extra keyword arguments will be passed to matplotlib's `plot`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <visualizations>`.\n",
      "        \n",
      "        .. versionadded:: 0.24\n",
      "        \n",
      "        .. deprecated:: 1.0\n",
      "           `plot_det_curve` is deprecated in 1.0 and will be removed in\n",
      "           1.2. Use one of the following class methods:\n",
      "           :func:`~sklearn.metrics.DetCurveDisplay.from_predictions` or\n",
      "           :func:`~sklearn.metrics.DetCurveDisplay.from_estimator`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator instance\n",
      "            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "            in which the last estimator is a classifier.\n",
      "        \n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Input values.\n",
      "        \n",
      "        y : array-like of shape (n_samples,)\n",
      "            Target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        response_method : {'predict_proba', 'decision_function', 'auto'}             default='auto'\n",
      "            Specifies whether to use :term:`predict_proba` or\n",
      "            :term:`decision_function` as the predicted target response. If set to\n",
      "            'auto', :term:`predict_proba` is tried first and if it does not exist\n",
      "            :term:`decision_function` is tried next.\n",
      "        \n",
      "        name : str, default=None\n",
      "            Name of DET curve for labeling. If `None`, use the name of the\n",
      "            estimator.\n",
      "        \n",
      "        ax : matplotlib axes, default=None\n",
      "            Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "        \n",
      "        pos_label : str or int, default=None\n",
      "            The label of the positive class.\n",
      "            When `pos_label=None`, if `y_true` is in {-1, 1} or {0, 1},\n",
      "            `pos_label` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        **kwargs : dict\n",
      "                Additional keywords arguments passed to matplotlib `plot` function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        display : :class:`~sklearn.metrics.DetCurveDisplay`\n",
      "            Object that stores computed values.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        det_curve : Compute error rates for different probability thresholds.\n",
      "        DetCurveDisplay : DET curve visualization.\n",
      "        DetCurveDisplay.from_estimator : Plot DET curve given an estimator and\n",
      "            some data.\n",
      "        DetCurveDisplay.from_predictions : Plot DET curve given the true and\n",
      "            predicted labels.\n",
      "        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given an estimator and some data.\n",
      "        RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given the true and predicted values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from sklearn.datasets import make_classification\n",
      "        >>> from sklearn.metrics import plot_det_curve\n",
      "        >>> from sklearn.model_selection import train_test_split\n",
      "        >>> from sklearn.svm import SVC\n",
      "        >>> X, y = make_classification(n_samples=1000, random_state=0)\n",
      "        >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "        ...     X, y, test_size=0.4, random_state=0)\n",
      "        >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "        >>> plot_det_curve(clf, X_test, y_test)  # doctest: +SKIP\n",
      "        <...>\n",
      "        >>> plt.show()\n",
      "    \n",
      "    plot_precision_recall_curve(estimator, X, y, *, sample_weight=None, response_method='auto', name=None, ax=None, pos_label=None, **kwargs)\n",
      "        DEPRECATED: Function `plot_precision_recall_curve` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: PrecisionRecallDisplay.from_predictions or PrecisionRecallDisplay.from_estimator.\n",
      "        \n",
      "        Plot Precision Recall Curve for binary classifiers.\n",
      "        \n",
      "        Extra keyword arguments will be passed to matplotlib's `plot`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        .. deprecated:: 1.0\n",
      "           `plot_precision_recall_curve` is deprecated in 1.0 and will be removed in\n",
      "           1.2. Use one of the following class methods:\n",
      "           :func:`~sklearn.metrics.PrecisionRecallDisplay.from_predictions` or\n",
      "           :func:`~sklearn.metrics.PrecisionRecallDisplay.from_estimator`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator instance\n",
      "            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "            in which the last estimator is a classifier.\n",
      "        \n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Input values.\n",
      "        \n",
      "        y : array-like of shape (n_samples,)\n",
      "            Binary target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        response_method : {'predict_proba', 'decision_function', 'auto'},                       default='auto'\n",
      "            Specifies whether to use :term:`predict_proba` or\n",
      "            :term:`decision_function` as the target response. If set to 'auto',\n",
      "            :term:`predict_proba` is tried first and if it does not exist\n",
      "            :term:`decision_function` is tried next.\n",
      "        \n",
      "        name : str, default=None\n",
      "            Name for labeling curve. If `None`, the name of the\n",
      "            estimator is used.\n",
      "        \n",
      "        ax : matplotlib axes, default=None\n",
      "            Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "        \n",
      "        pos_label : str or int, default=None\n",
      "            The class considered as the positive class when computing the precision\n",
      "            and recall metrics. By default, `estimators.classes_[1]` is considered\n",
      "            as the positive class.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        **kwargs : dict\n",
      "            Keyword arguments to be passed to matplotlib's `plot`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        display : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n",
      "            Object that stores computed values.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_curve : Compute precision-recall pairs for different\n",
      "            probability thresholds.\n",
      "        PrecisionRecallDisplay : Precision Recall visualization.\n",
      "    \n",
      "    plot_roc_curve(estimator, X, y, *, sample_weight=None, drop_intermediate=True, response_method='auto', name=None, ax=None, pos_label=None, **kwargs)\n",
      "        DEPRECATED: Function :func:`plot_roc_curve` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: :meth:`sklearn.metric.RocCurveDisplay.from_predictions` or :meth:`sklearn.metric.RocCurveDisplay.from_estimator`.\n",
      "        \n",
      "        Plot Receiver operating characteristic (ROC) curve.\n",
      "        \n",
      "        Extra keyword arguments will be passed to matplotlib's `plot`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <visualizations>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator instance\n",
      "            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "            in which the last estimator is a classifier.\n",
      "        \n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Input values.\n",
      "        \n",
      "        y : array-like of shape (n_samples,)\n",
      "            Target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        drop_intermediate : bool, default=True\n",
      "            Whether to drop some suboptimal thresholds which would not appear\n",
      "            on a plotted ROC curve. This is useful in order to create lighter\n",
      "            ROC curves.\n",
      "        \n",
      "        response_method : {'predict_proba', 'decision_function', 'auto'}             default='auto'\n",
      "            Specifies whether to use :term:`predict_proba` or\n",
      "            :term:`decision_function` as the target response. If set to 'auto',\n",
      "            :term:`predict_proba` is tried first and if it does not exist\n",
      "            :term:`decision_function` is tried next.\n",
      "        \n",
      "        name : str, default=None\n",
      "            Name of ROC Curve for labeling. If `None`, use the name of the\n",
      "            estimator.\n",
      "        \n",
      "        ax : matplotlib axes, default=None\n",
      "            Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "        \n",
      "        pos_label : str or int, default=None\n",
      "            The class considered as the positive class when computing the roc auc\n",
      "            metrics. By default, `estimators.classes_[1]` is considered\n",
      "            as the positive class.\n",
      "        \n",
      "        **kwargs : dict\n",
      "            Additional keywords arguments passed to matplotlib `plot` function.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        display : :class:`~sklearn.metrics.RocCurveDisplay`\n",
      "            Object that stores computed values.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        RocCurveDisplay.from_estimator : ROC Curve visualization given an estimator\n",
      "            and some data.\n",
      "        RocCurveDisplay.from_predictions : ROC Curve visualisation given the\n",
      "            true and predicted values.\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from sklearn import datasets, metrics, model_selection, svm\n",
      "        >>> X, y = datasets.make_classification(random_state=0)\n",
      "        >>> X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
      "        ...     X, y, random_state=0)\n",
      "        >>> clf = svm.SVC(random_state=0)\n",
      "        >>> clf.fit(X_train, y_train)\n",
      "        SVC(random_state=0)\n",
      "        >>> metrics.plot_roc_curve(clf, X_test, y_test) # doctest: +SKIP\n",
      "        <...>\n",
      "        >>> plt.show()\n",
      "    \n",
      "    precision_recall_curve(y_true, probas_pred, *, pos_label=None, sample_weight=None)\n",
      "        Compute precision-recall pairs for different probability thresholds.\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The last precision and recall values are 1. and 0. respectively and do not\n",
      "        have a corresponding threshold. This ensures that the graph starts on the\n",
      "        y axis.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        probas_pred : ndarray of shape (n_samples,)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, or non-thresholded measure of decisions (as returned by\n",
      "            `decision_function` on some classifiers).\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : ndarray of shape (n_thresholds + 1,)\n",
      "            Precision values such that element i is the precision of\n",
      "            predictions with score >= thresholds[i] and the last element is 1.\n",
      "        \n",
      "        recall : ndarray of shape (n_thresholds + 1,)\n",
      "            Decreasing recall values such that element i is the recall of\n",
      "            predictions with score >= thresholds[i] and the last element is 0.\n",
      "        \n",
      "        thresholds : ndarray of shape (n_thresholds,)\n",
      "            Increasing thresholds on the decision function used to compute\n",
      "            precision and recall. n_thresholds <= len(np.unique(probas_pred)).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        PrecisionRecallDisplay.from_estimator : Plot Precision Recall Curve given\n",
      "            a binary classifier.\n",
      "        PrecisionRecallDisplay.from_predictions : Plot Precision Recall Curve\n",
      "            using predictions from a binary classifier.\n",
      "        average_precision_score : Compute average precision from prediction scores.\n",
      "        det_curve: Compute error rates for different probability thresholds.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_curve\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> precision, recall, thresholds = precision_recall_curve(\n",
      "        ...     y_true, y_scores)\n",
      "        >>> precision\n",
      "        array([0.66666667, 0.5       , 1.        , 1.        ])\n",
      "        >>> recall\n",
      "        array([1. , 0.5, 0.5, 0. ])\n",
      "        >>> thresholds\n",
      "        array([0.35, 0.4 , 0.8 ])\n",
      "    \n",
      "    precision_recall_fscore_support(y_true, y_pred, *, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division='warn')\n",
      "        Compute precision, recall, F-measure and support for each class.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The F-beta score can be interpreted as a weighted harmonic mean of\n",
      "        the precision and recall, where an F-beta score reaches its best\n",
      "        value at 1 and worst score at 0.\n",
      "        \n",
      "        The F-beta score weights recall more than precision by a factor of\n",
      "        ``beta``. ``beta == 1.0`` means recall and precision are equally important.\n",
      "        \n",
      "        The support is the number of occurrences of each class in ``y_true``.\n",
      "        \n",
      "        If ``pos_label is None`` and in binary classification, this function\n",
      "        returns the average precision, recall and F-measure if ``average``\n",
      "        is one of ``'micro'``, ``'macro'``, ``'weighted'`` or ``'samples'``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            The strength of recall versus precision in the F-score.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'binary', 'micro', 'macro', 'samples','weighted'},             default=None\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        warn_for : tuple or set, for internal use\n",
      "            This determines which warnings will be made in the case that this\n",
      "            function is being used to return only one of its metrics.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division:\n",
      "               - recall: when there are no positive labels\n",
      "               - precision: when there are no positive predictions\n",
      "               - f-score: both\n",
      "        \n",
      "            If set to \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "        \n",
      "        recall : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "        \n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "        \n",
      "        support : None (if average is not None) or array of int, shape =        [n_unique_labels]\n",
      "            The number of occurrences of each label in ``y_true``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision is undefined.\n",
      "        When ``true positive + false negative == 0``, recall is undefined.\n",
      "        In such cases, by default the metric will be set to 0, as will f-score,\n",
      "        and ``UndefinedMetricWarning`` will be raised. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Precision and recall\n",
      "               <https://en.wikipedia.org/wiki/Precision_and_recall>`_.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        .. [3] `Discriminative Methods for Multi-labeled Classification Advances\n",
      "               in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu\n",
      "               Godbole, Sunita Sarawagi\n",
      "               <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_fscore_support\n",
      "        >>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n",
      "        >>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
      "        (0.33..., 0.33..., 0.33..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        \n",
      "        It is possible to compute per-label precisions, recalls, F1-scores and\n",
      "        supports instead of averaging:\n",
      "        \n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average=None,\n",
      "        ... labels=['pig', 'dog', 'cat'])\n",
      "        (array([0.        , 0.        , 0.66...]),\n",
      "         array([0., 0., 1.]), array([0. , 0. , 0.8]),\n",
      "         array([2, 2, 2]))\n",
      "    \n",
      "    precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the precision.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division. If set to\n",
      "            \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float of shape                 (n_unique_labels,)\n",
      "            Precision of the positive class in binary classification or weighted\n",
      "            average of the precision of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support : Compute precision, recall, F-measure and\n",
      "            support for each class.\n",
      "        recall_score :  Compute the ratio ``tp / (tp + fn)`` where ``tp`` is the\n",
      "            number of true positives and ``fn`` the number of false negatives.\n",
      "        PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given\n",
      "            an estimator and some data.\n",
      "        PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given\n",
      "            binary class predictions.\n",
      "        multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "            sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision returns 0 and\n",
      "        raises ``UndefinedMetricWarning``. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import precision_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> precision_score(y_true, y_pred, average='macro')\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average='micro')\n",
      "        0.33...\n",
      "        >>> precision_score(y_true, y_pred, average='weighted')\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average=None)\n",
      "        array([0.66..., 0.        , 0.        ])\n",
      "        >>> y_pred = [0, 0, 0, 0, 0, 0]\n",
      "        >>> precision_score(y_true, y_pred, average=None)\n",
      "        array([0.33..., 0.        , 0.        ])\n",
      "        >>> precision_score(y_true, y_pred, average=None, zero_division=1)\n",
      "        array([0.33..., 1.        , 1.        ])\n",
      "        >>> # multilabel classification\n",
      "        >>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
      "        >>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
      "        >>> precision_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 1. , 1. ])\n",
      "    \n",
      "    r2_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        :math:`R^2` (coefficient of determination) regression score function.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the\n",
      "        model can be arbitrarily worse). A constant model that always\n",
      "        predicts the expected value of y, disregarding the input features,\n",
      "        would get a :math:`R^2` score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <r2_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average', 'variance_weighted'},             array-like of shape (n_outputs,) or None, default='uniform_average'\n",
      "        \n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "            Default is \"uniform_average\".\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "            .. versionchanged:: 0.19\n",
      "                Default value of multioutput is 'uniform_average'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z : float or ndarray of floats\n",
      "            The :math:`R^2` score or ndarray of scores if 'multioutput' is\n",
      "            'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Unlike most other scores, :math:`R^2` score may be negative (it need not\n",
      "        actually be the square of a quantity R).\n",
      "        \n",
      "        This metric is not well-defined for single samples and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Coefficient of determination\n",
      "                <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import r2_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.948...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> r2_score(y_true, y_pred,\n",
      "        ...          multioutput='variance_weighted')\n",
      "        0.938...\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [1, 2, 3]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [2, 2, 2]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [3, 2, 1]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        -3.0\n",
      "    \n",
      "    rand_score(labels_true, labels_pred)\n",
      "        Rand index.\n",
      "        \n",
      "        The Rand Index computes a similarity measure between two clusterings\n",
      "        by considering all pairs of samples and counting pairs that are\n",
      "        assigned in the same or different clusters in the predicted and\n",
      "        true clusterings.\n",
      "        \n",
      "        The raw RI score is:\n",
      "        \n",
      "            RI = (number of agreeing pairs) / (number of pairs)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <rand_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=integral\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=integral\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        RI : float\n",
      "           Similarity score between 0.0 and 1.0, inclusive, 1.0 stands for\n",
      "           perfect match.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_rand_score: Adjusted Rand Score\n",
      "        adjusted_mutual_info_score: Adjusted Mutual Information\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have a score of 1 even\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import rand_score\n",
      "          >>> rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may not always be pure, hence penalized:\n",
      "        \n",
      "          >>> rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          0.83...\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. L. Hubert and P. Arabie, Comparing Partitions, Journal of\n",
      "          Classification 1985\n",
      "          https://link.springer.com/article/10.1007%2FBF01908075\n",
      "        \n",
      "        .. https://en.wikipedia.org/wiki/Simple_matching_coefficient\n",
      "        \n",
      "        .. https://en.wikipedia.org/wiki/Rand_index\n",
      "    \n",
      "    recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the recall.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall. Weighted recall\n",
      "                is equal to accuracy.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division. If set to\n",
      "            \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        recall : float (if average is not None) or array of float of shape              (n_unique_labels,)\n",
      "            Recall of the positive class in binary classification or weighted\n",
      "            average of the recall of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support : Compute precision, recall, F-measure and\n",
      "            support for each class.\n",
      "        precision_score : Compute the ratio ``tp / (tp + fp)`` where ``tp`` is the\n",
      "            number of true positives and ``fp`` the number of false positives.\n",
      "        balanced_accuracy_score : Compute balanced accuracy to deal with imbalanced\n",
      "            datasets.\n",
      "        multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "            sample.\n",
      "        PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given\n",
      "            an estimator and some data.\n",
      "        PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given\n",
      "            binary class predictions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false negative == 0``, recall returns 0 and raises\n",
      "        ``UndefinedMetricWarning``. This behavior can be modified with\n",
      "        ``zero_division``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import recall_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> recall_score(y_true, y_pred, average='macro')\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='micro')\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='weighted')\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([1., 0., 0.])\n",
      "        >>> y_true = [0, 0, 0, 0, 0, 0]\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 0. , 0. ])\n",
      "        >>> recall_score(y_true, y_pred, average=None, zero_division=1)\n",
      "        array([0.5, 1. , 1. ])\n",
      "        >>> # multilabel classification\n",
      "        >>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
      "        >>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([1. , 1. , 0.5])\n",
      "    \n",
      "    roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)\n",
      "        Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n",
      "        from prediction scores.\n",
      "        \n",
      "        Note: this implementation can be used with binary, multiclass and\n",
      "        multilabel classification, but some restrictions apply (see Parameters).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            True labels or binary label indicators. The binary and multiclass cases\n",
      "            expect labels with shape (n_samples,) while the multilabel case expects\n",
      "            binary label indicators with shape (n_samples, n_classes).\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores.\n",
      "        \n",
      "            * In the binary case, it corresponds to an array of shape\n",
      "              `(n_samples,)`. Both probability estimates and non-thresholded\n",
      "              decision values can be provided. The probability estimates correspond\n",
      "              to the **probability of the class with the greater label**,\n",
      "              i.e. `estimator.classes_[1]` and thus\n",
      "              `estimator.predict_proba(X, y)[:, 1]`. The decision values\n",
      "              corresponds to the output of `estimator.decision_function(X, y)`.\n",
      "              See more information in the :ref:`User guide <roc_auc_binary>`;\n",
      "            * In the multiclass case, it corresponds to an array of shape\n",
      "              `(n_samples, n_classes)` of probability estimates provided by the\n",
      "              `predict_proba` method. The probability estimates **must**\n",
      "              sum to 1 across the possible classes. In addition, the order of the\n",
      "              class scores must correspond to the order of ``labels``,\n",
      "              if provided, or else to the numerical or lexicographical order of\n",
      "              the labels in ``y_true``. See more information in the\n",
      "              :ref:`User guide <roc_auc_multiclass>`;\n",
      "            * In the multilabel case, it corresponds to an array of shape\n",
      "              `(n_samples, n_classes)`. Probability estimates are provided by the\n",
      "              `predict_proba` method and the non-thresholded decision values by\n",
      "              the `decision_function` method. The probability estimates correspond\n",
      "              to the **probability of the class with the greater label for each\n",
      "              output** of the classifier. See more information in the\n",
      "              :ref:`User guide <roc_auc_multilabel>`.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted'} or None,             default='macro'\n",
      "            If ``None``, the scores for each class are returned. Otherwise,\n",
      "            this determines the type of averaging performed on the data:\n",
      "            Note: multiclass ROC AUC currently only handles the 'macro' and\n",
      "            'weighted' averages.\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "            Will be ignored when ``y_true`` is binary.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        max_fpr : float > 0 and <= 1, default=None\n",
      "            If not ``None``, the standardized partial AUC [2]_ over the range\n",
      "            [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,\n",
      "            should be either equal to ``None`` or ``1.0`` as AUC ROC partial\n",
      "            computation currently is not supported for multiclass.\n",
      "        \n",
      "        multi_class : {'raise', 'ovr', 'ovo'}, default='raise'\n",
      "            Only used for multiclass targets. Determines the type of configuration\n",
      "            to use. The default value raises an error, so either\n",
      "            ``'ovr'`` or ``'ovo'`` must be passed explicitly.\n",
      "        \n",
      "            ``'ovr'``:\n",
      "                Stands for One-vs-rest. Computes the AUC of each class\n",
      "                against the rest [3]_ [4]_. This\n",
      "                treats the multiclass case in the same way as the multilabel case.\n",
      "                Sensitive to class imbalance even when ``average == 'macro'``,\n",
      "                because class imbalance affects the composition of each of the\n",
      "                'rest' groupings.\n",
      "            ``'ovo'``:\n",
      "                Stands for One-vs-one. Computes the average AUC of all\n",
      "                possible pairwise combinations of classes [5]_.\n",
      "                Insensitive to class imbalance when\n",
      "                ``average == 'macro'``.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            Only used for multiclass targets. List of labels that index the\n",
      "            classes in ``y_score``. If ``None``, the numerical or lexicographical\n",
      "            order of the labels in ``y_true`` is used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        .. [2] `Analyzing a portion of the ROC curve. McClish, 1989\n",
      "                <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_\n",
      "        \n",
      "        .. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving\n",
      "               probability estimation trees (Section 6.2), CeDER Working Paper\n",
      "               #IS-00-04, Stern School of Business, New York University.\n",
      "        \n",
      "        .. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern\n",
      "                Recognition Letters, 27(8), 861-874.\n",
      "                <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_\n",
      "        \n",
      "        .. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area\n",
      "                Under the ROC Curve for Multiple Class Classification Problems.\n",
      "                Machine Learning, 45(2), 171-186.\n",
      "                <http://link.springer.com/article/10.1023/A:1010920819831>`_\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        average_precision_score : Area under the precision-recall curve.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given an estimator and some data.\n",
      "        RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given the true and predicted values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Binary case:\n",
      "        \n",
      "        >>> from sklearn.datasets import load_breast_cancer\n",
      "        >>> from sklearn.linear_model import LogisticRegression\n",
      "        >>> from sklearn.metrics import roc_auc_score\n",
      "        >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "        >>> clf = LogisticRegression(solver=\"liblinear\", random_state=0).fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.predict_proba(X)[:, 1])\n",
      "        0.99...\n",
      "        >>> roc_auc_score(y, clf.decision_function(X))\n",
      "        0.99...\n",
      "        \n",
      "        Multiclass case:\n",
      "        \n",
      "        >>> from sklearn.datasets import load_iris\n",
      "        >>> X, y = load_iris(return_X_y=True)\n",
      "        >>> clf = LogisticRegression(solver=\"liblinear\").fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')\n",
      "        0.99...\n",
      "        \n",
      "        Multilabel case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.datasets import make_multilabel_classification\n",
      "        >>> from sklearn.multioutput import MultiOutputClassifier\n",
      "        >>> X, y = make_multilabel_classification(random_state=0)\n",
      "        >>> clf = MultiOutputClassifier(clf).fit(X, y)\n",
      "        >>> # get a list of n_output containing probability arrays of shape\n",
      "        >>> # (n_samples, n_classes)\n",
      "        >>> y_pred = clf.predict_proba(X)\n",
      "        >>> # extract the positive columns for each output\n",
      "        >>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])\n",
      "        >>> roc_auc_score(y, y_pred, average=None)\n",
      "        array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])\n",
      "        >>> from sklearn.linear_model import RidgeClassifierCV\n",
      "        >>> clf = RidgeClassifierCV().fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.decision_function(X), average=None)\n",
      "        array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])\n",
      "    \n",
      "    roc_curve(y_true, y_score, *, pos_label=None, sample_weight=None, drop_intermediate=True)\n",
      "        Compute Receiver operating characteristic (ROC).\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples,)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        drop_intermediate : bool, default=True\n",
      "            Whether to drop some suboptimal thresholds which would not appear\n",
      "            on a plotted ROC curve. This is useful in order to create lighter\n",
      "            ROC curves.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               parameter *drop_intermediate*.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fpr : ndarray of shape (>2,)\n",
      "            Increasing false positive rates such that element i is the false\n",
      "            positive rate of predictions with score >= `thresholds[i]`.\n",
      "        \n",
      "        tpr : ndarray of shape (>2,)\n",
      "            Increasing true positive rates such that element `i` is the true\n",
      "            positive rate of predictions with score >= `thresholds[i]`.\n",
      "        \n",
      "        thresholds : ndarray of shape = (n_thresholds,)\n",
      "            Decreasing thresholds on the decision function used to compute\n",
      "            fpr and tpr. `thresholds[0]` represents no instances being predicted\n",
      "            and is arbitrarily set to `max(y_score) + 1`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given an estimator and some data.\n",
      "        RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given the true and predicted values.\n",
      "        det_curve: Compute error rates for different probability thresholds.\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Since the thresholds are sorted from low to high values, they\n",
      "        are reversed upon returning them to ensure they correspond to both ``fpr``\n",
      "        and ``tpr``, which are sorted in reversed order during their calculation.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        .. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n",
      "               Letters, 2006, 27(8):861-874.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y = np.array([1, 1, 2, 2])\n",
      "        >>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n",
      "        >>> fpr\n",
      "        array([0. , 0. , 0.5, 0.5, 1. ])\n",
      "        >>> tpr\n",
      "        array([0. , 0.5, 0.5, 1. , 1. ])\n",
      "        >>> thresholds\n",
      "        array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\n",
      "    \n",
      "    silhouette_samples(X, labels, *, metric='euclidean', **kwds)\n",
      "        Compute the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The Silhouette Coefficient is a measure of how well samples are clustered\n",
      "        with samples that are similar to themselves. Clustering models with a high\n",
      "        Silhouette Coefficient are said to be dense, where samples in the same\n",
      "        cluster are similar to each other, and well separated, where samples in\n",
      "        different clusters are not very similar to each other.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is 2 ``<= n_labels <= n_samples - 1``.\n",
      "        \n",
      "        This function returns the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise\n",
      "            An array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Label values for each sample.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`sklearn.metrics.pairwise.pairwise_distances`.\n",
      "            If ``X`` is the distance array itself, use \"precomputed\" as the metric.\n",
      "            Precomputed distance matrices must have 0 along the diagonal.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a ``scipy.spatial.distance`` metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : array-like of shape (n_samples,)\n",
      "            Silhouette Coefficients for each sample.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "           <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "    \n",
      "    silhouette_score(X, labels, *, metric='euclidean', sample_size=None, random_state=None, **kwds)\n",
      "        Compute the mean Silhouette Coefficient of all samples.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.  To clarify, ``b`` is the distance between a sample and the nearest\n",
      "        cluster that the sample is not a part of.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is ``2 <= n_labels <= n_samples - 1``.\n",
      "        \n",
      "        This function returns the mean Silhouette Coefficient over all samples.\n",
      "        To obtain the values for each sample, use :func:`silhouette_samples`.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters. Negative values generally indicate that a sample has\n",
      "        been assigned to the wrong cluster, as a different cluster is more similar.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise\n",
      "            An array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`metrics.pairwise.pairwise_distances\n",
      "            <sklearn.metrics.pairwise.pairwise_distances>`. If ``X`` is\n",
      "            the distance array itself, use ``metric=\"precomputed\"``.\n",
      "        \n",
      "        sample_size : int, default=None\n",
      "            The size of the sample to use when computing the Silhouette Coefficient\n",
      "            on a random subset of the data.\n",
      "            If ``sample_size is None``, no sampling is used.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for selecting a subset of samples.\n",
      "            Used when ``sample_size is not None``.\n",
      "            Pass an int for reproducible results across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : float\n",
      "            Mean Silhouette Coefficient for all samples.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "               <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "    \n",
      "    top_k_accuracy_score(y_true, y_score, *, k=2, normalize=True, sample_weight=None, labels=None)\n",
      "        Top-k Accuracy classification score.\n",
      "        \n",
      "        This metric computes the number of times where the correct label is among\n",
      "        the top `k` labels predicted (ranked by predicted scores). Note that the\n",
      "        multilabel case isn't covered here.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <top_k_accuracy_score>`\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            True labels.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores. These can be either probability estimates or\n",
      "            non-thresholded decision values (as returned by\n",
      "            :term:`decision_function` on some classifiers).\n",
      "            The binary case expects scores with shape (n_samples,) while the\n",
      "            multiclass case expects scores with shape (n_samples, n_classes).\n",
      "            In the multiclass case, the order of the class scores must\n",
      "            correspond to the order of ``labels``, if provided, or else to\n",
      "            the numerical or lexicographical order of the labels in ``y_true``.\n",
      "            If ``y_true`` does not contain all the labels, ``labels`` must be\n",
      "            provided.\n",
      "        \n",
      "        k : int, default=2\n",
      "            Number of most likely outcomes considered to find the correct label.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If `True`, return the fraction of correctly classified samples.\n",
      "            Otherwise, return the number of correctly classified samples.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights. If `None`, all samples are given the same weight.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            Multiclass only. List of labels that index the classes in ``y_score``.\n",
      "            If ``None``, the numerical or lexicographical order of the labels in\n",
      "            ``y_true`` is used. If ``y_true`` does not contain all the labels,\n",
      "            ``labels`` must be provided.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The top-k accuracy score. The best performance is 1 with\n",
      "            `normalize == True` and the number of samples with\n",
      "            `normalize == False`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        accuracy_score\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In cases where two or more labels are assigned equal predicted scores,\n",
      "        the labels with the highest indices will be chosen first. This might\n",
      "        impact the result if the correct label falls after the threshold because\n",
      "        of that.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import top_k_accuracy_score\n",
      "        >>> y_true = np.array([0, 1, 2, 2])\n",
      "        >>> y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n",
      "        ...                     [0.3, 0.4, 0.2],  # 1 is in top 2\n",
      "        ...                     [0.2, 0.4, 0.3],  # 2 is in top 2\n",
      "        ...                     [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n",
      "        >>> top_k_accuracy_score(y_true, y_score, k=2)\n",
      "        0.75\n",
      "        >>> # Not normalizing gives the number of \"correctly\" classified samples\n",
      "        >>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False)\n",
      "        3\n",
      "    \n",
      "    v_measure_score(labels_true, labels_pred, *, beta=1.0)\n",
      "        V-measure cluster labeling given a ground truth.\n",
      "        \n",
      "        This score is identical to :func:`normalized_mutual_info_score` with\n",
      "        the ``'arithmetic'`` option for averaging.\n",
      "        \n",
      "        The V-measure is the harmonic mean between homogeneity and completeness::\n",
      "        \n",
      "            v = (1 + beta) * homogeneity * completeness\n",
      "                 / (beta * homogeneity + completeness)\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n",
      "            If ``beta`` is greater than 1, ``completeness`` is weighted more\n",
      "            strongly in the calculation. If ``beta`` is less than 1,\n",
      "            ``homogeneity`` is weighted more strongly.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        v_measure : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        completeness_score\n",
      "        normalized_mutual_info_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import v_measure_score\n",
      "          >>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete be not homogeneous, hence penalized::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\n",
      "          0.8...\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          0.66...\n",
      "        \n",
      "        Labelings that have pure clusters with members coming from the same\n",
      "        classes are homogeneous but un-necessary splits harms completeness\n",
      "        and thus penalize V-measure as well::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          0.8...\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          0.66...\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally incomplete, hence the V-Measure is null::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          0.0...\n",
      "        \n",
      "        Clusters that include samples from totally different classes totally\n",
      "        destroy the homogeneity of the labeling, hence::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          0.0...\n",
      "    \n",
      "    zero_one_loss(y_true, y_pred, *, normalize=True, sample_weight=None)\n",
      "        Zero-one classification loss.\n",
      "        \n",
      "        If normalize is ``True``, return the fraction of misclassifications\n",
      "        (float), else it returns the number of misclassifications (int). The best\n",
      "        performance is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <zero_one_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If ``False``, return the number of misclassifications.\n",
      "            Otherwise, return the fraction of misclassifications.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int,\n",
      "            If ``normalize == True``, return the fraction of misclassifications\n",
      "            (float), else it returns the number of misclassifications (int).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score : Compute the accuracy score. By default, the function will\n",
      "            return the fraction of correct predictions divided by the total number\n",
      "            of predictions.\n",
      "        hamming_loss : Compute the average Hamming loss or Hamming distance between\n",
      "            two sets of samples.\n",
      "        jaccard_score : Compute the Jaccard similarity coefficient score.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multilabel classification, the zero_one_loss function corresponds to\n",
      "        the subset zero-one loss: for each sample, the entire set of labels must be\n",
      "        correctly predicted, otherwise the loss for that sample is equal to one.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import zero_one_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> zero_one_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        >>> zero_one_loss(y_true, y_pred, normalize=False)\n",
      "        1\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "\n",
      "DATA\n",
      "    SCORERS = {'accuracy': make_scorer(accuracy_score), 'adjusted_mutual_i...\n",
      "    __all__ = ['accuracy_score', 'adjusted_mutual_info_score', 'adjusted_r...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aff3df53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.model_selection in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.model_selection\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _search\n",
      "    _search_successive_halving\n",
      "    _split\n",
      "    _validation\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        sklearn.model_selection._search.ParameterGrid\n",
      "        sklearn.model_selection._search.ParameterSampler\n",
      "        sklearn.model_selection._split.BaseCrossValidator\n",
      "            sklearn.model_selection._split.LeaveOneGroupOut\n",
      "            sklearn.model_selection._split.LeaveOneOut\n",
      "            sklearn.model_selection._split.LeavePGroupsOut\n",
      "            sklearn.model_selection._split.LeavePOut\n",
      "            sklearn.model_selection._split.PredefinedSplit\n",
      "        sklearn.model_selection._split.BaseShuffleSplit\n",
      "            sklearn.model_selection._split.ShuffleSplit\n",
      "                sklearn.model_selection._split.GroupShuffleSplit\n",
      "            sklearn.model_selection._split.StratifiedShuffleSplit\n",
      "    sklearn.model_selection._search.BaseSearchCV(sklearn.base.MetaEstimatorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.model_selection._search.GridSearchCV\n",
      "        sklearn.model_selection._search.RandomizedSearchCV\n",
      "    sklearn.model_selection._split._BaseKFold(sklearn.model_selection._split.BaseCrossValidator)\n",
      "        sklearn.model_selection._split.GroupKFold\n",
      "        sklearn.model_selection._split.KFold\n",
      "        sklearn.model_selection._split.StratifiedGroupKFold\n",
      "        sklearn.model_selection._split.StratifiedKFold\n",
      "        sklearn.model_selection._split.TimeSeriesSplit\n",
      "    sklearn.model_selection._split._RepeatedSplits(builtins.object)\n",
      "        sklearn.model_selection._split.RepeatedKFold\n",
      "        sklearn.model_selection._split.RepeatedStratifiedKFold\n",
      "    \n",
      "    class BaseCrossValidator(builtins.object)\n",
      "     |  Base class for all cross-validators\n",
      "     |  \n",
      "     |  Implementations must define `_iter_test_masks` or `_iter_test_indices`.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'get_n_splits'})\n",
      "    \n",
      "    class BaseShuffleSplit(builtins.object)\n",
      "     |  BaseShuffleSplit(n_splits=10, *, test_size=None, train_size=None, random_state=None)\n",
      "     |  \n",
      "     |  Base class for ShuffleSplit and StratifiedShuffleSplit\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=10, *, test_size=None, train_size=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Randomized CV splitters may return different results for each call of\n",
      "     |      split. You can make the results identical by setting `random_state`\n",
      "     |      to an integer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'_iter_indices'})\n",
      "    \n",
      "    class GridSearchCV(BaseSearchCV)\n",
      "     |  GridSearchCV(estimator, param_grid, *, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n",
      "     |  \n",
      "     |  Exhaustive search over specified parameter values for an estimator.\n",
      "     |  \n",
      "     |  Important members are fit, predict.\n",
      "     |  \n",
      "     |  GridSearchCV implements a \"fit\" and a \"score\" method.\n",
      "     |  It also implements \"score_samples\", \"predict\", \"predict_proba\",\n",
      "     |  \"decision_function\", \"transform\" and \"inverse_transform\" if they are\n",
      "     |  implemented in the estimator used.\n",
      "     |  \n",
      "     |  The parameters of the estimator used to apply these methods are optimized\n",
      "     |  by cross-validated grid-search over a parameter grid.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <grid_search>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : estimator object\n",
      "     |      This is assumed to implement the scikit-learn estimator interface.\n",
      "     |      Either estimator needs to provide a ``score`` function,\n",
      "     |      or ``scoring`` must be passed.\n",
      "     |  \n",
      "     |  param_grid : dict or list of dictionaries\n",
      "     |      Dictionary with parameters names (`str`) as keys and lists of\n",
      "     |      parameter settings to try as values, or a list of such\n",
      "     |      dictionaries, in which case the grids spanned by each dictionary\n",
      "     |      in the list are explored. This enables searching over any sequence\n",
      "     |      of parameter settings.\n",
      "     |  \n",
      "     |  scoring : str, callable, list, tuple or dict, default=None\n",
      "     |      Strategy to evaluate the performance of the cross-validated model on\n",
      "     |      the test set.\n",
      "     |  \n",
      "     |      If `scoring` represents a single score, one can use:\n",
      "     |  \n",
      "     |      - a single string (see :ref:`scoring_parameter`);\n",
      "     |      - a callable (see :ref:`scoring`) that returns a single value.\n",
      "     |  \n",
      "     |      If `scoring` represents multiple scores, one can use:\n",
      "     |  \n",
      "     |      - a list or tuple of unique strings;\n",
      "     |      - a callable returning a dictionary where the keys are the metric\n",
      "     |        names and the values are the metric scores;\n",
      "     |      - a dictionary with metric names as keys and callables a values.\n",
      "     |  \n",
      "     |      See :ref:`multimetric_grid_search` for an example.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of jobs to run in parallel.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |      .. versionchanged:: v0.20\n",
      "     |         `n_jobs` default changed from 1 to None\n",
      "     |  \n",
      "     |  refit : bool, str, or callable, default=True\n",
      "     |      Refit an estimator using the best found parameters on the whole\n",
      "     |      dataset.\n",
      "     |  \n",
      "     |      For multiple metric evaluation, this needs to be a `str` denoting the\n",
      "     |      scorer that would be used to find the best parameters for refitting\n",
      "     |      the estimator at the end.\n",
      "     |  \n",
      "     |      Where there are considerations other than maximum score in\n",
      "     |      choosing a best estimator, ``refit`` can be set to a function which\n",
      "     |      returns the selected ``best_index_`` given ``cv_results_``. In that\n",
      "     |      case, the ``best_estimator_`` and ``best_params_`` will be set\n",
      "     |      according to the returned ``best_index_`` while the ``best_score_``\n",
      "     |      attribute will not be available.\n",
      "     |  \n",
      "     |      The refitted estimator is made available at the ``best_estimator_``\n",
      "     |      attribute and permits using ``predict`` directly on this\n",
      "     |      ``GridSearchCV`` instance.\n",
      "     |  \n",
      "     |      Also for multiple metric evaluation, the attributes ``best_index_``,\n",
      "     |      ``best_score_`` and ``best_params_`` will only be available if\n",
      "     |      ``refit`` is set and all of them will be determined w.r.t this specific\n",
      "     |      scorer.\n",
      "     |  \n",
      "     |      See ``scoring`` parameter to know more about multiple metric\n",
      "     |      evaluation.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |          Support for callable added.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross validation,\n",
      "     |      - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      "     |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "     |      other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "     |      with `shuffle=False` so the splits will be the same across calls.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  verbose : int\n",
      "     |      Controls the verbosity: the higher, the more messages.\n",
      "     |  \n",
      "     |      - >1 : the computation time for each fold and parameter candidate is\n",
      "     |        displayed;\n",
      "     |      - >2 : the score is also displayed;\n",
      "     |      - >3 : the fold and candidate parameter indexes are also displayed\n",
      "     |        together with the starting time of the computation.\n",
      "     |  \n",
      "     |  pre_dispatch : int, or str, default='2*n_jobs'\n",
      "     |      Controls the number of jobs that get dispatched during parallel\n",
      "     |      execution. Reducing this number can be useful to avoid an\n",
      "     |      explosion of memory consumption when more jobs get dispatched\n",
      "     |      than CPUs can process. This parameter can be:\n",
      "     |  \n",
      "     |          - None, in which case all the jobs are immediately\n",
      "     |            created and spawned. Use this for lightweight and\n",
      "     |            fast-running jobs, to avoid delays due to on-demand\n",
      "     |            spawning of the jobs\n",
      "     |  \n",
      "     |          - An int, giving the exact number of total jobs that are\n",
      "     |            spawned\n",
      "     |  \n",
      "     |          - A str, giving an expression as a function of n_jobs,\n",
      "     |            as in '2*n_jobs'\n",
      "     |  \n",
      "     |  error_score : 'raise' or numeric, default=np.nan\n",
      "     |      Value to assign to the score if an error occurs in estimator fitting.\n",
      "     |      If set to 'raise', the error is raised. If a numeric value is given,\n",
      "     |      FitFailedWarning is raised. This parameter does not affect the refit\n",
      "     |      step, which will always raise the error.\n",
      "     |  \n",
      "     |  return_train_score : bool, default=False\n",
      "     |      If ``False``, the ``cv_results_`` attribute will not include training\n",
      "     |      scores.\n",
      "     |      Computing training scores is used to get insights on how different\n",
      "     |      parameter settings impact the overfitting/underfitting trade-off.\n",
      "     |      However computing the scores on the training set can be computationally\n",
      "     |      expensive and is not strictly required to select the parameters that\n",
      "     |      yield the best generalization performance.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.21\n",
      "     |          Default value was changed from ``True`` to ``False``\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_results_ : dict of numpy (masked) ndarrays\n",
      "     |      A dict with keys as column headers and values as columns, that can be\n",
      "     |      imported into a pandas ``DataFrame``.\n",
      "     |  \n",
      "     |      For instance the below given table\n",
      "     |  \n",
      "     |      +------------+-----------+------------+-----------------+---+---------+\n",
      "     |      |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n",
      "     |      +============+===========+============+=================+===+=========+\n",
      "     |      |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n",
      "     |      +------------+-----------+------------+-----------------+---+---------+\n",
      "     |      |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n",
      "     |      +------------+-----------+------------+-----------------+---+---------+\n",
      "     |      |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n",
      "     |      +------------+-----------+------------+-----------------+---+---------+\n",
      "     |      |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n",
      "     |      +------------+-----------+------------+-----------------+---+---------+\n",
      "     |  \n",
      "     |      will be represented by a ``cv_results_`` dict of::\n",
      "     |  \n",
      "     |          {\n",
      "     |          'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n",
      "     |                                       mask = [False False False False]...)\n",
      "     |          'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n",
      "     |                                      mask = [ True  True False False]...),\n",
      "     |          'param_degree': masked_array(data = [2.0 3.0 -- --],\n",
      "     |                                       mask = [False False  True  True]...),\n",
      "     |          'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n",
      "     |          'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n",
      "     |          'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n",
      "     |          'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n",
      "     |          'rank_test_score'    : [2, 4, 3, 1],\n",
      "     |          'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n",
      "     |          'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n",
      "     |          'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n",
      "     |          'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n",
      "     |          'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n",
      "     |          'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n",
      "     |          'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n",
      "     |          'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n",
      "     |          'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
      "     |          }\n",
      "     |  \n",
      "     |      NOTE\n",
      "     |  \n",
      "     |      The key ``'params'`` is used to store a list of parameter\n",
      "     |      settings dicts for all the parameter candidates.\n",
      "     |  \n",
      "     |      The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      "     |      ``std_score_time`` are all in seconds.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, the scores for all the scorers are\n",
      "     |      available in the ``cv_results_`` dict at the keys ending with that\n",
      "     |      scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
      "     |      above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      "     |  \n",
      "     |  best_estimator_ : estimator\n",
      "     |      Estimator that was chosen by the search, i.e. estimator\n",
      "     |      which gave highest score (or smallest loss if specified)\n",
      "     |      on the left out data. Not available if ``refit=False``.\n",
      "     |  \n",
      "     |      See ``refit`` parameter for more information on allowed values.\n",
      "     |  \n",
      "     |  best_score_ : float\n",
      "     |      Mean cross-validated score of the best_estimator\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      "     |      specified.\n",
      "     |  \n",
      "     |      This attribute is not available if ``refit`` is a function.\n",
      "     |  \n",
      "     |  best_params_ : dict\n",
      "     |      Parameter setting that gave the best results on the hold out data.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      "     |      specified.\n",
      "     |  \n",
      "     |  best_index_ : int\n",
      "     |      The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      "     |      candidate parameter setting.\n",
      "     |  \n",
      "     |      The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      "     |      the parameter setting for the best model, that gives the highest\n",
      "     |      mean score (``search.best_score_``).\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      "     |      specified.\n",
      "     |  \n",
      "     |  scorer_ : function or a dict\n",
      "     |      Scorer function used on the held out data to choose the best\n",
      "     |      parameters for the model.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this attribute holds the validated\n",
      "     |      ``scoring`` dict which maps the scorer key to the scorer callable.\n",
      "     |  \n",
      "     |  n_splits_ : int\n",
      "     |      The number of cross-validation splits (folds/iterations).\n",
      "     |  \n",
      "     |  refit_time_ : float\n",
      "     |      Seconds used for refitting the best model on the whole dataset.\n",
      "     |  \n",
      "     |      This is present only if ``refit`` is not False.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  multimetric_ : bool\n",
      "     |      Whether or not the scorers compute several metrics.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels. This is present only if ``refit`` is specified and\n",
      "     |      the underlying estimator is a classifier.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`. Only defined if\n",
      "     |      `best_estimator_` is defined (see the documentation for the `refit`\n",
      "     |      parameter for more details) and that `best_estimator_` exposes\n",
      "     |      `n_features_in_` when fit.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Only defined if\n",
      "     |      `best_estimator_` is defined (see the documentation for the `refit`\n",
      "     |      parameter for more details) and that `best_estimator_` exposes\n",
      "     |      `feature_names_in_` when fit.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The parameters selected are those that maximize the score of the left out\n",
      "     |  data, unless an explicit score is passed in which case it is used instead.\n",
      "     |  \n",
      "     |  If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      "     |  point in the grid (and not `n_jobs` times). This is done for efficiency\n",
      "     |  reasons if individual jobs take very little time, but may raise errors if\n",
      "     |  the dataset is large and not enough memory is available.  A workaround in\n",
      "     |  this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      "     |  `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      "     |  n_jobs`.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  ---------\n",
      "     |  ParameterGrid : Generates all the combinations of a hyperparameter grid.\n",
      "     |  train_test_split : Utility function to split the data into a development\n",
      "     |      set usable for fitting a GridSearchCV instance and an evaluation set\n",
      "     |      for its final evaluation.\n",
      "     |  sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n",
      "     |      loss function.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import svm, datasets\n",
      "     |  >>> from sklearn.model_selection import GridSearchCV\n",
      "     |  >>> iris = datasets.load_iris()\n",
      "     |  >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "     |  >>> svc = svm.SVC()\n",
      "     |  >>> clf = GridSearchCV(svc, parameters)\n",
      "     |  >>> clf.fit(iris.data, iris.target)\n",
      "     |  GridSearchCV(estimator=SVC(),\n",
      "     |               param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n",
      "     |  >>> sorted(clf.cv_results_.keys())\n",
      "     |  ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n",
      "     |   'param_C', 'param_kernel', 'params',...\n",
      "     |   'rank_test_score', 'split0_test_score',...\n",
      "     |   'split2_test_score', ...\n",
      "     |   'std_fit_time', 'std_score_time', 'std_test_score']\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GridSearchCV\n",
      "     |      BaseSearchCV\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator, param_grid, *, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSearchCV:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Call decision_function on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``decision_function``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_score : ndarray of shape (n_samples,) or (n_samples, n_classes)                 or (n_samples, n_classes * (n_classes-1) / 2)\n",
      "     |          Result of the decision function for `X` based on the estimator with\n",
      "     |          the best found parameters.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, *, groups=None, **fit_params)\n",
      "     |      Run fit with all sets of parameters.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      \n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training vector, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      "     |          Target relative to X for classification or regression;\n",
      "     |          None for unsupervised learning.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "     |          instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n",
      "     |      \n",
      "     |      **fit_params : dict of str -> object\n",
      "     |          Parameters passed to the ``fit`` method of the estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Instance of fitted estimator.\n",
      "     |  \n",
      "     |  inverse_transform(self, Xt)\n",
      "     |      Call inverse_transform on the estimator with the best found params.\n",
      "     |      \n",
      "     |      Only available if the underlying estimator implements\n",
      "     |      ``inverse_transform`` and ``refit=True``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      Xt : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Result of the `inverse_transform` function for `Xt` based on the\n",
      "     |          estimator with the best found parameters.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Call predict on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |          The predicted labels or values for `X` based on the estimator with\n",
      "     |          the best found parameters.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Call predict_log_proba on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict_log_proba``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Predicted class log-probabilities for `X` based on the estimator\n",
      "     |          with the best found parameters. The order of the classes\n",
      "     |          corresponds to that in the fitted attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Call predict_proba on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict_proba``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Predicted class probabilities for `X` based on the estimator with\n",
      "     |          the best found parameters. The order of the classes corresponds\n",
      "     |          to that in the fitted attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  score(self, X, y=None)\n",
      "     |      Return the score on the given data, if the estimator has been refit.\n",
      "     |      \n",
      "     |      This uses the score defined by ``scoring`` where provided, and the\n",
      "     |      ``best_estimator_.score`` method otherwise.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input data, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      "     |          Target relative to X for classification or regression;\n",
      "     |          None for unsupervised learning.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          The score defined by ``scoring`` if provided, and the\n",
      "     |          ``best_estimator_.score`` method otherwise.\n",
      "     |  \n",
      "     |  score_samples(self, X)\n",
      "     |      Call score_samples on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``score_samples``.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.24\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : iterable\n",
      "     |          Data to predict on. Must fulfill input requirements\n",
      "     |          of the underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_score : ndarray of shape (n_samples,)\n",
      "     |          The ``best_estimator_.score_samples`` method.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Call transform on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if the underlying estimator supports ``transform`` and\n",
      "     |      ``refit=True``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          `X` transformed in the new space based on the estimator with\n",
      "     |          the best found parameters.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseSearchCV:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |      Class labels.\n",
      "     |      \n",
      "     |      Only available when `refit=True` and the estimator is a classifier.\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |      \n",
      "     |      Only available when `refit=True`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class GroupKFold(_BaseKFold)\n",
      "     |  GroupKFold(n_splits=5)\n",
      "     |  \n",
      "     |  K-fold iterator variant with non-overlapping groups.\n",
      "     |  \n",
      "     |  The same group will not appear in two different folds (the number of\n",
      "     |  distinct groups has to be at least equal to the number of folds).\n",
      "     |  \n",
      "     |  The folds are approximately balanced in the sense that the number of\n",
      "     |  distinct groups is approximately the same in each fold.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <group_k_fold>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``n_splits`` default value changed from 3 to 5.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import GroupKFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
      "     |  >>> y = np.array([1, 2, 3, 4])\n",
      "     |  >>> groups = np.array([0, 0, 2, 2])\n",
      "     |  >>> group_kfold = GroupKFold(n_splits=2)\n",
      "     |  >>> group_kfold.get_n_splits(X, y, groups)\n",
      "     |  2\n",
      "     |  >>> print(group_kfold)\n",
      "     |  GroupKFold(n_splits=2)\n",
      "     |  >>> for train_index, test_index in group_kfold.split(X, y, groups):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  ...     print(X_train, X_test, y_train, y_test)\n",
      "     |  ...\n",
      "     |  TRAIN: [0 1] TEST: [2 3]\n",
      "     |  [[1 2]\n",
      "     |   [3 4]] [[5 6]\n",
      "     |   [7 8]] [1 2] [3 4]\n",
      "     |  TRAIN: [2 3] TEST: [0 1]\n",
      "     |  [[5 6]\n",
      "     |   [7 8]] [[1 2]\n",
      "     |   [3 4]] [3 4] [1 2]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  LeaveOneGroupOut : For splitting the data according to explicit\n",
      "     |      domain-specific stratification of the dataset.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GroupKFold\n",
      "     |      _BaseKFold\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=5)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseKFold:\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class GroupShuffleSplit(ShuffleSplit)\n",
      "     |  GroupShuffleSplit(n_splits=5, *, test_size=None, train_size=None, random_state=None)\n",
      "     |  \n",
      "     |  Shuffle-Group(s)-Out cross-validation iterator\n",
      "     |  \n",
      "     |  Provides randomized train/test indices to split data according to a\n",
      "     |  third-party provided group. This group information can be used to encode\n",
      "     |  arbitrary domain specific stratifications of the samples as integers.\n",
      "     |  \n",
      "     |  For instance the groups could be the year of collection of the samples\n",
      "     |  and thus allow for cross-validation against time-based splits.\n",
      "     |  \n",
      "     |  The difference between LeavePGroupsOut and GroupShuffleSplit is that\n",
      "     |  the former generates splits using all subsets of size ``p`` unique groups,\n",
      "     |  whereas GroupShuffleSplit generates a user-determined number of random\n",
      "     |  test splits, each with a user-determined fraction of unique groups.\n",
      "     |  \n",
      "     |  For example, a less computationally intensive alternative to\n",
      "     |  ``LeavePGroupsOut(p=10)`` would be\n",
      "     |  ``GroupShuffleSplit(test_size=10, n_splits=100)``.\n",
      "     |  \n",
      "     |  Note: The parameters ``test_size`` and ``train_size`` refer to groups, and\n",
      "     |  not to samples, as in ShuffleSplit.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <group_shuffle_split>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of re-shuffling & splitting iterations.\n",
      "     |  \n",
      "     |  test_size : float, int, default=0.2\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "     |      of groups to include in the test split (rounded up). If int,\n",
      "     |      represents the absolute number of test groups. If None, the value is\n",
      "     |      set to the complement of the train size.\n",
      "     |      The default will change in version 0.21. It will remain 0.2 only\n",
      "     |      if ``train_size`` is unspecified, otherwise it will complement\n",
      "     |      the specified ``train_size``.\n",
      "     |  \n",
      "     |  train_size : float or int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the\n",
      "     |      proportion of the groups to include in the train split. If\n",
      "     |      int, represents the absolute number of train groups. If None,\n",
      "     |      the value is automatically set to the complement of the test size.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the randomness of the training and testing indices produced.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import GroupShuffleSplit\n",
      "     |  >>> X = np.ones(shape=(8, 2))\n",
      "     |  >>> y = np.ones(shape=(8, 1))\n",
      "     |  >>> groups = np.array([1, 1, 2, 2, 2, 3, 3, 3])\n",
      "     |  >>> print(groups.shape)\n",
      "     |  (8,)\n",
      "     |  >>> gss = GroupShuffleSplit(n_splits=2, train_size=.7, random_state=42)\n",
      "     |  >>> gss.get_n_splits()\n",
      "     |  2\n",
      "     |  >>> for train_idx, test_idx in gss.split(X, y, groups):\n",
      "     |  ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
      "     |  TRAIN: [2 3 4 5 6 7] TEST: [0 1]\n",
      "     |  TRAIN: [0 1 5 6 7] TEST: [2 3 4]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GroupShuffleSplit\n",
      "     |      ShuffleSplit\n",
      "     |      BaseShuffleSplit\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=5, *, test_size=None, train_size=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Randomized CV splitters may return different results for each call of\n",
      "     |      split. You can make the results identical by setting `random_state`\n",
      "     |      to an integer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class KFold(_BaseKFold)\n",
      "     |  KFold(n_splits=5, *, shuffle=False, random_state=None)\n",
      "     |  \n",
      "     |  K-Folds cross-validator\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train/test sets. Split\n",
      "     |  dataset into k consecutive folds (without shuffling by default).\n",
      "     |  \n",
      "     |  Each fold is then used once as a validation while the k - 1 remaining\n",
      "     |  folds form the training set.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <k_fold>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``n_splits`` default value changed from 3 to 5.\n",
      "     |  \n",
      "     |  shuffle : bool, default=False\n",
      "     |      Whether to shuffle the data before splitting into batches.\n",
      "     |      Note that the samples within each split will not be shuffled.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      When `shuffle` is True, `random_state` affects the ordering of the\n",
      "     |      indices, which controls the randomness of each fold. Otherwise, this\n",
      "     |      parameter has no effect.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import KFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([1, 2, 3, 4])\n",
      "     |  >>> kf = KFold(n_splits=2)\n",
      "     |  >>> kf.get_n_splits(X)\n",
      "     |  2\n",
      "     |  >>> print(kf)\n",
      "     |  KFold(n_splits=2, random_state=None, shuffle=False)\n",
      "     |  >>> for train_index, test_index in kf.split(X):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [2 3] TEST: [0 1]\n",
      "     |  TRAIN: [0 1] TEST: [2 3]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The first ``n_samples % n_splits`` folds have size\n",
      "     |  ``n_samples // n_splits + 1``, other folds have size\n",
      "     |  ``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n",
      "     |  \n",
      "     |  Randomized CV splitters may return different results for each call of\n",
      "     |  split. You can make the results identical by setting `random_state`\n",
      "     |  to an integer.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  StratifiedKFold : Takes group information into account to avoid building\n",
      "     |      folds with imbalanced class distributions (for binary or multiclass\n",
      "     |      classification tasks).\n",
      "     |  \n",
      "     |  GroupKFold : K-fold iterator variant with non-overlapping groups.\n",
      "     |  \n",
      "     |  RepeatedKFold : Repeats K-Fold n times.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KFold\n",
      "     |      _BaseKFold\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=5, *, shuffle=False, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseKFold:\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LeaveOneGroupOut(BaseCrossValidator)\n",
      "     |  Leave One Group Out cross-validator\n",
      "     |  \n",
      "     |  Provides train/test indices to split data according to a third-party\n",
      "     |  provided group. This group information can be used to encode arbitrary\n",
      "     |  domain specific stratifications of the samples as integers.\n",
      "     |  \n",
      "     |  For instance the groups could be the year of collection of the samples\n",
      "     |  and thus allow for cross-validation against time-based splits.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <leave_one_group_out>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import LeaveOneGroupOut\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
      "     |  >>> y = np.array([1, 2, 1, 2])\n",
      "     |  >>> groups = np.array([1, 1, 2, 2])\n",
      "     |  >>> logo = LeaveOneGroupOut()\n",
      "     |  >>> logo.get_n_splits(X, y, groups)\n",
      "     |  2\n",
      "     |  >>> logo.get_n_splits(groups=groups)  # 'groups' is always required\n",
      "     |  2\n",
      "     |  >>> print(logo)\n",
      "     |  LeaveOneGroupOut()\n",
      "     |  >>> for train_index, test_index in logo.split(X, y, groups):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  ...     print(X_train, X_test, y_train, y_test)\n",
      "     |  TRAIN: [2 3] TEST: [0 1]\n",
      "     |  [[5 6]\n",
      "     |   [7 8]] [[1 2]\n",
      "     |   [3 4]] [1 2] [1 2]\n",
      "     |  TRAIN: [0 1] TEST: [2 3]\n",
      "     |  [[1 2]\n",
      "     |   [3 4]] [[5 6]\n",
      "     |   [7 8]] [1 2] [1 2]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LeaveOneGroupOut\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set. This 'groups' parameter must always be specified to\n",
      "     |          calculate the number of splits, though the other parameters can be\n",
      "     |          omitted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LeaveOneOut(BaseCrossValidator)\n",
      "     |  Leave-One-Out cross-validator\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train/test sets. Each\n",
      "     |  sample is used once as a test set (singleton) while the remaining\n",
      "     |  samples form the training set.\n",
      "     |  \n",
      "     |  Note: ``LeaveOneOut()`` is equivalent to ``KFold(n_splits=n)`` and\n",
      "     |  ``LeavePOut(p=1)`` where ``n`` is the number of samples.\n",
      "     |  \n",
      "     |  Due to the high number of test sets (which is the same as the\n",
      "     |  number of samples) this cross-validation method can be very costly.\n",
      "     |  For large datasets one should favor :class:`KFold`, :class:`ShuffleSplit`\n",
      "     |  or :class:`StratifiedKFold`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <leave_one_out>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import LeaveOneOut\n",
      "     |  >>> X = np.array([[1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([1, 2])\n",
      "     |  >>> loo = LeaveOneOut()\n",
      "     |  >>> loo.get_n_splits(X)\n",
      "     |  2\n",
      "     |  >>> print(loo)\n",
      "     |  LeaveOneOut()\n",
      "     |  >>> for train_index, test_index in loo.split(X):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  ...     print(X_train, X_test, y_train, y_test)\n",
      "     |  TRAIN: [1] TEST: [0]\n",
      "     |  [[3 4]] [[1 2]] [2] [1]\n",
      "     |  TRAIN: [0] TEST: [1]\n",
      "     |  [[1 2]] [[3 4]] [1] [2]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  LeaveOneGroupOut : For splitting the data according to explicit,\n",
      "     |      domain-specific stratification of the dataset.\n",
      "     |  GroupKFold : K-fold iterator variant with non-overlapping groups.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LeaveOneOut\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_n_splits(self, X, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LeavePGroupsOut(BaseCrossValidator)\n",
      "     |  LeavePGroupsOut(n_groups)\n",
      "     |  \n",
      "     |  Leave P Group(s) Out cross-validator\n",
      "     |  \n",
      "     |  Provides train/test indices to split data according to a third-party\n",
      "     |  provided group. This group information can be used to encode arbitrary\n",
      "     |  domain specific stratifications of the samples as integers.\n",
      "     |  \n",
      "     |  For instance the groups could be the year of collection of the samples\n",
      "     |  and thus allow for cross-validation against time-based splits.\n",
      "     |  \n",
      "     |  The difference between LeavePGroupsOut and LeaveOneGroupOut is that\n",
      "     |  the former builds the test sets with all the samples assigned to\n",
      "     |  ``p`` different values of the groups while the latter uses samples\n",
      "     |  all assigned the same groups.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <leave_p_groups_out>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_groups : int\n",
      "     |      Number of groups (``p``) to leave out in the test split.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import LeavePGroupsOut\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6]])\n",
      "     |  >>> y = np.array([1, 2, 1])\n",
      "     |  >>> groups = np.array([1, 2, 3])\n",
      "     |  >>> lpgo = LeavePGroupsOut(n_groups=2)\n",
      "     |  >>> lpgo.get_n_splits(X, y, groups)\n",
      "     |  3\n",
      "     |  >>> lpgo.get_n_splits(groups=groups)  # 'groups' is always required\n",
      "     |  3\n",
      "     |  >>> print(lpgo)\n",
      "     |  LeavePGroupsOut(n_groups=2)\n",
      "     |  >>> for train_index, test_index in lpgo.split(X, y, groups):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  ...     print(X_train, X_test, y_train, y_test)\n",
      "     |  TRAIN: [2] TEST: [0 1]\n",
      "     |  [[5 6]] [[1 2]\n",
      "     |   [3 4]] [1] [1 2]\n",
      "     |  TRAIN: [1] TEST: [0 2]\n",
      "     |  [[3 4]] [[1 2]\n",
      "     |   [5 6]] [2] [1 1]\n",
      "     |  TRAIN: [0] TEST: [1 2]\n",
      "     |  [[1 2]] [[3 4]\n",
      "     |   [5 6]] [1] [2 1]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  GroupKFold : K-fold iterator variant with non-overlapping groups.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LeavePGroupsOut\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_groups)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set. This 'groups' parameter must always be specified to\n",
      "     |          calculate the number of splits, though the other parameters can be\n",
      "     |          omitted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LeavePOut(BaseCrossValidator)\n",
      "     |  LeavePOut(p)\n",
      "     |  \n",
      "     |  Leave-P-Out cross-validator\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train/test sets. This results\n",
      "     |  in testing on all distinct samples of size p, while the remaining n - p\n",
      "     |  samples form the training set in each iteration.\n",
      "     |  \n",
      "     |  Note: ``LeavePOut(p)`` is NOT equivalent to\n",
      "     |  ``KFold(n_splits=n_samples // p)`` which creates non-overlapping test sets.\n",
      "     |  \n",
      "     |  Due to the high number of iterations which grows combinatorically with the\n",
      "     |  number of samples this cross-validation method can be very costly. For\n",
      "     |  large datasets one should favor :class:`KFold`, :class:`StratifiedKFold`\n",
      "     |  or :class:`ShuffleSplit`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <leave_p_out>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  p : int\n",
      "     |      Size of the test sets. Must be strictly less than the number of\n",
      "     |      samples.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import LeavePOut\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
      "     |  >>> y = np.array([1, 2, 3, 4])\n",
      "     |  >>> lpo = LeavePOut(2)\n",
      "     |  >>> lpo.get_n_splits(X)\n",
      "     |  6\n",
      "     |  >>> print(lpo)\n",
      "     |  LeavePOut(p=2)\n",
      "     |  >>> for train_index, test_index in lpo.split(X):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [2 3] TEST: [0 1]\n",
      "     |  TRAIN: [1 3] TEST: [0 2]\n",
      "     |  TRAIN: [1 2] TEST: [0 3]\n",
      "     |  TRAIN: [0 3] TEST: [1 2]\n",
      "     |  TRAIN: [0 2] TEST: [1 3]\n",
      "     |  TRAIN: [0 1] TEST: [2 3]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LeavePOut\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, p)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_n_splits(self, X, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ParameterGrid(builtins.object)\n",
      "     |  ParameterGrid(param_grid)\n",
      "     |  \n",
      "     |  Grid of parameters with a discrete number of values for each.\n",
      "     |  \n",
      "     |  Can be used to iterate over parameter value combinations with the\n",
      "     |  Python built-in function iter.\n",
      "     |  The order of the generated parameter combinations is deterministic.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <grid_search>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  param_grid : dict of str to sequence, or sequence of such\n",
      "     |      The parameter grid to explore, as a dictionary mapping estimator\n",
      "     |      parameters to sequences of allowed values.\n",
      "     |  \n",
      "     |      An empty dict signifies default parameters.\n",
      "     |  \n",
      "     |      A sequence of dicts signifies a sequence of grids to search, and is\n",
      "     |      useful to avoid exploring parameter combinations that make no sense\n",
      "     |      or have no effect. See the examples below.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.model_selection import ParameterGrid\n",
      "     |  >>> param_grid = {'a': [1, 2], 'b': [True, False]}\n",
      "     |  >>> list(ParameterGrid(param_grid)) == (\n",
      "     |  ...    [{'a': 1, 'b': True}, {'a': 1, 'b': False},\n",
      "     |  ...     {'a': 2, 'b': True}, {'a': 2, 'b': False}])\n",
      "     |  True\n",
      "     |  \n",
      "     |  >>> grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]\n",
      "     |  >>> list(ParameterGrid(grid)) == [{'kernel': 'linear'},\n",
      "     |  ...                               {'kernel': 'rbf', 'gamma': 1},\n",
      "     |  ...                               {'kernel': 'rbf', 'gamma': 10}]\n",
      "     |  True\n",
      "     |  >>> ParameterGrid(grid)[1] == {'kernel': 'rbf', 'gamma': 1}\n",
      "     |  True\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  GridSearchCV : Uses :class:`ParameterGrid` to perform a full parallelized\n",
      "     |      parameter search.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, ind)\n",
      "     |      Get the parameters that would be ``ind``th in iteration\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ind : int\n",
      "     |          The iteration index\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict of str to any\n",
      "     |          Equal to list(self)[ind]\n",
      "     |  \n",
      "     |  __init__(self, param_grid)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Iterate over the points in the grid.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : iterator over dict of str to any\n",
      "     |          Yields dictionaries mapping each estimator parameter to one of its\n",
      "     |          allowed values.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Number of points on the grid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ParameterSampler(builtins.object)\n",
      "     |  ParameterSampler(param_distributions, n_iter, *, random_state=None)\n",
      "     |  \n",
      "     |  Generator on parameters sampled from given distributions.\n",
      "     |  \n",
      "     |  Non-deterministic iterable over random candidate combinations for hyper-\n",
      "     |  parameter search. If all parameters are presented as a list,\n",
      "     |  sampling without replacement is performed. If at least one parameter\n",
      "     |  is given as a distribution, sampling with replacement is used.\n",
      "     |  It is highly recommended to use continuous distributions for continuous\n",
      "     |  parameters.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <grid_search>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  param_distributions : dict\n",
      "     |      Dictionary with parameters names (`str`) as keys and distributions\n",
      "     |      or lists of parameters to try. Distributions must provide a ``rvs``\n",
      "     |      method for sampling (such as those from scipy.stats.distributions).\n",
      "     |      If a list is given, it is sampled uniformly.\n",
      "     |      If a list of dicts is given, first a dict is sampled uniformly, and\n",
      "     |      then a parameter is sampled using that dict as above.\n",
      "     |  \n",
      "     |  n_iter : int\n",
      "     |      Number of parameter settings that are produced.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Pseudo random number generator state used for random uniform sampling\n",
      "     |      from lists of possible values instead of scipy.stats distributions.\n",
      "     |      Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Returns\n",
      "     |  -------\n",
      "     |  params : dict of str to any\n",
      "     |      **Yields** dictionaries mapping each estimator parameter to\n",
      "     |      as sampled value.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.model_selection import ParameterSampler\n",
      "     |  >>> from scipy.stats.distributions import expon\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> param_grid = {'a':[1, 2], 'b': expon()}\n",
      "     |  >>> param_list = list(ParameterSampler(param_grid, n_iter=4,\n",
      "     |  ...                                    random_state=rng))\n",
      "     |  >>> rounded_list = [dict((k, round(v, 6)) for (k, v) in d.items())\n",
      "     |  ...                 for d in param_list]\n",
      "     |  >>> rounded_list == [{'b': 0.89856, 'a': 1},\n",
      "     |  ...                  {'b': 0.923223, 'a': 1},\n",
      "     |  ...                  {'b': 1.878964, 'a': 2},\n",
      "     |  ...                  {'b': 1.038159, 'a': 2}]\n",
      "     |  True\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, param_distributions, n_iter, *, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Number of points that will be sampled.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PredefinedSplit(BaseCrossValidator)\n",
      "     |  PredefinedSplit(test_fold)\n",
      "     |  \n",
      "     |  Predefined split cross-validator\n",
      "     |  \n",
      "     |  Provides train/test indices to split data into train/test sets using a\n",
      "     |  predefined scheme specified by the user with the ``test_fold`` parameter.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <predefined_split>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.16\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  test_fold : array-like of shape (n_samples,)\n",
      "     |      The entry ``test_fold[i]`` represents the index of the test set that\n",
      "     |      sample ``i`` belongs to. It is possible to exclude sample ``i`` from\n",
      "     |      any test set (i.e. include sample ``i`` in every training set) by\n",
      "     |      setting ``test_fold[i]`` equal to -1.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import PredefinedSplit\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> test_fold = [0, 1, -1, 1]\n",
      "     |  >>> ps = PredefinedSplit(test_fold)\n",
      "     |  >>> ps.get_n_splits()\n",
      "     |  2\n",
      "     |  >>> print(ps)\n",
      "     |  PredefinedSplit(test_fold=array([ 0,  1, -1,  1]))\n",
      "     |  >>> for train_index, test_index in ps.split():\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [1 2 3] TEST: [0]\n",
      "     |  TRAIN: [0 2] TEST: [1 3]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PredefinedSplit\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, test_fold)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X=None, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class RandomizedSearchCV(BaseSearchCV)\n",
      "     |  RandomizedSearchCV(estimator, param_distributions, *, n_iter=10, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', random_state=None, error_score=nan, return_train_score=False)\n",
      "     |  \n",
      "     |  Randomized search on hyper parameters.\n",
      "     |  \n",
      "     |  RandomizedSearchCV implements a \"fit\" and a \"score\" method.\n",
      "     |  It also implements \"score_samples\", \"predict\", \"predict_proba\",\n",
      "     |  \"decision_function\", \"transform\" and \"inverse_transform\" if they are\n",
      "     |  implemented in the estimator used.\n",
      "     |  \n",
      "     |  The parameters of the estimator used to apply these methods are optimized\n",
      "     |  by cross-validated search over parameter settings.\n",
      "     |  \n",
      "     |  In contrast to GridSearchCV, not all parameter values are tried out, but\n",
      "     |  rather a fixed number of parameter settings is sampled from the specified\n",
      "     |  distributions. The number of parameter settings that are tried is\n",
      "     |  given by n_iter.\n",
      "     |  \n",
      "     |  If all parameters are presented as a list,\n",
      "     |  sampling without replacement is performed. If at least one parameter\n",
      "     |  is given as a distribution, sampling with replacement is used.\n",
      "     |  It is highly recommended to use continuous distributions for continuous\n",
      "     |  parameters.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <randomized_parameter_search>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.14\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : estimator object\n",
      "     |      A object of that type is instantiated for each grid point.\n",
      "     |      This is assumed to implement the scikit-learn estimator interface.\n",
      "     |      Either estimator needs to provide a ``score`` function,\n",
      "     |      or ``scoring`` must be passed.\n",
      "     |  \n",
      "     |  param_distributions : dict or list of dicts\n",
      "     |      Dictionary with parameters names (`str`) as keys and distributions\n",
      "     |      or lists of parameters to try. Distributions must provide a ``rvs``\n",
      "     |      method for sampling (such as those from scipy.stats.distributions).\n",
      "     |      If a list is given, it is sampled uniformly.\n",
      "     |      If a list of dicts is given, first a dict is sampled uniformly, and\n",
      "     |      then a parameter is sampled using that dict as above.\n",
      "     |  \n",
      "     |  n_iter : int, default=10\n",
      "     |      Number of parameter settings that are sampled. n_iter trades\n",
      "     |      off runtime vs quality of the solution.\n",
      "     |  \n",
      "     |  scoring : str, callable, list, tuple or dict, default=None\n",
      "     |      Strategy to evaluate the performance of the cross-validated model on\n",
      "     |      the test set.\n",
      "     |  \n",
      "     |      If `scoring` represents a single score, one can use:\n",
      "     |  \n",
      "     |      - a single string (see :ref:`scoring_parameter`);\n",
      "     |      - a callable (see :ref:`scoring`) that returns a single value.\n",
      "     |  \n",
      "     |      If `scoring` represents multiple scores, one can use:\n",
      "     |  \n",
      "     |      - a list or tuple of unique strings;\n",
      "     |      - a callable returning a dictionary where the keys are the metric\n",
      "     |        names and the values are the metric scores;\n",
      "     |      - a dictionary with metric names as keys and callables a values.\n",
      "     |  \n",
      "     |      See :ref:`multimetric_grid_search` for an example.\n",
      "     |  \n",
      "     |      If None, the estimator's score method is used.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of jobs to run in parallel.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |      .. versionchanged:: v0.20\n",
      "     |         `n_jobs` default changed from 1 to None\n",
      "     |  \n",
      "     |  refit : bool, str, or callable, default=True\n",
      "     |      Refit an estimator using the best found parameters on the whole\n",
      "     |      dataset.\n",
      "     |  \n",
      "     |      For multiple metric evaluation, this needs to be a `str` denoting the\n",
      "     |      scorer that would be used to find the best parameters for refitting\n",
      "     |      the estimator at the end.\n",
      "     |  \n",
      "     |      Where there are considerations other than maximum score in\n",
      "     |      choosing a best estimator, ``refit`` can be set to a function which\n",
      "     |      returns the selected ``best_index_`` given the ``cv_results``. In that\n",
      "     |      case, the ``best_estimator_`` and ``best_params_`` will be set\n",
      "     |      according to the returned ``best_index_`` while the ``best_score_``\n",
      "     |      attribute will not be available.\n",
      "     |  \n",
      "     |      The refitted estimator is made available at the ``best_estimator_``\n",
      "     |      attribute and permits using ``predict`` directly on this\n",
      "     |      ``RandomizedSearchCV`` instance.\n",
      "     |  \n",
      "     |      Also for multiple metric evaluation, the attributes ``best_index_``,\n",
      "     |      ``best_score_`` and ``best_params_`` will only be available if\n",
      "     |      ``refit`` is set and all of them will be determined w.r.t this specific\n",
      "     |      scorer.\n",
      "     |  \n",
      "     |      See ``scoring`` parameter to know more about multiple metric\n",
      "     |      evaluation.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |          Support for callable added.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross validation,\n",
      "     |      - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      "     |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "     |      other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "     |      with `shuffle=False` so the splits will be the same across calls.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  verbose : int\n",
      "     |      Controls the verbosity: the higher, the more messages.\n",
      "     |  \n",
      "     |  pre_dispatch : int, or str, default='2*n_jobs'\n",
      "     |      Controls the number of jobs that get dispatched during parallel\n",
      "     |      execution. Reducing this number can be useful to avoid an\n",
      "     |      explosion of memory consumption when more jobs get dispatched\n",
      "     |      than CPUs can process. This parameter can be:\n",
      "     |  \n",
      "     |          - None, in which case all the jobs are immediately\n",
      "     |            created and spawned. Use this for lightweight and\n",
      "     |            fast-running jobs, to avoid delays due to on-demand\n",
      "     |            spawning of the jobs\n",
      "     |  \n",
      "     |          - An int, giving the exact number of total jobs that are\n",
      "     |            spawned\n",
      "     |  \n",
      "     |          - A str, giving an expression as a function of n_jobs,\n",
      "     |            as in '2*n_jobs'\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Pseudo random number generator state used for random uniform sampling\n",
      "     |      from lists of possible values instead of scipy.stats distributions.\n",
      "     |      Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  error_score : 'raise' or numeric, default=np.nan\n",
      "     |      Value to assign to the score if an error occurs in estimator fitting.\n",
      "     |      If set to 'raise', the error is raised. If a numeric value is given,\n",
      "     |      FitFailedWarning is raised. This parameter does not affect the refit\n",
      "     |      step, which will always raise the error.\n",
      "     |  \n",
      "     |  return_train_score : bool, default=False\n",
      "     |      If ``False``, the ``cv_results_`` attribute will not include training\n",
      "     |      scores.\n",
      "     |      Computing training scores is used to get insights on how different\n",
      "     |      parameter settings impact the overfitting/underfitting trade-off.\n",
      "     |      However computing the scores on the training set can be computationally\n",
      "     |      expensive and is not strictly required to select the parameters that\n",
      "     |      yield the best generalization performance.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.21\n",
      "     |          Default value was changed from ``True`` to ``False``\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_results_ : dict of numpy (masked) ndarrays\n",
      "     |      A dict with keys as column headers and values as columns, that can be\n",
      "     |      imported into a pandas ``DataFrame``.\n",
      "     |  \n",
      "     |      For instance the below given table\n",
      "     |  \n",
      "     |      +--------------+-------------+-------------------+---+---------------+\n",
      "     |      | param_kernel | param_gamma | split0_test_score |...|rank_test_score|\n",
      "     |      +==============+=============+===================+===+===============+\n",
      "     |      |    'rbf'     |     0.1     |       0.80        |...|       1       |\n",
      "     |      +--------------+-------------+-------------------+---+---------------+\n",
      "     |      |    'rbf'     |     0.2     |       0.84        |...|       3       |\n",
      "     |      +--------------+-------------+-------------------+---+---------------+\n",
      "     |      |    'rbf'     |     0.3     |       0.70        |...|       2       |\n",
      "     |      +--------------+-------------+-------------------+---+---------------+\n",
      "     |  \n",
      "     |      will be represented by a ``cv_results_`` dict of::\n",
      "     |  \n",
      "     |          {\n",
      "     |          'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],\n",
      "     |                                        mask = False),\n",
      "     |          'param_gamma'  : masked_array(data = [0.1 0.2 0.3], mask = False),\n",
      "     |          'split0_test_score'  : [0.80, 0.84, 0.70],\n",
      "     |          'split1_test_score'  : [0.82, 0.50, 0.70],\n",
      "     |          'mean_test_score'    : [0.81, 0.67, 0.70],\n",
      "     |          'std_test_score'     : [0.01, 0.24, 0.00],\n",
      "     |          'rank_test_score'    : [1, 3, 2],\n",
      "     |          'split0_train_score' : [0.80, 0.92, 0.70],\n",
      "     |          'split1_train_score' : [0.82, 0.55, 0.70],\n",
      "     |          'mean_train_score'   : [0.81, 0.74, 0.70],\n",
      "     |          'std_train_score'    : [0.01, 0.19, 0.00],\n",
      "     |          'mean_fit_time'      : [0.73, 0.63, 0.43],\n",
      "     |          'std_fit_time'       : [0.01, 0.02, 0.01],\n",
      "     |          'mean_score_time'    : [0.01, 0.06, 0.04],\n",
      "     |          'std_score_time'     : [0.00, 0.00, 0.00],\n",
      "     |          'params'             : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...],\n",
      "     |          }\n",
      "     |  \n",
      "     |      NOTE\n",
      "     |  \n",
      "     |      The key ``'params'`` is used to store a list of parameter\n",
      "     |      settings dicts for all the parameter candidates.\n",
      "     |  \n",
      "     |      The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      "     |      ``std_score_time`` are all in seconds.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, the scores for all the scorers are\n",
      "     |      available in the ``cv_results_`` dict at the keys ending with that\n",
      "     |      scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
      "     |      above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      "     |  \n",
      "     |  best_estimator_ : estimator\n",
      "     |      Estimator that was chosen by the search, i.e. estimator\n",
      "     |      which gave highest score (or smallest loss if specified)\n",
      "     |      on the left out data. Not available if ``refit=False``.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this attribute is present only if\n",
      "     |      ``refit`` is specified.\n",
      "     |  \n",
      "     |      See ``refit`` parameter for more information on allowed values.\n",
      "     |  \n",
      "     |  best_score_ : float\n",
      "     |      Mean cross-validated score of the best_estimator.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      "     |      ``False``. See ``refit`` parameter for more information.\n",
      "     |  \n",
      "     |      This attribute is not available if ``refit`` is a function.\n",
      "     |  \n",
      "     |  best_params_ : dict\n",
      "     |      Parameter setting that gave the best results on the hold out data.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      "     |      ``False``. See ``refit`` parameter for more information.\n",
      "     |  \n",
      "     |  best_index_ : int\n",
      "     |      The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      "     |      candidate parameter setting.\n",
      "     |  \n",
      "     |      The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      "     |      the parameter setting for the best model, that gives the highest\n",
      "     |      mean score (``search.best_score_``).\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      "     |      ``False``. See ``refit`` parameter for more information.\n",
      "     |  \n",
      "     |  scorer_ : function or a dict\n",
      "     |      Scorer function used on the held out data to choose the best\n",
      "     |      parameters for the model.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this attribute holds the validated\n",
      "     |      ``scoring`` dict which maps the scorer key to the scorer callable.\n",
      "     |  \n",
      "     |  n_splits_ : int\n",
      "     |      The number of cross-validation splits (folds/iterations).\n",
      "     |  \n",
      "     |  refit_time_ : float\n",
      "     |      Seconds used for refitting the best model on the whole dataset.\n",
      "     |  \n",
      "     |      This is present only if ``refit`` is not False.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  multimetric_ : bool\n",
      "     |      Whether or not the scorers compute several metrics.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels. This is present only if ``refit`` is specified and\n",
      "     |      the underlying estimator is a classifier.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`. Only defined if\n",
      "     |      `best_estimator_` is defined (see the documentation for the `refit`\n",
      "     |      parameter for more details) and that `best_estimator_` exposes\n",
      "     |      `n_features_in_` when fit.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Only defined if\n",
      "     |      `best_estimator_` is defined (see the documentation for the `refit`\n",
      "     |      parameter for more details) and that `best_estimator_` exposes\n",
      "     |      `feature_names_in_` when fit.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  GridSearchCV : Does exhaustive search over a grid of parameters.\n",
      "     |  ParameterSampler : A generator over parameter settings, constructed from\n",
      "     |      param_distributions.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The parameters selected are those that maximize the score of the held-out\n",
      "     |  data, according to the scoring parameter.\n",
      "     |  \n",
      "     |  If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      "     |  parameter setting(and not `n_jobs` times). This is done for efficiency\n",
      "     |  reasons if individual jobs take very little time, but may raise errors if\n",
      "     |  the dataset is large and not enough memory is available.  A workaround in\n",
      "     |  this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      "     |  `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      "     |  n_jobs`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.linear_model import LogisticRegression\n",
      "     |  >>> from sklearn.model_selection import RandomizedSearchCV\n",
      "     |  >>> from scipy.stats import uniform\n",
      "     |  >>> iris = load_iris()\n",
      "     |  >>> logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n",
      "     |  ...                               random_state=0)\n",
      "     |  >>> distributions = dict(C=uniform(loc=0, scale=4),\n",
      "     |  ...                      penalty=['l2', 'l1'])\n",
      "     |  >>> clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
      "     |  >>> search = clf.fit(iris.data, iris.target)\n",
      "     |  >>> search.best_params_\n",
      "     |  {'C': 2..., 'penalty': 'l1'}\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomizedSearchCV\n",
      "     |      BaseSearchCV\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator, param_distributions, *, n_iter=10, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', random_state=None, error_score=nan, return_train_score=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSearchCV:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Call decision_function on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``decision_function``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_score : ndarray of shape (n_samples,) or (n_samples, n_classes)                 or (n_samples, n_classes * (n_classes-1) / 2)\n",
      "     |          Result of the decision function for `X` based on the estimator with\n",
      "     |          the best found parameters.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, *, groups=None, **fit_params)\n",
      "     |      Run fit with all sets of parameters.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      \n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training vector, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      "     |          Target relative to X for classification or regression;\n",
      "     |          None for unsupervised learning.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "     |          instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n",
      "     |      \n",
      "     |      **fit_params : dict of str -> object\n",
      "     |          Parameters passed to the ``fit`` method of the estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Instance of fitted estimator.\n",
      "     |  \n",
      "     |  inverse_transform(self, Xt)\n",
      "     |      Call inverse_transform on the estimator with the best found params.\n",
      "     |      \n",
      "     |      Only available if the underlying estimator implements\n",
      "     |      ``inverse_transform`` and ``refit=True``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      Xt : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Result of the `inverse_transform` function for `Xt` based on the\n",
      "     |          estimator with the best found parameters.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Call predict on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |          The predicted labels or values for `X` based on the estimator with\n",
      "     |          the best found parameters.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Call predict_log_proba on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict_log_proba``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Predicted class log-probabilities for `X` based on the estimator\n",
      "     |          with the best found parameters. The order of the classes\n",
      "     |          corresponds to that in the fitted attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Call predict_proba on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict_proba``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Predicted class probabilities for `X` based on the estimator with\n",
      "     |          the best found parameters. The order of the classes corresponds\n",
      "     |          to that in the fitted attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  score(self, X, y=None)\n",
      "     |      Return the score on the given data, if the estimator has been refit.\n",
      "     |      \n",
      "     |      This uses the score defined by ``scoring`` where provided, and the\n",
      "     |      ``best_estimator_.score`` method otherwise.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input data, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      "     |          Target relative to X for classification or regression;\n",
      "     |          None for unsupervised learning.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          The score defined by ``scoring`` if provided, and the\n",
      "     |          ``best_estimator_.score`` method otherwise.\n",
      "     |  \n",
      "     |  score_samples(self, X)\n",
      "     |      Call score_samples on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``score_samples``.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.24\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : iterable\n",
      "     |          Data to predict on. Must fulfill input requirements\n",
      "     |          of the underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_score : ndarray of shape (n_samples,)\n",
      "     |          The ``best_estimator_.score_samples`` method.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Call transform on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if the underlying estimator supports ``transform`` and\n",
      "     |      ``refit=True``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          `X` transformed in the new space based on the estimator with\n",
      "     |          the best found parameters.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseSearchCV:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |      Class labels.\n",
      "     |      \n",
      "     |      Only available when `refit=True` and the estimator is a classifier.\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |      \n",
      "     |      Only available when `refit=True`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RepeatedKFold(_RepeatedSplits)\n",
      "     |  RepeatedKFold(*, n_splits=5, n_repeats=10, random_state=None)\n",
      "     |  \n",
      "     |  Repeated K-Fold cross validator.\n",
      "     |  \n",
      "     |  Repeats K-Fold n times with different randomization in each repetition.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <repeated_k_fold>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |  n_repeats : int, default=10\n",
      "     |      Number of times cross-validator needs to be repeated.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the randomness of each repeated cross-validation instance.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import RepeatedKFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)\n",
      "     |  >>> for train_index, test_index in rkf.split(X):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  ...\n",
      "     |  TRAIN: [0 1] TEST: [2 3]\n",
      "     |  TRAIN: [2 3] TEST: [0 1]\n",
      "     |  TRAIN: [1 2] TEST: [0 3]\n",
      "     |  TRAIN: [0 3] TEST: [1 2]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Randomized CV splitters may return different results for each call of\n",
      "     |  split. You can make the results identical by setting `random_state`\n",
      "     |  to an integer.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RepeatedKFold\n",
      "     |      _RepeatedSplits\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_splits=5, n_repeats=10, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _RepeatedSplits:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |          ``np.zeros(n_samples)`` may be used as a placeholder.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |          ``np.zeros(n_samples)`` may be used as a placeholder.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generates indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _RepeatedSplits:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class RepeatedStratifiedKFold(_RepeatedSplits)\n",
      "     |  RepeatedStratifiedKFold(*, n_splits=5, n_repeats=10, random_state=None)\n",
      "     |  \n",
      "     |  Repeated Stratified K-Fold cross validator.\n",
      "     |  \n",
      "     |  Repeats Stratified K-Fold n times with different randomization in each\n",
      "     |  repetition.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <repeated_k_fold>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |  n_repeats : int, default=10\n",
      "     |      Number of times cross-validator needs to be repeated.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the generation of the random states for each repetition.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import RepeatedStratifiedKFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,\n",
      "     |  ...     random_state=36851234)\n",
      "     |  >>> for train_index, test_index in rskf.split(X, y):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  ...\n",
      "     |  TRAIN: [1 2] TEST: [0 3]\n",
      "     |  TRAIN: [0 3] TEST: [1 2]\n",
      "     |  TRAIN: [1 3] TEST: [0 2]\n",
      "     |  TRAIN: [0 2] TEST: [1 3]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Randomized CV splitters may return different results for each call of\n",
      "     |  split. You can make the results identical by setting `random_state`\n",
      "     |  to an integer.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RepeatedKFold : Repeats K-Fold n times.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RepeatedStratifiedKFold\n",
      "     |      _RepeatedSplits\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_splits=5, n_repeats=10, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _RepeatedSplits:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |          ``np.zeros(n_samples)`` may be used as a placeholder.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |          ``np.zeros(n_samples)`` may be used as a placeholder.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generates indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _RepeatedSplits:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ShuffleSplit(BaseShuffleSplit)\n",
      "     |  ShuffleSplit(n_splits=10, *, test_size=None, train_size=None, random_state=None)\n",
      "     |  \n",
      "     |  Random permutation cross-validator\n",
      "     |  \n",
      "     |  Yields indices to split data into training and test sets.\n",
      "     |  \n",
      "     |  Note: contrary to other cross-validation strategies, random splits\n",
      "     |  do not guarantee that all folds will be different, although this is\n",
      "     |  still very likely for sizeable datasets.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ShuffleSplit>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=10\n",
      "     |      Number of re-shuffling & splitting iterations.\n",
      "     |  \n",
      "     |  test_size : float or int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "     |      of the dataset to include in the test split. If int, represents the\n",
      "     |      absolute number of test samples. If None, the value is set to the\n",
      "     |      complement of the train size. If ``train_size`` is also None, it will\n",
      "     |      be set to 0.1.\n",
      "     |  \n",
      "     |  train_size : float or int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the\n",
      "     |      proportion of the dataset to include in the train split. If\n",
      "     |      int, represents the absolute number of train samples. If None,\n",
      "     |      the value is automatically set to the complement of the test size.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the randomness of the training and testing indices produced.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import ShuffleSplit\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6]])\n",
      "     |  >>> y = np.array([1, 2, 1, 2, 1, 2])\n",
      "     |  >>> rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n",
      "     |  >>> rs.get_n_splits(X)\n",
      "     |  5\n",
      "     |  >>> print(rs)\n",
      "     |  ShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)\n",
      "     |  >>> for train_index, test_index in rs.split(X):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  TRAIN: [1 3 0 4] TEST: [5 2]\n",
      "     |  TRAIN: [4 0 2 5] TEST: [1 3]\n",
      "     |  TRAIN: [1 2 4 0] TEST: [3 5]\n",
      "     |  TRAIN: [3 4 1 0] TEST: [5 2]\n",
      "     |  TRAIN: [3 5 1 0] TEST: [2 4]\n",
      "     |  >>> rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25,\n",
      "     |  ...                   random_state=0)\n",
      "     |  >>> for train_index, test_index in rs.split(X):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  TRAIN: [1 3 0] TEST: [5 2]\n",
      "     |  TRAIN: [4 0 2] TEST: [1 3]\n",
      "     |  TRAIN: [1 2 4] TEST: [3 5]\n",
      "     |  TRAIN: [3 4 1] TEST: [5 2]\n",
      "     |  TRAIN: [3 5 1] TEST: [2 4]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ShuffleSplit\n",
      "     |      BaseShuffleSplit\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=10, *, test_size=None, train_size=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Randomized CV splitters may return different results for each call of\n",
      "     |      split. You can make the results identical by setting `random_state`\n",
      "     |      to an integer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StratifiedGroupKFold(_BaseKFold)\n",
      "     |  StratifiedGroupKFold(n_splits=5, shuffle=False, random_state=None)\n",
      "     |  \n",
      "     |  Stratified K-Folds iterator variant with non-overlapping groups.\n",
      "     |  \n",
      "     |  This cross-validation object is a variation of StratifiedKFold attempts to\n",
      "     |  return stratified folds with non-overlapping groups. The folds are made by\n",
      "     |  preserving the percentage of samples for each class.\n",
      "     |  \n",
      "     |  The same group will not appear in two different folds (the number of\n",
      "     |  distinct groups has to be at least equal to the number of folds).\n",
      "     |  \n",
      "     |  The difference between GroupKFold and StratifiedGroupKFold is that\n",
      "     |  the former attempts to create balanced folds such that the number of\n",
      "     |  distinct groups is approximately the same in each fold, whereas\n",
      "     |  StratifiedGroupKFold attempts to create folds which preserve the\n",
      "     |  percentage of samples for each class as much as possible given the\n",
      "     |  constraint of non-overlapping groups between splits.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |  shuffle : bool, default=False\n",
      "     |      Whether to shuffle each class's samples before splitting into batches.\n",
      "     |      Note that the samples within each split will not be shuffled.\n",
      "     |      This implementation can only shuffle groups that have approximately the\n",
      "     |      same y distribution, no global shuffle will be performed.\n",
      "     |  \n",
      "     |  random_state : int or RandomState instance, default=None\n",
      "     |      When `shuffle` is True, `random_state` affects the ordering of the\n",
      "     |      indices, which controls the randomness of each fold for each class.\n",
      "     |      Otherwise, leave `random_state` as `None`.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import StratifiedGroupKFold\n",
      "     |  >>> X = np.ones((17, 2))\n",
      "     |  >>> y = np.array([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "     |  >>> groups = np.array([1, 1, 2, 2, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 7, 8, 8])\n",
      "     |  >>> cv = StratifiedGroupKFold(n_splits=3)\n",
      "     |  >>> for train_idxs, test_idxs in cv.split(X, y, groups):\n",
      "     |  ...     print(\"TRAIN:\", groups[train_idxs])\n",
      "     |  ...     print(\"      \", y[train_idxs])\n",
      "     |  ...     print(\" TEST:\", groups[test_idxs])\n",
      "     |  ...     print(\"      \", y[test_idxs])\n",
      "     |  TRAIN: [1 1 2 2 4 5 5 5 5 8 8]\n",
      "     |         [0 0 1 1 1 0 0 0 0 0 0]\n",
      "     |   TEST: [3 3 3 6 6 7]\n",
      "     |         [1 1 1 0 0 0]\n",
      "     |  TRAIN: [3 3 3 4 5 5 5 5 6 6 7]\n",
      "     |         [1 1 1 1 0 0 0 0 0 0 0]\n",
      "     |   TEST: [1 1 2 2 8 8]\n",
      "     |         [0 0 1 1 0 0]\n",
      "     |  TRAIN: [1 1 2 2 3 3 3 6 6 7 8 8]\n",
      "     |         [0 0 1 1 1 1 1 0 0 0 0 0]\n",
      "     |   TEST: [4 5 5 5 5]\n",
      "     |         [1 0 0 0 0]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The implementation is designed to:\n",
      "     |  \n",
      "     |  * Mimic the behavior of StratifiedKFold as much as possible for trivial\n",
      "     |    groups (e.g. when each group contains only one sample).\n",
      "     |  * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n",
      "     |    ``y = [1, 0]`` should not change the indices generated.\n",
      "     |  * Stratify based on samples as much as possible while keeping\n",
      "     |    non-overlapping groups constraint. That means that in some cases when\n",
      "     |    there is a small number of groups containing a large number of samples\n",
      "     |    the stratification will not be possible and the behavior will be close\n",
      "     |    to GroupKFold.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  StratifiedKFold: Takes class information into account to build folds which\n",
      "     |      retain class distributions (for binary or multiclass classification\n",
      "     |      tasks).\n",
      "     |  \n",
      "     |  GroupKFold: K-fold iterator variant with non-overlapping groups.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StratifiedGroupKFold\n",
      "     |      _BaseKFold\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=5, shuffle=False, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseKFold:\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StratifiedKFold(_BaseKFold)\n",
      "     |  StratifiedKFold(n_splits=5, *, shuffle=False, random_state=None)\n",
      "     |  \n",
      "     |  Stratified K-Folds cross-validator.\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train/test sets.\n",
      "     |  \n",
      "     |  This cross-validation object is a variation of KFold that returns\n",
      "     |  stratified folds. The folds are made by preserving the percentage of\n",
      "     |  samples for each class.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <stratified_k_fold>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``n_splits`` default value changed from 3 to 5.\n",
      "     |  \n",
      "     |  shuffle : bool, default=False\n",
      "     |      Whether to shuffle each class's samples before splitting into batches.\n",
      "     |      Note that the samples within each split will not be shuffled.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      When `shuffle` is True, `random_state` affects the ordering of the\n",
      "     |      indices, which controls the randomness of each fold for each class.\n",
      "     |      Otherwise, leave `random_state` as `None`.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import StratifiedKFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> skf = StratifiedKFold(n_splits=2)\n",
      "     |  >>> skf.get_n_splits(X, y)\n",
      "     |  2\n",
      "     |  >>> print(skf)\n",
      "     |  StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
      "     |  >>> for train_index, test_index in skf.split(X, y):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [1 3] TEST: [0 2]\n",
      "     |  TRAIN: [0 2] TEST: [1 3]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The implementation is designed to:\n",
      "     |  \n",
      "     |  * Generate test sets such that all contain the same distribution of\n",
      "     |    classes, or as close as possible.\n",
      "     |  * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n",
      "     |    ``y = [1, 0]`` should not change the indices generated.\n",
      "     |  * Preserve order dependencies in the dataset ordering, when\n",
      "     |    ``shuffle=False``: all samples from class k in some test set were\n",
      "     |    contiguous in y, or separated in y by samples from classes other than k.\n",
      "     |  * Generate test sets where the smallest and largest differ by at most one\n",
      "     |    sample.\n",
      "     |  \n",
      "     |  .. versionchanged:: 0.22\n",
      "     |      The previous implementation did not follow the last constraint.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StratifiedKFold\n",
      "     |      _BaseKFold\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=5, *, shuffle=False, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  split(self, X, y, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |          Note that providing ``y`` is sufficient to generate the splits and\n",
      "     |          hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
      "     |          ``X`` instead of actual training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |          Stratification is done based on the y labels.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Randomized CV splitters may return different results for each call of\n",
      "     |      split. You can make the results identical by setting `random_state`\n",
      "     |      to an integer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseKFold:\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StratifiedShuffleSplit(BaseShuffleSplit)\n",
      "     |  StratifiedShuffleSplit(n_splits=10, *, test_size=None, train_size=None, random_state=None)\n",
      "     |  \n",
      "     |  Stratified ShuffleSplit cross-validator\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train/test sets.\n",
      "     |  \n",
      "     |  This cross-validation object is a merge of StratifiedKFold and\n",
      "     |  ShuffleSplit, which returns stratified randomized folds. The folds\n",
      "     |  are made by preserving the percentage of samples for each class.\n",
      "     |  \n",
      "     |  Note: like the ShuffleSplit strategy, stratified random splits\n",
      "     |  do not guarantee that all folds will be different, although this is\n",
      "     |  still very likely for sizeable datasets.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <stratified_shuffle_split>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=10\n",
      "     |      Number of re-shuffling & splitting iterations.\n",
      "     |  \n",
      "     |  test_size : float or int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "     |      of the dataset to include in the test split. If int, represents the\n",
      "     |      absolute number of test samples. If None, the value is set to the\n",
      "     |      complement of the train size. If ``train_size`` is also None, it will\n",
      "     |      be set to 0.1.\n",
      "     |  \n",
      "     |  train_size : float or int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the\n",
      "     |      proportion of the dataset to include in the train split. If\n",
      "     |      int, represents the absolute number of train samples. If None,\n",
      "     |      the value is automatically set to the complement of the test size.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the randomness of the training and testing indices produced.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import StratifiedShuffleSplit\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 0, 1, 1, 1])\n",
      "     |  >>> sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
      "     |  >>> sss.get_n_splits(X, y)\n",
      "     |  5\n",
      "     |  >>> print(sss)\n",
      "     |  StratifiedShuffleSplit(n_splits=5, random_state=0, ...)\n",
      "     |  >>> for train_index, test_index in sss.split(X, y):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [5 2 3] TEST: [4 1 0]\n",
      "     |  TRAIN: [5 1 4] TEST: [0 2 3]\n",
      "     |  TRAIN: [5 0 2] TEST: [4 3 1]\n",
      "     |  TRAIN: [4 1 0] TEST: [2 3 5]\n",
      "     |  TRAIN: [0 5 1] TEST: [3 4 2]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StratifiedShuffleSplit\n",
      "     |      BaseShuffleSplit\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=10, *, test_size=None, train_size=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  split(self, X, y, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |          Note that providing ``y`` is sufficient to generate the splits and\n",
      "     |          hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
      "     |          ``X`` instead of actual training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_labels)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |          Stratification is done based on the y labels.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Randomized CV splitters may return different results for each call of\n",
      "     |      split. You can make the results identical by setting `random_state`\n",
      "     |      to an integer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TimeSeriesSplit(_BaseKFold)\n",
      "     |  TimeSeriesSplit(n_splits=5, *, max_train_size=None, test_size=None, gap=0)\n",
      "     |  \n",
      "     |  Time Series cross-validator\n",
      "     |  \n",
      "     |  Provides train/test indices to split time series data samples\n",
      "     |  that are observed at fixed time intervals, in train/test sets.\n",
      "     |  In each split, test indices must be higher than before, and thus shuffling\n",
      "     |  in cross validator is inappropriate.\n",
      "     |  \n",
      "     |  This cross-validation object is a variation of :class:`KFold`.\n",
      "     |  In the kth split, it returns first k folds as train set and the\n",
      "     |  (k+1)th fold as test set.\n",
      "     |  \n",
      "     |  Note that unlike standard cross-validation methods, successive\n",
      "     |  training sets are supersets of those that come before them.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <time_series_split>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of splits. Must be at least 2.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``n_splits`` default value changed from 3 to 5.\n",
      "     |  \n",
      "     |  max_train_size : int, default=None\n",
      "     |      Maximum size for a single training set.\n",
      "     |  \n",
      "     |  test_size : int, default=None\n",
      "     |      Used to limit the size of the test set. Defaults to\n",
      "     |      ``n_samples // (n_splits + 1)``, which is the maximum allowed value\n",
      "     |      with ``gap=0``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  gap : int, default=0\n",
      "     |      Number of samples to exclude from the end of each train set before\n",
      "     |      the test set.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import TimeSeriesSplit\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
      "     |  >>> tscv = TimeSeriesSplit()\n",
      "     |  >>> print(tscv)\n",
      "     |  TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n",
      "     |  >>> for train_index, test_index in tscv.split(X):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [0] TEST: [1]\n",
      "     |  TRAIN: [0 1] TEST: [2]\n",
      "     |  TRAIN: [0 1 2] TEST: [3]\n",
      "     |  TRAIN: [0 1 2 3] TEST: [4]\n",
      "     |  TRAIN: [0 1 2 3 4] TEST: [5]\n",
      "     |  >>> # Fix test_size to 2 with 12 samples\n",
      "     |  >>> X = np.random.randn(12, 2)\n",
      "     |  >>> y = np.random.randint(0, 2, 12)\n",
      "     |  >>> tscv = TimeSeriesSplit(n_splits=3, test_size=2)\n",
      "     |  >>> for train_index, test_index in tscv.split(X):\n",
      "     |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [0 1 2 3 4 5] TEST: [6 7]\n",
      "     |  TRAIN: [0 1 2 3 4 5 6 7] TEST: [8 9]\n",
      "     |  TRAIN: [0 1 2 3 4 5 6 7 8 9] TEST: [10 11]\n",
      "     |  >>> # Add in a 2 period gap\n",
      "     |  >>> tscv = TimeSeriesSplit(n_splits=3, test_size=2, gap=2)\n",
      "     |  >>> for train_index, test_index in tscv.split(X):\n",
      "     |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [0 1 2 3] TEST: [6 7]\n",
      "     |  TRAIN: [0 1 2 3 4 5] TEST: [8 9]\n",
      "     |  TRAIN: [0 1 2 3 4 5 6 7] TEST: [10 11]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The training set has size ``i * n_samples // (n_splits + 1)\n",
      "     |  + n_samples % (n_splits + 1)`` in the ``i`` th split,\n",
      "     |  with a test set of size ``n_samples//(n_splits + 1)`` by default,\n",
      "     |  where ``n_samples`` is the number of samples.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TimeSeriesSplit\n",
      "     |      _BaseKFold\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=5, *, max_train_size=None, test_size=None, gap=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseKFold:\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    check_cv(cv=5, y=None, *, classifier=False)\n",
      "        Input checker utility for building a cross-validator\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "            - None, to use the default 5-fold cross validation,\n",
      "            - integer, to specify the number of folds.\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable yielding (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For integer/None inputs, if classifier is True and ``y`` is either\n",
      "            binary or multiclass, :class:`StratifiedKFold` is used. In all other\n",
      "            cases, :class:`KFold` is used.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                ``cv`` default value changed from 3-fold to 5-fold.\n",
      "        \n",
      "        y : array-like, default=None\n",
      "            The target variable for supervised learning problems.\n",
      "        \n",
      "        classifier : bool, default=False\n",
      "            Whether the task is a classification task, in which case\n",
      "            stratified KFold will be used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        checked_cv : a cross-validator instance.\n",
      "            The return value is a cross-validator which generates the train/test\n",
      "            splits via the ``split`` method.\n",
      "    \n",
      "    cross_val_predict(estimator, X, y=None, *, groups=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', method='predict')\n",
      "        Generate cross-validated estimates for each input data point.\n",
      "        \n",
      "        The data is split according to the cv parameter. Each sample belongs\n",
      "        to exactly one test set, and its prediction is computed with an\n",
      "        estimator fitted on the corresponding training set.\n",
      "        \n",
      "        Passing these predictions into an evaluation metric may not be a valid\n",
      "        way to measure generalization performance. Results can differ from\n",
      "        :func:`cross_validate` and :func:`cross_val_score` unless all tests sets\n",
      "        have equal size and the metric decomposes over samples.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cross_validation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit' and 'predict'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            The data to fit. Can be, for example a list, or an array at least 2d.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None\n",
      "            The target variable to try to predict in the case of\n",
      "            supervised learning.\n",
      "        \n",
      "        groups : array-like of shape (n_samples,), default=None\n",
      "            Group labels for the samples used while splitting the dataset into\n",
      "            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "            instance (e.g., :class:`GroupKFold`).\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable that generates (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For int/None inputs, if the estimator is a classifier and ``y`` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and\n",
      "            predicting are parallelized over the cross-validation splits.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            The verbosity level.\n",
      "        \n",
      "        fit_params : dict, default=None\n",
      "            Parameters to pass to the fit method of the estimator.\n",
      "        \n",
      "        pre_dispatch : int or str, default='2*n_jobs'\n",
      "            Controls the number of jobs that get dispatched during parallel\n",
      "            execution. Reducing this number can be useful to avoid an\n",
      "            explosion of memory consumption when more jobs get dispatched\n",
      "            than CPUs can process. This parameter can be:\n",
      "        \n",
      "                - None, in which case all the jobs are immediately\n",
      "                  created and spawned. Use this for lightweight and\n",
      "                  fast-running jobs, to avoid delays due to on-demand\n",
      "                  spawning of the jobs\n",
      "        \n",
      "                - An int, giving the exact number of total jobs that are\n",
      "                  spawned\n",
      "        \n",
      "                - A str, giving an expression as a function of n_jobs,\n",
      "                  as in '2*n_jobs'\n",
      "        \n",
      "        method : {'predict', 'predict_proba', 'predict_log_proba',               'decision_function'}, default='predict'\n",
      "            The method to be invoked by `estimator`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        predictions : ndarray\n",
      "            This is the result of calling `method`. Shape:\n",
      "        \n",
      "                - When `method` is 'predict' and in special case where `method` is\n",
      "                  'decision_function' and the target is binary: (n_samples,)\n",
      "                - When `method` is one of {'predict_proba', 'predict_log_proba',\n",
      "                  'decision_function'} (unless special case above):\n",
      "                  (n_samples, n_classes)\n",
      "                - If `estimator` is :term:`multioutput`, an extra dimension\n",
      "                  'n_outputs' is added to the end of each shape above.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        cross_val_score : Calculate score for each CV split.\n",
      "        cross_validate : Calculate one or more scores and timings for each CV\n",
      "            split.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In the case that one or more classes are absent in a training portion, a\n",
      "        default score needs to be assigned to all instances for that class if\n",
      "        ``method`` produces columns per class, as in {'decision_function',\n",
      "        'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is\n",
      "        0.  In order to ensure finite output, we approximate negative infinity by\n",
      "        the minimum finite float value for the dtype in other cases.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import datasets, linear_model\n",
      "        >>> from sklearn.model_selection import cross_val_predict\n",
      "        >>> diabetes = datasets.load_diabetes()\n",
      "        >>> X = diabetes.data[:150]\n",
      "        >>> y = diabetes.target[:150]\n",
      "        >>> lasso = linear_model.Lasso()\n",
      "        >>> y_pred = cross_val_predict(lasso, X, y, cv=3)\n",
      "    \n",
      "    cross_val_score(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', error_score=nan)\n",
      "        Evaluate a score by cross-validation.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cross_validation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            The data to fit. Can be for example a list, or an array.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None\n",
      "            The target variable to try to predict in the case of\n",
      "            supervised learning.\n",
      "        \n",
      "        groups : array-like of shape (n_samples,), default=None\n",
      "            Group labels for the samples used while splitting the dataset into\n",
      "            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "            instance (e.g., :class:`GroupKFold`).\n",
      "        \n",
      "        scoring : str or callable, default=None\n",
      "            A str (see model evaluation documentation) or\n",
      "            a scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)`` which should return only\n",
      "            a single value.\n",
      "        \n",
      "            Similar to :func:`cross_validate`\n",
      "            but only a single metric is permitted.\n",
      "        \n",
      "            If `None`, the estimator's default scorer (if available) is used.\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - `None`, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable that generates (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For `int`/`None` inputs, if the estimator is a classifier and `y` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                `cv` default value if `None` changed from 3-fold to 5-fold.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and computing\n",
      "            the score are parallelized over the cross-validation splits.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            The verbosity level.\n",
      "        \n",
      "        fit_params : dict, default=None\n",
      "            Parameters to pass to the fit method of the estimator.\n",
      "        \n",
      "        pre_dispatch : int or str, default='2*n_jobs'\n",
      "            Controls the number of jobs that get dispatched during parallel\n",
      "            execution. Reducing this number can be useful to avoid an\n",
      "            explosion of memory consumption when more jobs get dispatched\n",
      "            than CPUs can process. This parameter can be:\n",
      "        \n",
      "                - ``None``, in which case all the jobs are immediately\n",
      "                  created and spawned. Use this for lightweight and\n",
      "                  fast-running jobs, to avoid delays due to on-demand\n",
      "                  spawning of the jobs\n",
      "        \n",
      "                - An int, giving the exact number of total jobs that are\n",
      "                  spawned\n",
      "        \n",
      "                - A str, giving an expression as a function of n_jobs,\n",
      "                  as in '2*n_jobs'\n",
      "        \n",
      "        error_score : 'raise' or numeric, default=np.nan\n",
      "            Value to assign to the score if an error occurs in estimator fitting.\n",
      "            If set to 'raise', the error is raised.\n",
      "            If a numeric value is given, FitFailedWarning is raised.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scores : ndarray of float of shape=(len(list(cv)),)\n",
      "            Array of scores of the estimator for each run of the cross validation.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import datasets, linear_model\n",
      "        >>> from sklearn.model_selection import cross_val_score\n",
      "        >>> diabetes = datasets.load_diabetes()\n",
      "        >>> X = diabetes.data[:150]\n",
      "        >>> y = diabetes.target[:150]\n",
      "        >>> lasso = linear_model.Lasso()\n",
      "        >>> print(cross_val_score(lasso, X, y, cv=3))\n",
      "        [0.33150734 0.08022311 0.03531764]\n",
      "        \n",
      "        See Also\n",
      "        ---------\n",
      "        cross_validate : To run cross-validation on multiple metrics and also to\n",
      "            return train scores, fit times and score times.\n",
      "        \n",
      "        cross_val_predict : Get predictions from each split of cross-validation for\n",
      "            diagnostic purposes.\n",
      "        \n",
      "        sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n",
      "            loss function.\n",
      "    \n",
      "    cross_validate(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', return_train_score=False, return_estimator=False, error_score=nan)\n",
      "        Evaluate metric(s) by cross-validation and also record fit/score times.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <multimetric_cross_validation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            The data to fit. Can be for example a list, or an array.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None\n",
      "            The target variable to try to predict in the case of\n",
      "            supervised learning.\n",
      "        \n",
      "        groups : array-like of shape (n_samples,), default=None\n",
      "            Group labels for the samples used while splitting the dataset into\n",
      "            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "            instance (e.g., :class:`GroupKFold`).\n",
      "        \n",
      "        scoring : str, callable, list, tuple, or dict, default=None\n",
      "            Strategy to evaluate the performance of the cross-validated model on\n",
      "            the test set.\n",
      "        \n",
      "            If `scoring` represents a single score, one can use:\n",
      "        \n",
      "            - a single string (see :ref:`scoring_parameter`);\n",
      "            - a callable (see :ref:`scoring`) that returns a single value.\n",
      "        \n",
      "            If `scoring` represents multiple scores, one can use:\n",
      "        \n",
      "            - a list or tuple of unique strings;\n",
      "            - a callable returning a dictionary where the keys are the metric\n",
      "              names and the values are the metric scores;\n",
      "            - a dictionary with metric names as keys and callables a values.\n",
      "        \n",
      "            See :ref:`multimetric_grid_search` for an example.\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable yielding (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For int/None inputs, if the estimator is a classifier and ``y`` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`.Fold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and computing\n",
      "            the score are parallelized over the cross-validation splits.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            The verbosity level.\n",
      "        \n",
      "        fit_params : dict, default=None\n",
      "            Parameters to pass to the fit method of the estimator.\n",
      "        \n",
      "        pre_dispatch : int or str, default='2*n_jobs'\n",
      "            Controls the number of jobs that get dispatched during parallel\n",
      "            execution. Reducing this number can be useful to avoid an\n",
      "            explosion of memory consumption when more jobs get dispatched\n",
      "            than CPUs can process. This parameter can be:\n",
      "        \n",
      "                - None, in which case all the jobs are immediately\n",
      "                  created and spawned. Use this for lightweight and\n",
      "                  fast-running jobs, to avoid delays due to on-demand\n",
      "                  spawning of the jobs\n",
      "        \n",
      "                - An int, giving the exact number of total jobs that are\n",
      "                  spawned\n",
      "        \n",
      "                - A str, giving an expression as a function of n_jobs,\n",
      "                  as in '2*n_jobs'\n",
      "        \n",
      "        return_train_score : bool, default=False\n",
      "            Whether to include train scores.\n",
      "            Computing training scores is used to get insights on how different\n",
      "            parameter settings impact the overfitting/underfitting trade-off.\n",
      "            However computing the scores on the training set can be computationally\n",
      "            expensive and is not strictly required to select the parameters that\n",
      "            yield the best generalization performance.\n",
      "        \n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "            .. versionchanged:: 0.21\n",
      "                Default value was changed from ``True`` to ``False``\n",
      "        \n",
      "        return_estimator : bool, default=False\n",
      "            Whether to return the estimators fitted on each split.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        error_score : 'raise' or numeric, default=np.nan\n",
      "            Value to assign to the score if an error occurs in estimator fitting.\n",
      "            If set to 'raise', the error is raised.\n",
      "            If a numeric value is given, FitFailedWarning is raised.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scores : dict of float arrays of shape (n_splits,)\n",
      "            Array of scores of the estimator for each run of the cross validation.\n",
      "        \n",
      "            A dict of arrays containing the score/time arrays for each scorer is\n",
      "            returned. The possible keys for this ``dict`` are:\n",
      "        \n",
      "                ``test_score``\n",
      "                    The score array for test scores on each cv split.\n",
      "                    Suffix ``_score`` in ``test_score`` changes to a specific\n",
      "                    metric like ``test_r2`` or ``test_auc`` if there are\n",
      "                    multiple scoring metrics in the scoring parameter.\n",
      "                ``train_score``\n",
      "                    The score array for train scores on each cv split.\n",
      "                    Suffix ``_score`` in ``train_score`` changes to a specific\n",
      "                    metric like ``train_r2`` or ``train_auc`` if there are\n",
      "                    multiple scoring metrics in the scoring parameter.\n",
      "                    This is available only if ``return_train_score`` parameter\n",
      "                    is ``True``.\n",
      "                ``fit_time``\n",
      "                    The time for fitting the estimator on the train\n",
      "                    set for each cv split.\n",
      "                ``score_time``\n",
      "                    The time for scoring the estimator on the test set for each\n",
      "                    cv split. (Note time for scoring on the train set is not\n",
      "                    included even if ``return_train_score`` is set to ``True``\n",
      "                ``estimator``\n",
      "                    The estimator objects for each cv split.\n",
      "                    This is available only if ``return_estimator`` parameter\n",
      "                    is set to ``True``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import datasets, linear_model\n",
      "        >>> from sklearn.model_selection import cross_validate\n",
      "        >>> from sklearn.metrics import make_scorer\n",
      "        >>> from sklearn.metrics import confusion_matrix\n",
      "        >>> from sklearn.svm import LinearSVC\n",
      "        >>> diabetes = datasets.load_diabetes()\n",
      "        >>> X = diabetes.data[:150]\n",
      "        >>> y = diabetes.target[:150]\n",
      "        >>> lasso = linear_model.Lasso()\n",
      "        \n",
      "        Single metric evaluation using ``cross_validate``\n",
      "        \n",
      "        >>> cv_results = cross_validate(lasso, X, y, cv=3)\n",
      "        >>> sorted(cv_results.keys())\n",
      "        ['fit_time', 'score_time', 'test_score']\n",
      "        >>> cv_results['test_score']\n",
      "        array([0.33150734, 0.08022311, 0.03531764])\n",
      "        \n",
      "        Multiple metric evaluation using ``cross_validate``\n",
      "        (please refer the ``scoring`` parameter doc for more information)\n",
      "        \n",
      "        >>> scores = cross_validate(lasso, X, y, cv=3,\n",
      "        ...                         scoring=('r2', 'neg_mean_squared_error'),\n",
      "        ...                         return_train_score=True)\n",
      "        >>> print(scores['test_neg_mean_squared_error'])\n",
      "        [-3635.5... -3573.3... -6114.7...]\n",
      "        >>> print(scores['train_r2'])\n",
      "        [0.28010158 0.39088426 0.22784852]\n",
      "        \n",
      "        See Also\n",
      "        ---------\n",
      "        cross_val_score : Run cross-validation for single metric evaluation.\n",
      "        \n",
      "        cross_val_predict : Get predictions from each split of cross-validation for\n",
      "            diagnostic purposes.\n",
      "        \n",
      "        sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n",
      "            loss function.\n",
      "    \n",
      "    learning_curve(estimator, X, y, *, groups=None, train_sizes=array([0.1  , 0.325, 0.55 , 0.775, 1.   ]), cv=None, scoring=None, exploit_incremental_learning=False, n_jobs=None, pre_dispatch='all', verbose=0, shuffle=False, random_state=None, error_score=nan, return_times=False, fit_params=None)\n",
      "        Learning curve.\n",
      "        \n",
      "        Determines cross-validated training and test scores for different training\n",
      "        set sizes.\n",
      "        \n",
      "        A cross-validation generator splits the whole dataset k times in training\n",
      "        and test data. Subsets of the training set with varying sizes will be used\n",
      "        to train the estimator and a score for each training subset size and the\n",
      "        test set will be computed. Afterwards, the scores will be averaged over\n",
      "        all k runs for each training subset size.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <learning_curve>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : object type that implements the \"fit\" and \"predict\" methods\n",
      "            An object of that type which is cloned for each validation.\n",
      "        \n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            Training vector, where `n_samples` is the number of samples and\n",
      "            `n_features` is the number of features.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Target relative to X for classification or regression;\n",
      "            None for unsupervised learning.\n",
      "        \n",
      "        groups : array-like of  shape (n_samples,), default=None\n",
      "            Group labels for the samples used while splitting the dataset into\n",
      "            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "            instance (e.g., :class:`GroupKFold`).\n",
      "        \n",
      "        train_sizes : array-like of shape (n_ticks,),             default=np.linspace(0.1, 1.0, 5)\n",
      "            Relative or absolute numbers of training examples that will be used to\n",
      "            generate the learning curve. If the dtype is float, it is regarded as a\n",
      "            fraction of the maximum size of the training set (that is determined\n",
      "            by the selected validation method), i.e. it has to be within (0, 1].\n",
      "            Otherwise it is interpreted as absolute sizes of the training sets.\n",
      "            Note that for classification the number of samples usually have to\n",
      "            be big enough to contain at least one sample from each class.\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable yielding (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For int/None inputs, if the estimator is a classifier and ``y`` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "        \n",
      "        scoring : str or callable, default=None\n",
      "            A str (see model evaluation documentation) or\n",
      "            a scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "        \n",
      "        exploit_incremental_learning : bool, default=False\n",
      "            If the estimator supports incremental learning, this will be\n",
      "            used to speed up fitting for different training set sizes.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and computing\n",
      "            the score are parallelized over the different training and test sets.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        pre_dispatch : int or str, default='all'\n",
      "            Number of predispatched jobs for parallel execution (default is\n",
      "            all). The option can reduce the allocated memory. The str can\n",
      "            be an expression like '2*n_jobs'.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Controls the verbosity: the higher, the more messages.\n",
      "        \n",
      "        shuffle : bool, default=False\n",
      "            Whether to shuffle training data before taking prefixes of it\n",
      "            based on``train_sizes``.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Used when ``shuffle`` is True. Pass an int for reproducible\n",
      "            output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        error_score : 'raise' or numeric, default=np.nan\n",
      "            Value to assign to the score if an error occurs in estimator fitting.\n",
      "            If set to 'raise', the error is raised.\n",
      "            If a numeric value is given, FitFailedWarning is raised.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        return_times : bool, default=False\n",
      "            Whether to return the fit and score times.\n",
      "        \n",
      "        fit_params : dict, default=None\n",
      "            Parameters to pass to the fit method of the estimator.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        train_sizes_abs : array of shape (n_unique_ticks,)\n",
      "            Numbers of training examples that has been used to generate the\n",
      "            learning curve. Note that the number of ticks might be less\n",
      "            than n_ticks because duplicate entries will be removed.\n",
      "        \n",
      "        train_scores : array of shape (n_ticks, n_cv_folds)\n",
      "            Scores on training sets.\n",
      "        \n",
      "        test_scores : array of shape (n_ticks, n_cv_folds)\n",
      "            Scores on test set.\n",
      "        \n",
      "        fit_times : array of shape (n_ticks, n_cv_folds)\n",
      "            Times spent for fitting in seconds. Only present if ``return_times``\n",
      "            is True.\n",
      "        \n",
      "        score_times : array of shape (n_ticks, n_cv_folds)\n",
      "            Times spent for scoring in seconds. Only present if ``return_times``\n",
      "            is True.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        See :ref:`examples/model_selection/plot_learning_curve.py\n",
      "        <sphx_glr_auto_examples_model_selection_plot_learning_curve.py>`\n",
      "    \n",
      "    permutation_test_score(estimator, X, y, *, groups=None, cv=None, n_permutations=100, n_jobs=None, random_state=0, verbose=0, scoring=None, fit_params=None)\n",
      "        Evaluate the significance of a cross-validated score with permutations\n",
      "        \n",
      "        Permutes targets to generate 'randomized data' and compute the empirical\n",
      "        p-value against the null hypothesis that features and targets are\n",
      "        independent.\n",
      "        \n",
      "        The p-value represents the fraction of randomized data sets where the\n",
      "        estimator performed as well or better than in the original data. A small\n",
      "        p-value suggests that there is a real dependency between features and\n",
      "        targets which has been used by the estimator to give good predictions.\n",
      "        A large p-value may be due to lack of real dependency between features\n",
      "        and targets or the estimator was not able to use the dependency to\n",
      "        give good predictions.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <permutation_test_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        X : array-like of shape at least 2D\n",
      "            The data to fit.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n",
      "            The target variable to try to predict in the case of\n",
      "            supervised learning.\n",
      "        \n",
      "        groups : array-like of shape (n_samples,), default=None\n",
      "            Labels to constrain permutation within groups, i.e. ``y`` values\n",
      "            are permuted among samples with the same group identifier.\n",
      "            When not specified, ``y`` values are permuted among all samples.\n",
      "        \n",
      "            When a grouped cross-validator is used, the group labels are\n",
      "            also passed on to the ``split`` method of the cross-validator. The\n",
      "            cross-validator uses them for grouping the samples  while splitting\n",
      "            the dataset into train/test set.\n",
      "        \n",
      "        scoring : str or callable, default=None\n",
      "            A single str (see :ref:`scoring_parameter`) or a callable\n",
      "            (see :ref:`scoring`) to evaluate the predictions on the test set.\n",
      "        \n",
      "            If `None` the estimator's score method is used.\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - `None`, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable yielding (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For `int`/`None` inputs, if the estimator is a classifier and `y` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                `cv` default value if `None` changed from 3-fold to 5-fold.\n",
      "        \n",
      "        n_permutations : int, default=100\n",
      "            Number of times to permute ``y``.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and computing\n",
      "            the cross-validated score are parallelized over the permutations.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=0\n",
      "            Pass an int for reproducible output for permutation of\n",
      "            ``y`` values among samples. See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            The verbosity level.\n",
      "        \n",
      "        fit_params : dict, default=None\n",
      "            Parameters to pass to the fit method of the estimator.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The true score without permuting targets.\n",
      "        \n",
      "        permutation_scores : array of shape (n_permutations,)\n",
      "            The scores obtained for each permutations.\n",
      "        \n",
      "        pvalue : float\n",
      "            The p-value, which approximates the probability that the score would\n",
      "            be obtained by chance. This is calculated as:\n",
      "        \n",
      "            `(C + 1) / (n_permutations + 1)`\n",
      "        \n",
      "            Where C is the number of permutations whose score >= the true score.\n",
      "        \n",
      "            The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function implements Test 1 in:\n",
      "        \n",
      "            Ojala and Garriga. `Permutation Tests for Studying Classifier\n",
      "            Performance\n",
      "            <http://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf>`_. The\n",
      "            Journal of Machine Learning Research (2010) vol. 11\n",
      "    \n",
      "    train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
      "        Split arrays or matrices into random train and test subsets.\n",
      "        \n",
      "        Quick utility that wraps input validation and\n",
      "        ``next(ShuffleSplit().split(X, y))`` and application to input data\n",
      "        into a single call for splitting (and optionally subsampling) data in a\n",
      "        oneliner.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cross_validation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        *arrays : sequence of indexables with same length / shape[0]\n",
      "            Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "            matrices or pandas dataframes.\n",
      "        \n",
      "        test_size : float or int, default=None\n",
      "            If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "            of the dataset to include in the test split. If int, represents the\n",
      "            absolute number of test samples. If None, the value is set to the\n",
      "            complement of the train size. If ``train_size`` is also None, it will\n",
      "            be set to 0.25.\n",
      "        \n",
      "        train_size : float or int, default=None\n",
      "            If float, should be between 0.0 and 1.0 and represent the\n",
      "            proportion of the dataset to include in the train split. If\n",
      "            int, represents the absolute number of train samples. If None,\n",
      "            the value is automatically set to the complement of the test size.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Controls the shuffling applied to the data before applying the split.\n",
      "            Pass an int for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        shuffle : bool, default=True\n",
      "            Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "            then stratify must be None.\n",
      "        \n",
      "        stratify : array-like, default=None\n",
      "            If not None, data is split in a stratified fashion, using this as\n",
      "            the class labels.\n",
      "            Read more in the :ref:`User Guide <stratification>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        splitting : list, length=2 * len(arrays)\n",
      "            List containing train-test split of inputs.\n",
      "        \n",
      "            .. versionadded:: 0.16\n",
      "                If the input is sparse, the output will be a\n",
      "                ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "                input type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.model_selection import train_test_split\n",
      "        >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "        >>> X\n",
      "        array([[0, 1],\n",
      "               [2, 3],\n",
      "               [4, 5],\n",
      "               [6, 7],\n",
      "               [8, 9]])\n",
      "        >>> list(y)\n",
      "        [0, 1, 2, 3, 4]\n",
      "        \n",
      "        >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "        ...     X, y, test_size=0.33, random_state=42)\n",
      "        ...\n",
      "        >>> X_train\n",
      "        array([[4, 5],\n",
      "               [0, 1],\n",
      "               [6, 7]])\n",
      "        >>> y_train\n",
      "        [2, 0, 3]\n",
      "        >>> X_test\n",
      "        array([[2, 3],\n",
      "               [8, 9]])\n",
      "        >>> y_test\n",
      "        [1, 4]\n",
      "        \n",
      "        >>> train_test_split(y, shuffle=False)\n",
      "        [[0, 1, 2], [3, 4]]\n",
      "    \n",
      "    validation_curve(estimator, X, y, *, param_name, param_range, groups=None, cv=None, scoring=None, n_jobs=None, pre_dispatch='all', verbose=0, error_score=nan, fit_params=None)\n",
      "        Validation curve.\n",
      "        \n",
      "        Determine training and test scores for varying parameter values.\n",
      "        \n",
      "        Compute scores for an estimator with different values of a specified\n",
      "        parameter. This is similar to grid search with one parameter. However, this\n",
      "        will also compute training scores and is merely a utility for plotting the\n",
      "        results.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <validation_curve>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : object type that implements the \"fit\" and \"predict\" methods\n",
      "            An object of that type which is cloned for each validation.\n",
      "        \n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            Training vector, where `n_samples` is the number of samples and\n",
      "            `n_features` is the number of features.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n",
      "            Target relative to X for classification or regression;\n",
      "            None for unsupervised learning.\n",
      "        \n",
      "        param_name : str\n",
      "            Name of the parameter that will be varied.\n",
      "        \n",
      "        param_range : array-like of shape (n_values,)\n",
      "            The values of the parameter that will be evaluated.\n",
      "        \n",
      "        groups : array-like of shape (n_samples,), default=None\n",
      "            Group labels for the samples used while splitting the dataset into\n",
      "            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "            instance (e.g., :class:`GroupKFold`).\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable yielding (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For int/None inputs, if the estimator is a classifier and ``y`` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "        \n",
      "        scoring : str or callable, default=None\n",
      "            A str (see model evaluation documentation) or\n",
      "            a scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and computing\n",
      "            the score are parallelized over the combinations of each parameter\n",
      "            value and each cross-validation split.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        pre_dispatch : int or str, default='all'\n",
      "            Number of predispatched jobs for parallel execution (default is\n",
      "            all). The option can reduce the allocated memory. The str can\n",
      "            be an expression like '2*n_jobs'.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Controls the verbosity: the higher, the more messages.\n",
      "        \n",
      "        fit_params : dict, default=None\n",
      "            Parameters to pass to the fit method of the estimator.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        error_score : 'raise' or numeric, default=np.nan\n",
      "            Value to assign to the score if an error occurs in estimator fitting.\n",
      "            If set to 'raise', the error is raised.\n",
      "            If a numeric value is given, FitFailedWarning is raised.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        train_scores : array of shape (n_ticks, n_cv_folds)\n",
      "            Scores on training sets.\n",
      "        \n",
      "        test_scores : array of shape (n_ticks, n_cv_folds)\n",
      "            Scores on test set.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        See :ref:`sphx_glr_auto_examples_model_selection_plot_validation_curve.py`\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BaseCrossValidator', 'BaseShuffleSplit', 'GridSearchCV', '...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.model_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d35cf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module sklearn.naive_bayes in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.naive_bayes\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.naive_bayes` module implements Naive Bayes algorithms. These\n",
      "    are supervised learning methods based on applying Bayes' theorem with strong\n",
      "    (naive) feature independence assumptions.\n",
      "\n",
      "CLASSES\n",
      "    _BaseDiscreteNB(_BaseNB)\n",
      "        BernoulliNB\n",
      "        CategoricalNB\n",
      "        ComplementNB\n",
      "        MultinomialNB\n",
      "    _BaseNB(sklearn.base.ClassifierMixin, sklearn.base.BaseEstimator)\n",
      "        GaussianNB\n",
      "    \n",
      "    class BernoulliNB(_BaseDiscreteNB)\n",
      "     |  BernoulliNB(*, alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)\n",
      "     |  \n",
      "     |  Naive Bayes classifier for multivariate Bernoulli models.\n",
      "     |  \n",
      "     |  Like MultinomialNB, this classifier is suitable for discrete data. The\n",
      "     |  difference is that while MultinomialNB works with occurrence counts,\n",
      "     |  BernoulliNB is designed for binary/boolean features.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bernoulli_naive_bayes>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Additive (Laplace/Lidstone) smoothing parameter\n",
      "     |      (0 for no smoothing).\n",
      "     |  \n",
      "     |  binarize : float or None, default=0.0\n",
      "     |      Threshold for binarizing (mapping to booleans) of sample features.\n",
      "     |      If None, input is presumed to already consist of binary vectors.\n",
      "     |  \n",
      "     |  fit_prior : bool, default=True\n",
      "     |      Whether to learn class prior probabilities or not.\n",
      "     |      If false, a uniform prior will be used.\n",
      "     |  \n",
      "     |  class_prior : array-like of shape (n_classes,), default=None\n",
      "     |      Prior probabilities of the classes. If specified the priors are not\n",
      "     |      adjusted according to the data.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  class_count_ : ndarray of shape (n_classes,)\n",
      "     |      Number of samples encountered for each class during fitting. This\n",
      "     |      value is weighted by the sample weight when provided.\n",
      "     |  \n",
      "     |  class_log_prior_ : ndarray of shape (n_classes,)\n",
      "     |      Log probability of each class (smoothed).\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      Class labels known to the classifier\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Mirrors ``feature_log_prob_`` for interpreting `BernoulliNB`\n",
      "     |      as a linear model.\n",
      "     |  \n",
      "     |  feature_count_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Number of samples encountered for each (class, feature)\n",
      "     |      during fitting. This value is weighted by the sample weight when\n",
      "     |      provided.\n",
      "     |  \n",
      "     |  feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Empirical log probability of features given a class, P(x_i|y).\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (n_classes,)\n",
      "     |      Mirrors ``class_log_prior_`` for interpreting `BernoulliNB`\n",
      "     |      as a linear model.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      Number of features of each sample.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          Attribute `n_features_` was deprecated in version 1.0 and will be\n",
      "     |          removed in 1.2. Use `n_features_in_` instead.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  CategoricalNB : Naive Bayes classifier for categorical features.\n",
      "     |  ComplementNB : The Complement Naive Bayes classifier\n",
      "     |      described in Rennie et al. (2003).\n",
      "     |  GaussianNB : Gaussian Naive Bayes (GaussianNB).\n",
      "     |  MultinomialNB : Naive Bayes classifier for multinomial models.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
      "     |  Information Retrieval. Cambridge University Press, pp. 234-265.\n",
      "     |  https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html\n",
      "     |  \n",
      "     |  A. McCallum and K. Nigam (1998). A comparison of event models for naive\n",
      "     |  Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for\n",
      "     |  Text Categorization, pp. 41-48.\n",
      "     |  \n",
      "     |  V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with\n",
      "     |  naive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> rng = np.random.RandomState(1)\n",
      "     |  >>> X = rng.randint(5, size=(6, 100))\n",
      "     |  >>> Y = np.array([1, 2, 3, 4, 4, 5])\n",
      "     |  >>> from sklearn.naive_bayes import BernoulliNB\n",
      "     |  >>> clf = BernoulliNB()\n",
      "     |  >>> clf.fit(X, Y)\n",
      "     |  BernoulliNB()\n",
      "     |  >>> print(clf.predict(X[2:3]))\n",
      "     |  [3]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BernoulliNB\n",
      "     |      _BaseDiscreteNB\n",
      "     |      _BaseNB\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseDiscreteNB:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Naive Bayes classifier according to X, y.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Incremental fit on a batch of samples.\n",
      "     |      \n",
      "     |      This method is expected to be called several times consecutively\n",
      "     |      on different chunks of a dataset so as to implement out-of-core\n",
      "     |      or online learning.\n",
      "     |      \n",
      "     |      This is especially useful when the whole dataset is too big to fit in\n",
      "     |      memory at once.\n",
      "     |      \n",
      "     |      This method has some performance overhead hence it is better to call\n",
      "     |      partial_fit on chunks of data that are as large as possible\n",
      "     |      (as long as fitting in the memory budget) to hide the overhead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      classes : array-like of shape (n_classes,), default=None\n",
      "     |          List of all the classes that can possibly appear in the y vector.\n",
      "     |      \n",
      "     |          Must be provided at the first call to partial_fit, can be omitted\n",
      "     |          in subsequent calls.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _BaseDiscreteNB:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      DEPRECATED: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      DEPRECATED: Attribute `intercept_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "     |  \n",
      "     |  n_features_\n",
      "     |      DEPRECATED: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseNB:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on an array of test vectors X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : ndarray of shape (n_samples,)\n",
      "     |          Predicted target values for X.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Return log-probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Return probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class CategoricalNB(_BaseDiscreteNB)\n",
      "     |  CategoricalNB(*, alpha=1.0, fit_prior=True, class_prior=None, min_categories=None)\n",
      "     |  \n",
      "     |  Naive Bayes classifier for categorical features.\n",
      "     |  \n",
      "     |  The categorical Naive Bayes classifier is suitable for classification with\n",
      "     |  discrete features that are categorically distributed. The categories of\n",
      "     |  each feature are drawn from a categorical distribution.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <categorical_naive_bayes>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Additive (Laplace/Lidstone) smoothing parameter\n",
      "     |      (0 for no smoothing).\n",
      "     |  \n",
      "     |  fit_prior : bool, default=True\n",
      "     |      Whether to learn class prior probabilities or not.\n",
      "     |      If false, a uniform prior will be used.\n",
      "     |  \n",
      "     |  class_prior : array-like of shape (n_classes,), default=None\n",
      "     |      Prior probabilities of the classes. If specified the priors are not\n",
      "     |      adjusted according to the data.\n",
      "     |  \n",
      "     |  min_categories : int or array-like of shape (n_features,), default=None\n",
      "     |      Minimum number of categories per feature.\n",
      "     |  \n",
      "     |      - integer: Sets the minimum number of categories per feature to\n",
      "     |        `n_categories` for each features.\n",
      "     |      - array-like: shape (n_features,) where `n_categories[i]` holds the\n",
      "     |        minimum number of categories for the ith column of the input.\n",
      "     |      - None (default): Determines the number of categories automatically\n",
      "     |        from the training data.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  category_count_ : list of arrays of shape (n_features,)\n",
      "     |      Holds arrays of shape (n_classes, n_categories of respective feature)\n",
      "     |      for each feature. Each array provides the number of samples\n",
      "     |      encountered for each class and category of the specific feature.\n",
      "     |  \n",
      "     |  class_count_ : ndarray of shape (n_classes,)\n",
      "     |      Number of samples encountered for each class during fitting. This\n",
      "     |      value is weighted by the sample weight when provided.\n",
      "     |  \n",
      "     |  class_log_prior_ : ndarray of shape (n_classes,)\n",
      "     |      Smoothed empirical log probability for each class.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      Class labels known to the classifier\n",
      "     |  \n",
      "     |  feature_log_prob_ : list of arrays of shape (n_features,)\n",
      "     |      Holds arrays of shape (n_classes, n_categories of respective feature)\n",
      "     |      for each feature. Each array provides the empirical log probability\n",
      "     |      of categories given the respective feature and class, ``P(x_i|y)``.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      Number of features of each sample.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          Attribute `n_features_` was deprecated in version 1.0 and will be\n",
      "     |          removed in 1.2. Use `n_features_in_` instead.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_categories_ : ndarray of shape (n_features,), dtype=np.int64\n",
      "     |      Number of categories for each feature. This value is\n",
      "     |      inferred from the data or set by the minimum number of categories.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
      "     |  ComplementNB : Complement Naive Bayes classifier.\n",
      "     |  GaussianNB : Gaussian Naive Bayes.\n",
      "     |  MultinomialNB : Naive Bayes classifier for multinomial models.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> rng = np.random.RandomState(1)\n",
      "     |  >>> X = rng.randint(5, size=(6, 100))\n",
      "     |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
      "     |  >>> from sklearn.naive_bayes import CategoricalNB\n",
      "     |  >>> clf = CategoricalNB()\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  CategoricalNB()\n",
      "     |  >>> print(clf.predict(X[2:3]))\n",
      "     |  [3]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CategoricalNB\n",
      "     |      _BaseDiscreteNB\n",
      "     |      _BaseNB\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, alpha=1.0, fit_prior=True, class_prior=None, min_categories=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Naive Bayes classifier according to X, y.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features. Here, each feature of X is\n",
      "     |          assumed to be from a different categorical distribution.\n",
      "     |          It is further assumed that all categories of each feature are\n",
      "     |          represented by the numbers 0, ..., n - 1, where n refers to the\n",
      "     |          total number of categories for the given feature. This can, for\n",
      "     |          instance, be achieved with the help of OrdinalEncoder.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Incremental fit on a batch of samples.\n",
      "     |      \n",
      "     |      This method is expected to be called several times consecutively\n",
      "     |      on different chunks of a dataset so as to implement out-of-core\n",
      "     |      or online learning.\n",
      "     |      \n",
      "     |      This is especially useful when the whole dataset is too big to fit in\n",
      "     |      memory at once.\n",
      "     |      \n",
      "     |      This method has some performance overhead hence it is better to call\n",
      "     |      partial_fit on chunks of data that are as large as possible\n",
      "     |      (as long as fitting in the memory budget) to hide the overhead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features. Here, each feature of X is\n",
      "     |          assumed to be from a different categorical distribution.\n",
      "     |          It is further assumed that all categories of each feature are\n",
      "     |          represented by the numbers 0, ..., n - 1, where n refers to the\n",
      "     |          total number of categories for the given feature. This can, for\n",
      "     |          instance, be achieved with the help of OrdinalEncoder.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      classes : array-like of shape (n_classes,), default=None\n",
      "     |          List of all the classes that can possibly appear in the y vector.\n",
      "     |      \n",
      "     |          Must be provided at the first call to partial_fit, can be omitted\n",
      "     |          in subsequent calls.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _BaseDiscreteNB:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      DEPRECATED: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      DEPRECATED: Attribute `intercept_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "     |  \n",
      "     |  n_features_\n",
      "     |      DEPRECATED: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseNB:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on an array of test vectors X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : ndarray of shape (n_samples,)\n",
      "     |          Predicted target values for X.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Return log-probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Return probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class ComplementNB(_BaseDiscreteNB)\n",
      "     |  ComplementNB(*, alpha=1.0, fit_prior=True, class_prior=None, norm=False)\n",
      "     |  \n",
      "     |  The Complement Naive Bayes classifier described in Rennie et al. (2003).\n",
      "     |  \n",
      "     |  The Complement Naive Bayes classifier was designed to correct the \"severe\n",
      "     |  assumptions\" made by the standard Multinomial Naive Bayes classifier. It is\n",
      "     |  particularly suited for imbalanced data sets.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <complement_naive_bayes>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
      "     |  \n",
      "     |  fit_prior : bool, default=True\n",
      "     |      Only used in edge case with a single class in the training set.\n",
      "     |  \n",
      "     |  class_prior : array-like of shape (n_classes,), default=None\n",
      "     |      Prior probabilities of the classes. Not used.\n",
      "     |  \n",
      "     |  norm : bool, default=False\n",
      "     |      Whether or not a second normalization of the weights is performed. The\n",
      "     |      default behavior mirrors the implementations found in Mahout and Weka,\n",
      "     |      which do not follow the full algorithm described in Table 9 of the\n",
      "     |      paper.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  class_count_ : ndarray of shape (n_classes,)\n",
      "     |      Number of samples encountered for each class during fitting. This\n",
      "     |      value is weighted by the sample weight when provided.\n",
      "     |  \n",
      "     |  class_log_prior_ : ndarray of shape (n_classes,)\n",
      "     |      Smoothed empirical log probability for each class. Only used in edge\n",
      "     |      case with a single class in the training set.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      Class labels known to the classifier\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Mirrors ``feature_log_prob_`` for interpreting `ComplementNB`\n",
      "     |      as a linear model.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.24\n",
      "     |          ``coef_`` is deprecated in 0.24 and will be removed in 1.1\n",
      "     |          (renaming of 0.26).\n",
      "     |  \n",
      "     |  feature_all_ : ndarray of shape (n_features,)\n",
      "     |      Number of samples encountered for each feature during fitting. This\n",
      "     |      value is weighted by the sample weight when provided.\n",
      "     |  \n",
      "     |  feature_count_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Number of samples encountered for each (class, feature) during fitting.\n",
      "     |      This value is weighted by the sample weight when provided.\n",
      "     |  \n",
      "     |  feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Empirical weights for class complements.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (n_classes,)\n",
      "     |      Mirrors ``class_log_prior_`` for interpreting `ComplementNB`\n",
      "     |      as a linear model.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.24\n",
      "     |          ``coef_`` is deprecated in 0.24 and will be removed in 1.1\n",
      "     |          (renaming of 0.26).\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      Number of features of each sample.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          Attribute `n_features_` was deprecated in version 1.0 and will be\n",
      "     |          removed in 1.2. Use `n_features_in_` instead.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
      "     |  CategoricalNB : Naive Bayes classifier for categorical features.\n",
      "     |  GaussianNB : Gaussian Naive Bayes.\n",
      "     |  MultinomialNB : Naive Bayes classifier for multinomial models.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\n",
      "     |  Tackling the poor assumptions of naive bayes text classifiers. In ICML\n",
      "     |  (Vol. 3, pp. 616-623).\n",
      "     |  https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> rng = np.random.RandomState(1)\n",
      "     |  >>> X = rng.randint(5, size=(6, 100))\n",
      "     |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
      "     |  >>> from sklearn.naive_bayes import ComplementNB\n",
      "     |  >>> clf = ComplementNB()\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  ComplementNB()\n",
      "     |  >>> print(clf.predict(X[2:3]))\n",
      "     |  [3]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ComplementNB\n",
      "     |      _BaseDiscreteNB\n",
      "     |      _BaseNB\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, alpha=1.0, fit_prior=True, class_prior=None, norm=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseDiscreteNB:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Naive Bayes classifier according to X, y.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Incremental fit on a batch of samples.\n",
      "     |      \n",
      "     |      This method is expected to be called several times consecutively\n",
      "     |      on different chunks of a dataset so as to implement out-of-core\n",
      "     |      or online learning.\n",
      "     |      \n",
      "     |      This is especially useful when the whole dataset is too big to fit in\n",
      "     |      memory at once.\n",
      "     |      \n",
      "     |      This method has some performance overhead hence it is better to call\n",
      "     |      partial_fit on chunks of data that are as large as possible\n",
      "     |      (as long as fitting in the memory budget) to hide the overhead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      classes : array-like of shape (n_classes,), default=None\n",
      "     |          List of all the classes that can possibly appear in the y vector.\n",
      "     |      \n",
      "     |          Must be provided at the first call to partial_fit, can be omitted\n",
      "     |          in subsequent calls.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _BaseDiscreteNB:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      DEPRECATED: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      DEPRECATED: Attribute `intercept_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "     |  \n",
      "     |  n_features_\n",
      "     |      DEPRECATED: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseNB:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on an array of test vectors X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : ndarray of shape (n_samples,)\n",
      "     |          Predicted target values for X.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Return log-probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Return probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class GaussianNB(_BaseNB)\n",
      "     |  GaussianNB(*, priors=None, var_smoothing=1e-09)\n",
      "     |  \n",
      "     |  Gaussian Naive Bayes (GaussianNB).\n",
      "     |  \n",
      "     |  Can perform online updates to model parameters via :meth:`partial_fit`.\n",
      "     |  For details on algorithm used to update feature means and variance online,\n",
      "     |  see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
      "     |  \n",
      "     |      http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  priors : array-like of shape (n_classes,)\n",
      "     |      Prior probabilities of the classes. If specified the priors are not\n",
      "     |      adjusted according to the data.\n",
      "     |  \n",
      "     |  var_smoothing : float, default=1e-9\n",
      "     |      Portion of the largest variance of all features that is added to\n",
      "     |      variances for calculation stability.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  class_count_ : ndarray of shape (n_classes,)\n",
      "     |      number of training samples observed in each class.\n",
      "     |  \n",
      "     |  class_prior_ : ndarray of shape (n_classes,)\n",
      "     |      probability of each class.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      class labels known to the classifier.\n",
      "     |  \n",
      "     |  epsilon_ : float\n",
      "     |      absolute additive value to variances.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  sigma_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Variance of each feature per class.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |         `sigma_` is deprecated in 1.0 and will be removed in 1.2.\n",
      "     |         Use `var_` instead.\n",
      "     |  \n",
      "     |  var_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Variance of each feature per class.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  theta_ : ndarray of shape (n_classes, n_features)\n",
      "     |      mean of each feature per class.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
      "     |  CategoricalNB : Naive Bayes classifier for categorical features.\n",
      "     |  ComplementNB : Complement Naive Bayes classifier.\n",
      "     |  MultinomialNB : Naive Bayes classifier for multinomial models.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      "     |  >>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
      "     |  >>> from sklearn.naive_bayes import GaussianNB\n",
      "     |  >>> clf = GaussianNB()\n",
      "     |  >>> clf.fit(X, Y)\n",
      "     |  GaussianNB()\n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  >>> clf_pf = GaussianNB()\n",
      "     |  >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
      "     |  GaussianNB()\n",
      "     |  >>> print(clf_pf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GaussianNB\n",
      "     |      _BaseNB\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, priors=None, var_smoothing=1e-09)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Gaussian Naive Bayes according to X, y.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Incremental fit on a batch of samples.\n",
      "     |      \n",
      "     |      This method is expected to be called several times consecutively\n",
      "     |      on different chunks of a dataset so as to implement out-of-core\n",
      "     |      or online learning.\n",
      "     |      \n",
      "     |      This is especially useful when the whole dataset is too big to fit in\n",
      "     |      memory at once.\n",
      "     |      \n",
      "     |      This method has some performance and numerical stability overhead,\n",
      "     |      hence it is better to call partial_fit on chunks of data that are\n",
      "     |      as large as possible (as long as fitting in the memory budget) to\n",
      "     |      hide the overhead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      classes : array-like of shape (n_classes,), default=None\n",
      "     |          List of all the classes that can possibly appear in the y vector.\n",
      "     |      \n",
      "     |          Must be provided at the first call to partial_fit, can be omitted\n",
      "     |          in subsequent calls.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  sigma_\n",
      "     |      DEPRECATED: Attribute `sigma_` was deprecated in 1.0 and will be removed in1.2. Use `var_` instead.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseNB:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on an array of test vectors X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : ndarray of shape (n_samples,)\n",
      "     |          Predicted target values for X.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Return log-probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Return probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MultinomialNB(_BaseDiscreteNB)\n",
      "     |  MultinomialNB(*, alpha=1.0, fit_prior=True, class_prior=None)\n",
      "     |  \n",
      "     |  Naive Bayes classifier for multinomial models.\n",
      "     |  \n",
      "     |  The multinomial Naive Bayes classifier is suitable for classification with\n",
      "     |  discrete features (e.g., word counts for text classification). The\n",
      "     |  multinomial distribution normally requires integer feature counts. However,\n",
      "     |  in practice, fractional counts such as tf-idf may also work.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Additive (Laplace/Lidstone) smoothing parameter\n",
      "     |      (0 for no smoothing).\n",
      "     |  \n",
      "     |  fit_prior : bool, default=True\n",
      "     |      Whether to learn class prior probabilities or not.\n",
      "     |      If false, a uniform prior will be used.\n",
      "     |  \n",
      "     |  class_prior : array-like of shape (n_classes,), default=None\n",
      "     |      Prior probabilities of the classes. If specified the priors are not\n",
      "     |      adjusted according to the data.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  class_count_ : ndarray of shape (n_classes,)\n",
      "     |      Number of samples encountered for each class during fitting. This\n",
      "     |      value is weighted by the sample weight when provided.\n",
      "     |  \n",
      "     |  class_log_prior_ : ndarray of shape (n_classes,)\n",
      "     |      Smoothed empirical log probability for each class.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      Class labels known to the classifier\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Mirrors ``feature_log_prob_`` for interpreting `MultinomialNB`\n",
      "     |      as a linear model.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.24\n",
      "     |          ``coef_`` is deprecated in 0.24 and will be removed in 1.1\n",
      "     |          (renaming of 0.26).\n",
      "     |  \n",
      "     |  feature_count_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Number of samples encountered for each (class, feature)\n",
      "     |      during fitting. This value is weighted by the sample weight when\n",
      "     |      provided.\n",
      "     |  \n",
      "     |  feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Empirical log probability of features\n",
      "     |      given a class, ``P(x_i|y)``.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (n_classes,)\n",
      "     |      Mirrors ``class_log_prior_`` for interpreting `MultinomialNB`\n",
      "     |      as a linear model.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.24\n",
      "     |          ``intercept_`` is deprecated in 0.24 and will be removed in 1.1\n",
      "     |          (renaming of 0.26).\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      Number of features of each sample.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          Attribute `n_features_` was deprecated in version 1.0 and will be\n",
      "     |          removed in 1.2. Use `n_features_in_` instead.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
      "     |  CategoricalNB : Naive Bayes classifier for categorical features.\n",
      "     |  ComplementNB : Complement Naive Bayes classifier.\n",
      "     |  GaussianNB : Gaussian Naive Bayes.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For the rationale behind the names `coef_` and `intercept_`, i.e.\n",
      "     |  naive Bayes as a linear classifier, see J. Rennie et al. (2003),\n",
      "     |  Tackling the poor assumptions of naive Bayes text classifiers, ICML.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
      "     |  Information Retrieval. Cambridge University Press, pp. 234-265.\n",
      "     |  https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> rng = np.random.RandomState(1)\n",
      "     |  >>> X = rng.randint(5, size=(6, 100))\n",
      "     |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
      "     |  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "     |  >>> clf = MultinomialNB()\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  MultinomialNB()\n",
      "     |  >>> print(clf.predict(X[2:3]))\n",
      "     |  [3]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultinomialNB\n",
      "     |      _BaseDiscreteNB\n",
      "     |      _BaseNB\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, alpha=1.0, fit_prior=True, class_prior=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseDiscreteNB:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Naive Bayes classifier according to X, y.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Incremental fit on a batch of samples.\n",
      "     |      \n",
      "     |      This method is expected to be called several times consecutively\n",
      "     |      on different chunks of a dataset so as to implement out-of-core\n",
      "     |      or online learning.\n",
      "     |      \n",
      "     |      This is especially useful when the whole dataset is too big to fit in\n",
      "     |      memory at once.\n",
      "     |      \n",
      "     |      This method has some performance overhead hence it is better to call\n",
      "     |      partial_fit on chunks of data that are as large as possible\n",
      "     |      (as long as fitting in the memory budget) to hide the overhead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      classes : array-like of shape (n_classes,), default=None\n",
      "     |          List of all the classes that can possibly appear in the y vector.\n",
      "     |      \n",
      "     |          Must be provided at the first call to partial_fit, can be omitted\n",
      "     |          in subsequent calls.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _BaseDiscreteNB:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      DEPRECATED: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      DEPRECATED: Attribute `intercept_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "     |  \n",
      "     |  n_features_\n",
      "     |      DEPRECATED: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseNB:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on an array of test vectors X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : ndarray of shape (n_samples,)\n",
      "     |          Predicted target values for X.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Return log-probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Return probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BernoulliNB', 'GaussianNB', 'MultinomialNB', 'ComplementNB...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.naive_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1b12ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.neighbors in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.neighbors\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.neighbors` module implements the k-nearest neighbors\n",
      "    algorithm.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _ball_tree\n",
      "    _base\n",
      "    _classification\n",
      "    _distance_metric\n",
      "    _graph\n",
      "    _kd_tree\n",
      "    _kde\n",
      "    _lof\n",
      "    _nca\n",
      "    _nearest_centroid\n",
      "    _partition_nodes\n",
      "    _quad_tree\n",
      "    _regression\n",
      "    _unsupervised\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        sklearn.neighbors._kde.KernelDensity\n",
      "        sklearn.neighbors._nca.NeighborhoodComponentsAnalysis(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.neighbors._nearest_centroid.NearestCentroid(sklearn.base.ClassifierMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.ClassifierMixin(builtins.object)\n",
      "        sklearn.neighbors._classification.KNeighborsClassifier(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._classification.RadiusNeighborsClassifier(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._nearest_centroid.NearestCentroid(sklearn.base.ClassifierMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.OutlierMixin(builtins.object)\n",
      "        sklearn.neighbors._lof.LocalOutlierFactor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.OutlierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "    sklearn.base.RegressorMixin(builtins.object)\n",
      "        sklearn.neighbors._regression.KNeighborsRegressor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._regression.RadiusNeighborsRegressor(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "    sklearn.base.TransformerMixin(builtins.object)\n",
      "        sklearn.neighbors._graph.KNeighborsTransformer(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.TransformerMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._graph.RadiusNeighborsTransformer(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.TransformerMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._nca.NeighborhoodComponentsAnalysis(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.metrics._dist_metrics.DistanceMetric(builtins.object)\n",
      "        sklearn.neighbors._distance_metric.DistanceMetric\n",
      "    sklearn.neighbors._ball_tree.BinaryTree(builtins.object)\n",
      "        sklearn.neighbors._ball_tree.BallTree\n",
      "    sklearn.neighbors._base.KNeighborsMixin(builtins.object)\n",
      "        sklearn.neighbors._classification.KNeighborsClassifier(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._graph.KNeighborsTransformer(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.TransformerMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._lof.LocalOutlierFactor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.OutlierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._regression.KNeighborsRegressor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._unsupervised.NearestNeighbors(sklearn.neighbors._base.KNeighborsMixin, sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "    sklearn.neighbors._base.NeighborsBase(sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.neighbors._classification.KNeighborsClassifier(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._classification.RadiusNeighborsClassifier(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._graph.KNeighborsTransformer(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.TransformerMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._graph.RadiusNeighborsTransformer(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.TransformerMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._lof.LocalOutlierFactor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.OutlierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._regression.KNeighborsRegressor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._regression.RadiusNeighborsRegressor(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._unsupervised.NearestNeighbors(sklearn.neighbors._base.KNeighborsMixin, sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "    sklearn.neighbors._base.RadiusNeighborsMixin(builtins.object)\n",
      "        sklearn.neighbors._classification.RadiusNeighborsClassifier(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._graph.RadiusNeighborsTransformer(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.TransformerMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._regression.RadiusNeighborsRegressor(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._unsupervised.NearestNeighbors(sklearn.neighbors._base.KNeighborsMixin, sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "    sklearn.neighbors._kd_tree.BinaryTree(builtins.object)\n",
      "        sklearn.neighbors._kd_tree.KDTree\n",
      "    \n",
      "    class BallTree(BinaryTree)\n",
      "     |  BallTree(X, leaf_size=40, metric='minkowski', **kwargs)\n",
      "     |  \n",
      "     |  BallTree for fast generalized N-point problems\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <unsupervised_neighbors>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  X : array-like of shape (n_samples, n_features)\n",
      "     |      n_samples is the number of points in the data set, and\n",
      "     |      n_features is the dimension of the parameter space.\n",
      "     |      Note: if X is a C-contiguous array of doubles then data will\n",
      "     |      not be copied. Otherwise, an internal copy will be made.\n",
      "     |  \n",
      "     |  leaf_size : positive int, default=40\n",
      "     |      Number of points at which to switch to brute-force. Changing\n",
      "     |      leaf_size will not affect the results of a query, but can\n",
      "     |      significantly impact the speed of a query and the memory required\n",
      "     |      to store the constructed tree.  The amount of memory needed to\n",
      "     |      store the tree scales as approximately n_samples / leaf_size.\n",
      "     |      For a specified ``leaf_size``, a leaf node is guaranteed to\n",
      "     |      satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in\n",
      "     |      the case that ``n_samples < leaf_size``.\n",
      "     |  \n",
      "     |  metric : str or DistanceMetric object\n",
      "     |      the distance metric to use for the tree.  Default='minkowski'\n",
      "     |      with p=2 (that is, a euclidean metric). See the documentation\n",
      "     |      of the DistanceMetric class for a list of available metrics.\n",
      "     |      ball_tree.valid_metrics gives a list of the metrics which\n",
      "     |      are valid for BallTree.\n",
      "     |  \n",
      "     |  Additional keywords are passed to the distance metric class.\n",
      "     |  Note: Callable functions in the metric parameter are NOT supported for KDTree\n",
      "     |  and Ball Tree. Function call overhead will result in very poor performance.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  data : memory view\n",
      "     |      The training data\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Query for k-nearest neighbors\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> from sklearn.neighbors import BallTree\n",
      "     |      >>> rng = np.random.RandomState(0)\n",
      "     |      >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n",
      "     |      >>> tree = BallTree(X, leaf_size=2)              # doctest: +SKIP\n",
      "     |      >>> dist, ind = tree.query(X[:1], k=3)                # doctest: +SKIP\n",
      "     |      >>> print(ind)  # indices of 3 closest neighbors\n",
      "     |      [0 3 1]\n",
      "     |      >>> print(dist)  # distances to 3 closest neighbors\n",
      "     |      [ 0.          0.19662693  0.29473397]\n",
      "     |  \n",
      "     |  Pickle and Unpickle a tree.  Note that the state of the tree is saved in the\n",
      "     |  pickle operation: the tree needs not be rebuilt upon unpickling.\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> import pickle\n",
      "     |      >>> rng = np.random.RandomState(0)\n",
      "     |      >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n",
      "     |      >>> tree = BallTree(X, leaf_size=2)        # doctest: +SKIP\n",
      "     |      >>> s = pickle.dumps(tree)                     # doctest: +SKIP\n",
      "     |      >>> tree_copy = pickle.loads(s)                # doctest: +SKIP\n",
      "     |      >>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP\n",
      "     |      >>> print(ind)  # indices of 3 closest neighbors\n",
      "     |      [0 3 1]\n",
      "     |      >>> print(dist)  # distances to 3 closest neighbors\n",
      "     |      [ 0.          0.19662693  0.29473397]\n",
      "     |  \n",
      "     |  Query for neighbors within a given radius\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> rng = np.random.RandomState(0)\n",
      "     |      >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n",
      "     |      >>> tree = BallTree(X, leaf_size=2)     # doctest: +SKIP\n",
      "     |      >>> print(tree.query_radius(X[:1], r=0.3, count_only=True))\n",
      "     |      3\n",
      "     |      >>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP\n",
      "     |      >>> print(ind)  # indices of neighbors within distance 0.3\n",
      "     |      [3 0 1]\n",
      "     |  \n",
      "     |  \n",
      "     |  Compute a gaussian kernel density estimate:\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> rng = np.random.RandomState(42)\n",
      "     |      >>> X = rng.random_sample((100, 3))\n",
      "     |      >>> tree = BallTree(X)                # doctest: +SKIP\n",
      "     |      >>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')\n",
      "     |      array([ 6.94114649,  7.83281226,  7.2071716 ])\n",
      "     |  \n",
      "     |  Compute a two-point auto-correlation function\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> rng = np.random.RandomState(0)\n",
      "     |      >>> X = rng.random_sample((30, 3))\n",
      "     |      >>> r = np.linspace(0, 1, 5)\n",
      "     |      >>> tree = BallTree(X)                # doctest: +SKIP\n",
      "     |      >>> tree.two_point_correlation(X, r)\n",
      "     |      array([ 30,  62, 278, 580, 820])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BallTree\n",
      "     |      BinaryTree\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BinaryTree:\n",
      "     |  \n",
      "     |  __getstate__(...)\n",
      "     |      get state for pickling\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      reduce method used for pickling\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |      set state for pickling\n",
      "     |  \n",
      "     |  get_arrays(...)\n",
      "     |      get_arrays(self)\n",
      "     |      \n",
      "     |      Get data and node arrays.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      arrays: tuple of array\n",
      "     |          Arrays for storing tree data, index, node data and node bounds.\n",
      "     |  \n",
      "     |  get_n_calls(...)\n",
      "     |      get_n_calls(self)\n",
      "     |      \n",
      "     |      Get number of calls.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_calls: int\n",
      "     |          number of distance computation calls\n",
      "     |  \n",
      "     |  get_tree_stats(...)\n",
      "     |      get_tree_stats(self)\n",
      "     |      \n",
      "     |      Get tree status.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      tree_stats: tuple of int\n",
      "     |          (number of trims, number of leaves, number of splits)\n",
      "     |  \n",
      "     |  kernel_density(...)\n",
      "     |      kernel_density(self, X, h, kernel='gaussian', atol=0, rtol=1E-8,\n",
      "     |                     breadth_first=True, return_log=False)\n",
      "     |      \n",
      "     |      Compute the kernel density estimate at points X with the given kernel,\n",
      "     |      using the distance metric specified at tree creation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query.  Last dimension should match dimension\n",
      "     |          of training data.\n",
      "     |      h : float\n",
      "     |          the bandwidth of the kernel\n",
      "     |      kernel : str, default=\"gaussian\"\n",
      "     |          specify the kernel to use.  Options are\n",
      "     |          - 'gaussian'\n",
      "     |          - 'tophat'\n",
      "     |          - 'epanechnikov'\n",
      "     |          - 'exponential'\n",
      "     |          - 'linear'\n",
      "     |          - 'cosine'\n",
      "     |          Default is kernel = 'gaussian'\n",
      "     |      atol : float, default=0\n",
      "     |          Specify the desired absolute tolerance of the result.\n",
      "     |          If the true result is `K_true`, then the returned result `K_ret`\n",
      "     |          satisfies ``abs(K_true - K_ret) < atol + rtol * K_ret``\n",
      "     |          The default is zero (i.e. machine precision).\n",
      "     |      rtol : float, default=1e-8\n",
      "     |          Specify the desired relative tolerance of the result.\n",
      "     |          If the true result is `K_true`, then the returned result `K_ret`\n",
      "     |          satisfies ``abs(K_true - K_ret) < atol + rtol * K_ret``\n",
      "     |          The default is `1e-8` (i.e. machine precision).\n",
      "     |      breadth_first : bool, default=False\n",
      "     |          If True, use a breadth-first search.  If False (default) use a\n",
      "     |          depth-first search.  Breadth-first is generally faster for\n",
      "     |          compact kernels and/or high tolerances.\n",
      "     |      return_log : bool, default=False\n",
      "     |          Return the logarithm of the result.  This can be more accurate\n",
      "     |          than returning the result itself for narrow kernels.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      density : ndarray of shape X.shape[:-1]\n",
      "     |          The array of (log)-density evaluations\n",
      "     |  \n",
      "     |  query(...)\n",
      "     |      query(X, k=1, return_distance=True,\n",
      "     |            dualtree=False, breadth_first=False)\n",
      "     |      \n",
      "     |      query the tree for the k nearest neighbors\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query\n",
      "     |      k : int, default=1\n",
      "     |          The number of nearest neighbors to return\n",
      "     |      return_distance : bool, default=True\n",
      "     |          if True, return a tuple (d, i) of distances and indices\n",
      "     |          if False, return array i\n",
      "     |      dualtree : bool, default=False\n",
      "     |          if True, use the dual tree formalism for the query: a tree is\n",
      "     |          built for the query points, and the pair of trees is used to\n",
      "     |          efficiently search this space.  This can lead to better\n",
      "     |          performance as the number of points grows large.\n",
      "     |      breadth_first : bool, default=False\n",
      "     |          if True, then query the nodes in a breadth-first manner.\n",
      "     |          Otherwise, query the nodes in a depth-first manner.\n",
      "     |      sort_results : bool, default=True\n",
      "     |          if True, then distances and indices of each point are sorted\n",
      "     |          on return, so that the first column contains the closest points.\n",
      "     |          Otherwise, neighbors are returned in an arbitrary order.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      i    : if return_distance == False\n",
      "     |      (d,i) : if return_distance == True\n",
      "     |      \n",
      "     |      d : ndarray of shape X.shape[:-1] + (k,), dtype=double\n",
      "     |          Each entry gives the list of distances to the neighbors of the\n",
      "     |          corresponding point.\n",
      "     |      \n",
      "     |      i : ndarray of shape X.shape[:-1] + (k,), dtype=int\n",
      "     |          Each entry gives the list of indices of neighbors of the\n",
      "     |          corresponding point.\n",
      "     |  \n",
      "     |  query_radius(...)\n",
      "     |      query_radius(X, r, return_distance=False,\n",
      "     |      count_only=False, sort_results=False)\n",
      "     |      \n",
      "     |      query the tree for neighbors within a radius r\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query\n",
      "     |      r : distance within which neighbors are returned\n",
      "     |          r can be a single value, or an array of values of shape\n",
      "     |          x.shape[:-1] if different radii are desired for each point.\n",
      "     |      return_distance : bool, default=False\n",
      "     |          if True,  return distances to neighbors of each point\n",
      "     |          if False, return only neighbors\n",
      "     |          Note that unlike the query() method, setting return_distance=True\n",
      "     |          here adds to the computation time.  Not all distances need to be\n",
      "     |          calculated explicitly for return_distance=False.  Results are\n",
      "     |          not sorted by default: see ``sort_results`` keyword.\n",
      "     |      count_only : bool, default=False\n",
      "     |          if True,  return only the count of points within distance r\n",
      "     |          if False, return the indices of all points within distance r\n",
      "     |          If return_distance==True, setting count_only=True will\n",
      "     |          result in an error.\n",
      "     |      sort_results : bool, default=False\n",
      "     |          if True, the distances and indices will be sorted before being\n",
      "     |          returned.  If False, the results will not be sorted.  If\n",
      "     |          return_distance == False, setting sort_results = True will\n",
      "     |          result in an error.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      count       : if count_only == True\n",
      "     |      ind         : if count_only == False and return_distance == False\n",
      "     |      (ind, dist) : if count_only == False and return_distance == True\n",
      "     |      \n",
      "     |      count : ndarray of shape X.shape[:-1], dtype=int\n",
      "     |          Each entry gives the number of neighbors within a distance r of the\n",
      "     |          corresponding point.\n",
      "     |      \n",
      "     |      ind : ndarray of shape X.shape[:-1], dtype=object\n",
      "     |          Each element is a numpy integer array listing the indices of\n",
      "     |          neighbors of the corresponding point.  Note that unlike\n",
      "     |          the results of a k-neighbors query, the returned neighbors\n",
      "     |          are not sorted by distance by default.\n",
      "     |      \n",
      "     |      dist : ndarray of shape X.shape[:-1], dtype=object\n",
      "     |          Each element is a numpy double array listing the distances\n",
      "     |          corresponding to indices in i.\n",
      "     |  \n",
      "     |  reset_n_calls(...)\n",
      "     |      reset_n_calls(self)\n",
      "     |      \n",
      "     |      Reset number of calls to 0.\n",
      "     |  \n",
      "     |  two_point_correlation(...)\n",
      "     |      two_point_correlation(X, r, dualtree=False)\n",
      "     |      \n",
      "     |      Compute the two-point correlation function\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query.  Last dimension should match dimension\n",
      "     |          of training data.\n",
      "     |      r : array-like\n",
      "     |          A one-dimensional array of distances\n",
      "     |      dualtree : bool, default=False\n",
      "     |          If True, use a dualtree algorithm.  Otherwise, use a single-tree\n",
      "     |          algorithm.  Dual tree algorithms can have better scaling for\n",
      "     |          large N.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      counts : ndarray\n",
      "     |          counts[i] contains the number of pairs of points with distance\n",
      "     |          less than or equal to r[i]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BinaryTree:\n",
      "     |  \n",
      "     |  data\n",
      "     |  \n",
      "     |  idx_array\n",
      "     |  \n",
      "     |  node_bounds\n",
      "     |  \n",
      "     |  node_data\n",
      "     |  \n",
      "     |  sample_weight\n",
      "     |  \n",
      "     |  sum_weight\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BinaryTree:\n",
      "     |  \n",
      "     |  valid_metrics = ['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'ci...\n",
      "    \n",
      "    class DistanceMetric(sklearn.metrics._dist_metrics.DistanceMetric)\n",
      "     |  Method resolution order:\n",
      "     |      DistanceMetric\n",
      "     |      sklearn.metrics._dist_metrics.DistanceMetric\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  get_metric(metric, **kwargs) from builtins.type\n",
      "     |      Get the given distance metric from the string identifier.\n",
      "     |      \n",
      "     |      See the docstring of DistanceMetric for a list of available metrics.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      metric : str or class name\n",
      "     |          The distance metric to use\n",
      "     |      **kwargs\n",
      "     |          additional arguments will be passed to the requested metric\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.metrics._dist_metrics.DistanceMetric:\n",
      "     |  \n",
      "     |  __getstate__(...)\n",
      "     |      get state for pickling\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      reduce method used for pickling\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |      set state for pickling\n",
      "     |  \n",
      "     |  dist_to_rdist(...)\n",
      "     |      Convert the true distance to the rank-preserving surrogate distance.\n",
      "     |      \n",
      "     |      The surrogate distance is any measure that yields the same rank as the\n",
      "     |      distance, but is more efficient to compute. For example, for the\n",
      "     |      Euclidean metric, the surrogate distance is the squared-euclidean distance.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dist : double\n",
      "     |          True distance.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          Surrogate distance.\n",
      "     |  \n",
      "     |  pairwise(...)\n",
      "     |      Compute the pairwise distances between X and Y\n",
      "     |      \n",
      "     |      This is a convenience routine for the sake of testing.  For many\n",
      "     |      metrics, the utilities in scipy.spatial.distance.cdist and\n",
      "     |      scipy.spatial.distance.pdist will be faster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like\n",
      "     |          Array of shape (Nx, D), representing Nx points in D dimensions.\n",
      "     |      Y : array-like (optional)\n",
      "     |          Array of shape (Ny, D), representing Ny points in D dimensions.\n",
      "     |          If not specified, then Y=X.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dist : ndarray\n",
      "     |          The shape (Nx, Ny) array of pairwise distances between points in\n",
      "     |          X and Y.\n",
      "     |  \n",
      "     |  rdist_to_dist(...)\n",
      "     |      Convert the rank-preserving surrogate distance to the distance.\n",
      "     |      \n",
      "     |      The surrogate distance is any measure that yields the same rank as the\n",
      "     |      distance, but is more efficient to compute. For example, for the\n",
      "     |      Euclidean metric, the surrogate distance is the squared-euclidean distance.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      rdist : double\n",
      "     |          Surrogate distance.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          True distance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from sklearn.metrics._dist_metrics.DistanceMetric:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.metrics._dist_metrics.DistanceMetric:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "    \n",
      "    class KDTree(BinaryTree)\n",
      "     |  KDTree(X, leaf_size=40, metric='minkowski', **kwargs)\n",
      "     |  \n",
      "     |  KDTree for fast generalized N-point problems\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <unsupervised_neighbors>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  X : array-like of shape (n_samples, n_features)\n",
      "     |      n_samples is the number of points in the data set, and\n",
      "     |      n_features is the dimension of the parameter space.\n",
      "     |      Note: if X is a C-contiguous array of doubles then data will\n",
      "     |      not be copied. Otherwise, an internal copy will be made.\n",
      "     |  \n",
      "     |  leaf_size : positive int, default=40\n",
      "     |      Number of points at which to switch to brute-force. Changing\n",
      "     |      leaf_size will not affect the results of a query, but can\n",
      "     |      significantly impact the speed of a query and the memory required\n",
      "     |      to store the constructed tree.  The amount of memory needed to\n",
      "     |      store the tree scales as approximately n_samples / leaf_size.\n",
      "     |      For a specified ``leaf_size``, a leaf node is guaranteed to\n",
      "     |      satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in\n",
      "     |      the case that ``n_samples < leaf_size``.\n",
      "     |  \n",
      "     |  metric : str or DistanceMetric object\n",
      "     |      the distance metric to use for the tree.  Default='minkowski'\n",
      "     |      with p=2 (that is, a euclidean metric). See the documentation\n",
      "     |      of the DistanceMetric class for a list of available metrics.\n",
      "     |      kd_tree.valid_metrics gives a list of the metrics which\n",
      "     |      are valid for KDTree.\n",
      "     |  \n",
      "     |  Additional keywords are passed to the distance metric class.\n",
      "     |  Note: Callable functions in the metric parameter are NOT supported for KDTree\n",
      "     |  and Ball Tree. Function call overhead will result in very poor performance.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  data : memory view\n",
      "     |      The training data\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Query for k-nearest neighbors\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> from sklearn.neighbors import KDTree\n",
      "     |      >>> rng = np.random.RandomState(0)\n",
      "     |      >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n",
      "     |      >>> tree = KDTree(X, leaf_size=2)              # doctest: +SKIP\n",
      "     |      >>> dist, ind = tree.query(X[:1], k=3)                # doctest: +SKIP\n",
      "     |      >>> print(ind)  # indices of 3 closest neighbors\n",
      "     |      [0 3 1]\n",
      "     |      >>> print(dist)  # distances to 3 closest neighbors\n",
      "     |      [ 0.          0.19662693  0.29473397]\n",
      "     |  \n",
      "     |  Pickle and Unpickle a tree.  Note that the state of the tree is saved in the\n",
      "     |  pickle operation: the tree needs not be rebuilt upon unpickling.\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> import pickle\n",
      "     |      >>> rng = np.random.RandomState(0)\n",
      "     |      >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n",
      "     |      >>> tree = KDTree(X, leaf_size=2)        # doctest: +SKIP\n",
      "     |      >>> s = pickle.dumps(tree)                     # doctest: +SKIP\n",
      "     |      >>> tree_copy = pickle.loads(s)                # doctest: +SKIP\n",
      "     |      >>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP\n",
      "     |      >>> print(ind)  # indices of 3 closest neighbors\n",
      "     |      [0 3 1]\n",
      "     |      >>> print(dist)  # distances to 3 closest neighbors\n",
      "     |      [ 0.          0.19662693  0.29473397]\n",
      "     |  \n",
      "     |  Query for neighbors within a given radius\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> rng = np.random.RandomState(0)\n",
      "     |      >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n",
      "     |      >>> tree = KDTree(X, leaf_size=2)     # doctest: +SKIP\n",
      "     |      >>> print(tree.query_radius(X[:1], r=0.3, count_only=True))\n",
      "     |      3\n",
      "     |      >>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP\n",
      "     |      >>> print(ind)  # indices of neighbors within distance 0.3\n",
      "     |      [3 0 1]\n",
      "     |  \n",
      "     |  \n",
      "     |  Compute a gaussian kernel density estimate:\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> rng = np.random.RandomState(42)\n",
      "     |      >>> X = rng.random_sample((100, 3))\n",
      "     |      >>> tree = KDTree(X)                # doctest: +SKIP\n",
      "     |      >>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')\n",
      "     |      array([ 6.94114649,  7.83281226,  7.2071716 ])\n",
      "     |  \n",
      "     |  Compute a two-point auto-correlation function\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> rng = np.random.RandomState(0)\n",
      "     |      >>> X = rng.random_sample((30, 3))\n",
      "     |      >>> r = np.linspace(0, 1, 5)\n",
      "     |      >>> tree = KDTree(X)                # doctest: +SKIP\n",
      "     |      >>> tree.two_point_correlation(X, r)\n",
      "     |      array([ 30,  62, 278, 580, 820])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KDTree\n",
      "     |      BinaryTree\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BinaryTree:\n",
      "     |  \n",
      "     |  __getstate__(...)\n",
      "     |      get state for pickling\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      reduce method used for pickling\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |      set state for pickling\n",
      "     |  \n",
      "     |  get_arrays(...)\n",
      "     |      get_arrays(self)\n",
      "     |      \n",
      "     |      Get data and node arrays.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      arrays: tuple of array\n",
      "     |          Arrays for storing tree data, index, node data and node bounds.\n",
      "     |  \n",
      "     |  get_n_calls(...)\n",
      "     |      get_n_calls(self)\n",
      "     |      \n",
      "     |      Get number of calls.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_calls: int\n",
      "     |          number of distance computation calls\n",
      "     |  \n",
      "     |  get_tree_stats(...)\n",
      "     |      get_tree_stats(self)\n",
      "     |      \n",
      "     |      Get tree status.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      tree_stats: tuple of int\n",
      "     |          (number of trims, number of leaves, number of splits)\n",
      "     |  \n",
      "     |  kernel_density(...)\n",
      "     |      kernel_density(self, X, h, kernel='gaussian', atol=0, rtol=1E-8,\n",
      "     |                     breadth_first=True, return_log=False)\n",
      "     |      \n",
      "     |      Compute the kernel density estimate at points X with the given kernel,\n",
      "     |      using the distance metric specified at tree creation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query.  Last dimension should match dimension\n",
      "     |          of training data.\n",
      "     |      h : float\n",
      "     |          the bandwidth of the kernel\n",
      "     |      kernel : str, default=\"gaussian\"\n",
      "     |          specify the kernel to use.  Options are\n",
      "     |          - 'gaussian'\n",
      "     |          - 'tophat'\n",
      "     |          - 'epanechnikov'\n",
      "     |          - 'exponential'\n",
      "     |          - 'linear'\n",
      "     |          - 'cosine'\n",
      "     |          Default is kernel = 'gaussian'\n",
      "     |      atol : float, default=0\n",
      "     |          Specify the desired absolute tolerance of the result.\n",
      "     |          If the true result is `K_true`, then the returned result `K_ret`\n",
      "     |          satisfies ``abs(K_true - K_ret) < atol + rtol * K_ret``\n",
      "     |          The default is zero (i.e. machine precision).\n",
      "     |      rtol : float, default=1e-8\n",
      "     |          Specify the desired relative tolerance of the result.\n",
      "     |          If the true result is `K_true`, then the returned result `K_ret`\n",
      "     |          satisfies ``abs(K_true - K_ret) < atol + rtol * K_ret``\n",
      "     |          The default is `1e-8` (i.e. machine precision).\n",
      "     |      breadth_first : bool, default=False\n",
      "     |          If True, use a breadth-first search.  If False (default) use a\n",
      "     |          depth-first search.  Breadth-first is generally faster for\n",
      "     |          compact kernels and/or high tolerances.\n",
      "     |      return_log : bool, default=False\n",
      "     |          Return the logarithm of the result.  This can be more accurate\n",
      "     |          than returning the result itself for narrow kernels.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      density : ndarray of shape X.shape[:-1]\n",
      "     |          The array of (log)-density evaluations\n",
      "     |  \n",
      "     |  query(...)\n",
      "     |      query(X, k=1, return_distance=True,\n",
      "     |            dualtree=False, breadth_first=False)\n",
      "     |      \n",
      "     |      query the tree for the k nearest neighbors\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query\n",
      "     |      k : int, default=1\n",
      "     |          The number of nearest neighbors to return\n",
      "     |      return_distance : bool, default=True\n",
      "     |          if True, return a tuple (d, i) of distances and indices\n",
      "     |          if False, return array i\n",
      "     |      dualtree : bool, default=False\n",
      "     |          if True, use the dual tree formalism for the query: a tree is\n",
      "     |          built for the query points, and the pair of trees is used to\n",
      "     |          efficiently search this space.  This can lead to better\n",
      "     |          performance as the number of points grows large.\n",
      "     |      breadth_first : bool, default=False\n",
      "     |          if True, then query the nodes in a breadth-first manner.\n",
      "     |          Otherwise, query the nodes in a depth-first manner.\n",
      "     |      sort_results : bool, default=True\n",
      "     |          if True, then distances and indices of each point are sorted\n",
      "     |          on return, so that the first column contains the closest points.\n",
      "     |          Otherwise, neighbors are returned in an arbitrary order.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      i    : if return_distance == False\n",
      "     |      (d,i) : if return_distance == True\n",
      "     |      \n",
      "     |      d : ndarray of shape X.shape[:-1] + (k,), dtype=double\n",
      "     |          Each entry gives the list of distances to the neighbors of the\n",
      "     |          corresponding point.\n",
      "     |      \n",
      "     |      i : ndarray of shape X.shape[:-1] + (k,), dtype=int\n",
      "     |          Each entry gives the list of indices of neighbors of the\n",
      "     |          corresponding point.\n",
      "     |  \n",
      "     |  query_radius(...)\n",
      "     |      query_radius(X, r, return_distance=False,\n",
      "     |      count_only=False, sort_results=False)\n",
      "     |      \n",
      "     |      query the tree for neighbors within a radius r\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query\n",
      "     |      r : distance within which neighbors are returned\n",
      "     |          r can be a single value, or an array of values of shape\n",
      "     |          x.shape[:-1] if different radii are desired for each point.\n",
      "     |      return_distance : bool, default=False\n",
      "     |          if True,  return distances to neighbors of each point\n",
      "     |          if False, return only neighbors\n",
      "     |          Note that unlike the query() method, setting return_distance=True\n",
      "     |          here adds to the computation time.  Not all distances need to be\n",
      "     |          calculated explicitly for return_distance=False.  Results are\n",
      "     |          not sorted by default: see ``sort_results`` keyword.\n",
      "     |      count_only : bool, default=False\n",
      "     |          if True,  return only the count of points within distance r\n",
      "     |          if False, return the indices of all points within distance r\n",
      "     |          If return_distance==True, setting count_only=True will\n",
      "     |          result in an error.\n",
      "     |      sort_results : bool, default=False\n",
      "     |          if True, the distances and indices will be sorted before being\n",
      "     |          returned.  If False, the results will not be sorted.  If\n",
      "     |          return_distance == False, setting sort_results = True will\n",
      "     |          result in an error.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      count       : if count_only == True\n",
      "     |      ind         : if count_only == False and return_distance == False\n",
      "     |      (ind, dist) : if count_only == False and return_distance == True\n",
      "     |      \n",
      "     |      count : ndarray of shape X.shape[:-1], dtype=int\n",
      "     |          Each entry gives the number of neighbors within a distance r of the\n",
      "     |          corresponding point.\n",
      "     |      \n",
      "     |      ind : ndarray of shape X.shape[:-1], dtype=object\n",
      "     |          Each element is a numpy integer array listing the indices of\n",
      "     |          neighbors of the corresponding point.  Note that unlike\n",
      "     |          the results of a k-neighbors query, the returned neighbors\n",
      "     |          are not sorted by distance by default.\n",
      "     |      \n",
      "     |      dist : ndarray of shape X.shape[:-1], dtype=object\n",
      "     |          Each element is a numpy double array listing the distances\n",
      "     |          corresponding to indices in i.\n",
      "     |  \n",
      "     |  reset_n_calls(...)\n",
      "     |      reset_n_calls(self)\n",
      "     |      \n",
      "     |      Reset number of calls to 0.\n",
      "     |  \n",
      "     |  two_point_correlation(...)\n",
      "     |      two_point_correlation(X, r, dualtree=False)\n",
      "     |      \n",
      "     |      Compute the two-point correlation function\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query.  Last dimension should match dimension\n",
      "     |          of training data.\n",
      "     |      r : array-like\n",
      "     |          A one-dimensional array of distances\n",
      "     |      dualtree : bool, default=False\n",
      "     |          If True, use a dualtree algorithm.  Otherwise, use a single-tree\n",
      "     |          algorithm.  Dual tree algorithms can have better scaling for\n",
      "     |          large N.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      counts : ndarray\n",
      "     |          counts[i] contains the number of pairs of points with distance\n",
      "     |          less than or equal to r[i]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BinaryTree:\n",
      "     |  \n",
      "     |  data\n",
      "     |  \n",
      "     |  idx_array\n",
      "     |  \n",
      "     |  node_bounds\n",
      "     |  \n",
      "     |  node_data\n",
      "     |  \n",
      "     |  sample_weight\n",
      "     |  \n",
      "     |  sum_weight\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BinaryTree:\n",
      "     |  \n",
      "     |  valid_metrics = ['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'ci...\n",
      "    \n",
      "    class KNeighborsClassifier(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "     |  KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
      "     |  \n",
      "     |  Classifier implementing the k-nearest neighbors vote.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <classification>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_neighbors : int, default=5\n",
      "     |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      "     |  \n",
      "     |  weights : {'uniform', 'distance'} or callable, default='uniform'\n",
      "     |      Weight function used in prediction.  Possible values:\n",
      "     |  \n",
      "     |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      "     |        are weighted equally.\n",
      "     |      - 'distance' : weight points by the inverse of their distance.\n",
      "     |        in this case, closer neighbors of a query point will have a\n",
      "     |        greater influence than neighbors which are further away.\n",
      "     |      - [callable] : a user-defined function which accepts an\n",
      "     |        array of distances, and returns an array of the same shape\n",
      "     |        containing the weights.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method.\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      "     |      speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree.  The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  p : int, default=2\n",
      "     |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      "     |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric : str or callable, default='minkowski'\n",
      "     |      The distance metric to use for the tree.  The default metric is\n",
      "     |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      "     |      metric. For a list of available metrics, see the documentation of\n",
      "     |      :class:`~sklearn.metrics.DistanceMetric`.\n",
      "     |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "     |      must be square during fit. X may be a :term:`sparse graph`,\n",
      "     |      in which case only \"nonzero\" elements may be considered neighbors.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |      Doesn't affect :meth:`fit` method.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : array of shape (n_classes,)\n",
      "     |      Class labels known to the classifier\n",
      "     |  \n",
      "     |  effective_metric_ : str or callble\n",
      "     |      The distance metric used. It will be same as the `metric` parameter\n",
      "     |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      "     |      'minkowski' and `p` parameter set to 2.\n",
      "     |  \n",
      "     |  effective_metric_params_ : dict\n",
      "     |      Additional keyword arguments for the metric function. For most metrics\n",
      "     |      will be same with `metric_params` parameter, but may also contain the\n",
      "     |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      "     |      'minkowski'.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_samples_fit_ : int\n",
      "     |      Number of samples in the fitted data.\n",
      "     |  \n",
      "     |  outputs_2d_ : bool\n",
      "     |      False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n",
      "     |      otherwise True.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RadiusNeighborsClassifier: Classifier based on neighbors within a fixed radius.\n",
      "     |  KNeighborsRegressor: Regression based on k-nearest neighbors.\n",
      "     |  RadiusNeighborsRegressor: Regression based on neighbors within a fixed radius.\n",
      "     |  NearestNeighbors: Unsupervised learner for implementing neighbor searches.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      "     |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      "     |  \n",
      "     |  .. warning::\n",
      "     |  \n",
      "     |     Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      "     |     neighbors, neighbor `k+1` and `k`, have identical distances\n",
      "     |     but different labels, the results will depend on the ordering of the\n",
      "     |     training data.\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> X = [[0], [1], [2], [3]]\n",
      "     |  >>> y = [0, 0, 1, 1]\n",
      "     |  >>> from sklearn.neighbors import KNeighborsClassifier\n",
      "     |  >>> neigh = KNeighborsClassifier(n_neighbors=3)\n",
      "     |  >>> neigh.fit(X, y)\n",
      "     |  KNeighborsClassifier(...)\n",
      "     |  >>> print(neigh.predict([[1.1]]))\n",
      "     |  [0]\n",
      "     |  >>> print(neigh.predict_proba([[0.9]]))\n",
      "     |  [[0.666... 0.333...]]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KNeighborsClassifier\n",
      "     |      sklearn.neighbors._base.KNeighborsMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.neighbors._base.NeighborsBase\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the k-nearest neighbors classifier from the training dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : KNeighborsClassifier\n",
      "     |          The fitted k-nearest neighbors classifier.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict the class labels for the provided data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n",
      "     |          Class labels for each data sample.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Return probability estimates for the test data X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_queries, n_classes), or a list of n_outputs                 of such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. Classes are ordered\n",
      "     |          by lexicographic order.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      "     |      Find the K-neighbors of a point.\n",
      "     |      \n",
      "     |      Returns indices of and distances to the neighbors of each point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors required for each sample. The default is the\n",
      "     |          value passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Array representing the lengths to points, only present if\n",
      "     |          return_distance=True.\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Indices of the nearest points in the population matrix.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NearestNeighbors\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1,1,1]\n",
      "     |      \n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
      "     |      (array([[0.5]]), array([[2]]))\n",
      "     |      \n",
      "     |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      "     |      element is at distance 0.5 and is the third element of samples\n",
      "     |      (indexes start at 0). You can also query for multiple points:\n",
      "     |      \n",
      "     |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      "     |      >>> neigh.kneighbors(X, return_distance=False)\n",
      "     |      array([[1],\n",
      "     |             [2]]...)\n",
      "     |  \n",
      "     |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      "     |      Compute the (weighted) graph of k-Neighbors for points in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |          For ``metric='precomputed'`` the shape should be\n",
      "     |          (n_queries, n_indexed). Otherwise the shape should be\n",
      "     |          (n_queries, n_features).\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors for each sample. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are distances between points, type of distance\n",
      "     |          depends on the selected metric parameter in\n",
      "     |          NearestNeighbors class.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data.\n",
      "     |          `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n",
      "     |          The matrix is of CSR format.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n",
      "     |          of Neighbors for points in X.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> A = neigh.kneighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 1.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class KNeighborsRegressor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "     |  KNeighborsRegressor(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
      "     |  \n",
      "     |  Regression based on k-nearest neighbors.\n",
      "     |  \n",
      "     |  The target is predicted by local interpolation of the targets\n",
      "     |  associated of the nearest neighbors in the training set.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <regression>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.9\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_neighbors : int, default=5\n",
      "     |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      "     |  \n",
      "     |  weights : {'uniform', 'distance'} or callable, default='uniform'\n",
      "     |      Weight function used in prediction.  Possible values:\n",
      "     |  \n",
      "     |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      "     |        are weighted equally.\n",
      "     |      - 'distance' : weight points by the inverse of their distance.\n",
      "     |        in this case, closer neighbors of a query point will have a\n",
      "     |        greater influence than neighbors which are further away.\n",
      "     |      - [callable] : a user-defined function which accepts an\n",
      "     |        array of distances, and returns an array of the same shape\n",
      "     |        containing the weights.\n",
      "     |  \n",
      "     |      Uniform weights are used by default.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method.\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      "     |      speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree.  The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  p : int, default=2\n",
      "     |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      "     |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric : str or callable, default='minkowski'\n",
      "     |      The distance metric to use for the tree.  The default metric is\n",
      "     |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      "     |      metric. See the documentation of :class:`DistanceMetric` for a\n",
      "     |      list of available metrics.\n",
      "     |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "     |      must be square during fit. X may be a :term:`sparse graph`,\n",
      "     |      in which case only \"nonzero\" elements may be considered neighbors.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |      Doesn't affect :meth:`fit` method.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  effective_metric_ : str or callable\n",
      "     |      The distance metric to use. It will be same as the `metric` parameter\n",
      "     |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      "     |      'minkowski' and `p` parameter set to 2.\n",
      "     |  \n",
      "     |  effective_metric_params_ : dict\n",
      "     |      Additional keyword arguments for the metric function. For most metrics\n",
      "     |      will be same with `metric_params` parameter, but may also contain the\n",
      "     |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      "     |      'minkowski'.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_samples_fit_ : int\n",
      "     |      Number of samples in the fitted data.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  NearestNeighbors : Unsupervised learner for implementing neighbor searches.\n",
      "     |  RadiusNeighborsRegressor : Regression based on neighbors within a fixed radius.\n",
      "     |  KNeighborsClassifier : Classifier implementing the k-nearest neighbors vote.\n",
      "     |  RadiusNeighborsClassifier : Classifier implementing\n",
      "     |      a vote among neighbors within a given radius.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      "     |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      "     |  \n",
      "     |  .. warning::\n",
      "     |  \n",
      "     |     Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      "     |     neighbors, neighbor `k+1` and `k`, have identical distances but\n",
      "     |     different labels, the results will depend on the ordering of the\n",
      "     |     training data.\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> X = [[0], [1], [2], [3]]\n",
      "     |  >>> y = [0, 0, 1, 1]\n",
      "     |  >>> from sklearn.neighbors import KNeighborsRegressor\n",
      "     |  >>> neigh = KNeighborsRegressor(n_neighbors=2)\n",
      "     |  >>> neigh.fit(X, y)\n",
      "     |  KNeighborsRegressor(...)\n",
      "     |  >>> print(neigh.predict([[1.5]]))\n",
      "     |  [0.5]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KNeighborsRegressor\n",
      "     |      sklearn.neighbors._base.KNeighborsMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.neighbors._base.NeighborsBase\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the k-nearest neighbors regressor from the training dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : KNeighborsRegressor\n",
      "     |          The fitted k-nearest neighbors regressor.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict the target for the provided data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_queries,) or (n_queries, n_outputs), dtype=int\n",
      "     |          Target values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      "     |      Find the K-neighbors of a point.\n",
      "     |      \n",
      "     |      Returns indices of and distances to the neighbors of each point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors required for each sample. The default is the\n",
      "     |          value passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Array representing the lengths to points, only present if\n",
      "     |          return_distance=True.\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Indices of the nearest points in the population matrix.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NearestNeighbors\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1,1,1]\n",
      "     |      \n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
      "     |      (array([[0.5]]), array([[2]]))\n",
      "     |      \n",
      "     |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      "     |      element is at distance 0.5 and is the third element of samples\n",
      "     |      (indexes start at 0). You can also query for multiple points:\n",
      "     |      \n",
      "     |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      "     |      >>> neigh.kneighbors(X, return_distance=False)\n",
      "     |      array([[1],\n",
      "     |             [2]]...)\n",
      "     |  \n",
      "     |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      "     |      Compute the (weighted) graph of k-Neighbors for points in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |          For ``metric='precomputed'`` the shape should be\n",
      "     |          (n_queries, n_indexed). Otherwise the shape should be\n",
      "     |          (n_queries, n_features).\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors for each sample. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are distances between points, type of distance\n",
      "     |          depends on the selected metric parameter in\n",
      "     |          NearestNeighbors class.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data.\n",
      "     |          `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n",
      "     |          The matrix is of CSR format.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n",
      "     |          of Neighbors for points in X.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> A = neigh.kneighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 1.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class KNeighborsTransformer(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.TransformerMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "     |  KNeighborsTransformer(*, mode='distance', n_neighbors=5, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=1)\n",
      "     |  \n",
      "     |  Transform X into a (weighted) graph of k nearest neighbors.\n",
      "     |  \n",
      "     |  The transformed data is a sparse graph as returned by kneighbors_graph.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <neighbors_transformer>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  mode : {'distance', 'connectivity'}, default='distance'\n",
      "     |      Type of returned matrix: 'connectivity' will return the connectivity\n",
      "     |      matrix with ones and zeros, and 'distance' will return the distances\n",
      "     |      between neighbors according to the given metric.\n",
      "     |  \n",
      "     |  n_neighbors : int, default=5\n",
      "     |      Number of neighbors for each sample in the transformed sparse graph.\n",
      "     |      For compatibility reasons, as each sample is considered as its own\n",
      "     |      neighbor, one extra neighbor will be computed when mode == 'distance'.\n",
      "     |      In this case, the sparse graph contains (n_neighbors + 1) neighbors.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method.\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      "     |      speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree.  The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  metric : str or callable, default='minkowski'\n",
      "     |      Metric to use for distance computation. Any metric from scikit-learn\n",
      "     |      or scipy.spatial.distance can be used.\n",
      "     |  \n",
      "     |      If metric is a callable function, it is called on each\n",
      "     |      pair of instances (rows) and the resulting value recorded. The callable\n",
      "     |      should take two arrays as input and return one value indicating the\n",
      "     |      distance between them. This works for Scipy's metrics, but is less\n",
      "     |      efficient than passing the metric name as a string.\n",
      "     |  \n",
      "     |      Distance matrices are not supported.\n",
      "     |  \n",
      "     |      Valid values for metric are:\n",
      "     |  \n",
      "     |      - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "     |        'manhattan']\n",
      "     |  \n",
      "     |      - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "     |        'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "     |        'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "     |        'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "     |        'yule']\n",
      "     |  \n",
      "     |      See the documentation for scipy.spatial.distance for details on these\n",
      "     |      metrics.\n",
      "     |  \n",
      "     |  p : int, default=2\n",
      "     |      Parameter for the Minkowski metric from\n",
      "     |      sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n",
      "     |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  n_jobs : int, default=1\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  effective_metric_ : str or callable\n",
      "     |      The distance metric used. It will be same as the `metric` parameter\n",
      "     |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      "     |      'minkowski' and `p` parameter set to 2.\n",
      "     |  \n",
      "     |  effective_metric_params_ : dict\n",
      "     |      Additional keyword arguments for the metric function. For most metrics\n",
      "     |      will be same with `metric_params` parameter, but may also contain the\n",
      "     |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      "     |      'minkowski'.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_samples_fit_ : int\n",
      "     |      Number of samples in the fitted data.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  kneighbors_graph : Compute the weighted graph of k-neighbors for\n",
      "     |      points in X.\n",
      "     |  RadiusNeighborsTransformer : Transform X into a weighted graph of\n",
      "     |      neighbors nearer than a radius.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_wine\n",
      "     |  >>> from sklearn.neighbors import KNeighborsTransformer\n",
      "     |  >>> X, _ = load_wine(return_X_y=True)\n",
      "     |  >>> X.shape\n",
      "     |  (178, 13)\n",
      "     |  >>> transformer = KNeighborsTransformer(n_neighbors=5, mode='distance')\n",
      "     |  >>> X_dist_graph = transformer.fit_transform(X)\n",
      "     |  >>> X_dist_graph.shape\n",
      "     |  (178, 178)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KNeighborsTransformer\n",
      "     |      sklearn.neighbors._base.KNeighborsMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.neighbors._base.NeighborsBase\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, mode='distance', n_neighbors=5, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit the k-nearest neighbors transformer from the training dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      "     |          Training data.\n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : KNeighborsTransformer\n",
      "     |          The fitted k-nearest neighbors transformer.\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : sparse matrix of shape (n_samples, n_samples)\n",
      "     |          Xt[i, j] is assigned the weight of edge that connects i to j.\n",
      "     |          Only the neighbors have an explicit value.\n",
      "     |          The diagonal is always explicit.\n",
      "     |          The matrix is of CSR format.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Compute the (weighted) graph of Neighbors for points in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples_transform, n_features)\n",
      "     |          Sample data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : sparse matrix of shape (n_samples_transform, n_samples_fit)\n",
      "     |          Xt[i, j] is assigned the weight of edge that connects i to j.\n",
      "     |          Only the neighbors have an explicit value.\n",
      "     |          The diagonal is always explicit.\n",
      "     |          The matrix is of CSR format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      "     |      Find the K-neighbors of a point.\n",
      "     |      \n",
      "     |      Returns indices of and distances to the neighbors of each point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors required for each sample. The default is the\n",
      "     |          value passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Array representing the lengths to points, only present if\n",
      "     |          return_distance=True.\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Indices of the nearest points in the population matrix.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NearestNeighbors\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1,1,1]\n",
      "     |      \n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
      "     |      (array([[0.5]]), array([[2]]))\n",
      "     |      \n",
      "     |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      "     |      element is at distance 0.5 and is the third element of samples\n",
      "     |      (indexes start at 0). You can also query for multiple points:\n",
      "     |      \n",
      "     |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      "     |      >>> neigh.kneighbors(X, return_distance=False)\n",
      "     |      array([[1],\n",
      "     |             [2]]...)\n",
      "     |  \n",
      "     |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      "     |      Compute the (weighted) graph of k-Neighbors for points in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |          For ``metric='precomputed'`` the shape should be\n",
      "     |          (n_queries, n_indexed). Otherwise the shape should be\n",
      "     |          (n_queries, n_features).\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors for each sample. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are distances between points, type of distance\n",
      "     |          depends on the selected metric parameter in\n",
      "     |          NearestNeighbors class.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data.\n",
      "     |          `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n",
      "     |          The matrix is of CSR format.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n",
      "     |          of Neighbors for points in X.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> A = neigh.kneighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 1.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class KernelDensity(sklearn.base.BaseEstimator)\n",
      "     |  KernelDensity(*, bandwidth=1.0, algorithm='auto', kernel='gaussian', metric='euclidean', atol=0, rtol=0, breadth_first=True, leaf_size=40, metric_params=None)\n",
      "     |  \n",
      "     |  Kernel Density Estimation.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <kernel_density>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  bandwidth : float, default=1.0\n",
      "     |      The bandwidth of the kernel.\n",
      "     |  \n",
      "     |  algorithm : {'kd_tree', 'ball_tree', 'auto'}, default='auto'\n",
      "     |      The tree algorithm to use.\n",
      "     |  \n",
      "     |  kernel : {'gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear',                  'cosine'}, default='gaussian'\n",
      "     |      The kernel to use.\n",
      "     |  \n",
      "     |  metric : str, default='euclidean'\n",
      "     |      The distance metric to use.  Note that not all metrics are\n",
      "     |      valid with all algorithms.  Refer to the documentation of\n",
      "     |      :class:`BallTree` and :class:`KDTree` for a description of\n",
      "     |      available algorithms.  Note that the normalization of the density\n",
      "     |      output is correct only for the Euclidean distance metric. Default\n",
      "     |      is 'euclidean'.\n",
      "     |  \n",
      "     |  atol : float, default=0\n",
      "     |      The desired absolute tolerance of the result.  A larger tolerance will\n",
      "     |      generally lead to faster execution.\n",
      "     |  \n",
      "     |  rtol : float, default=0\n",
      "     |      The desired relative tolerance of the result.  A larger tolerance will\n",
      "     |      generally lead to faster execution.\n",
      "     |  \n",
      "     |  breadth_first : bool, default=True\n",
      "     |      If true (default), use a breadth-first approach to the problem.\n",
      "     |      Otherwise use a depth-first approach.\n",
      "     |  \n",
      "     |  leaf_size : int, default=40\n",
      "     |      Specify the leaf size of the underlying tree.  See :class:`BallTree`\n",
      "     |      or :class:`KDTree` for details.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional parameters to be passed to the tree for use with the\n",
      "     |      metric.  For more information, see the documentation of\n",
      "     |      :class:`BallTree` or :class:`KDTree`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  tree_ : ``BinaryTree`` instance\n",
      "     |      The tree algorithm for fast generalized N-point problems.\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.neighbors.KDTree : K-dimensional tree for fast generalized N-point\n",
      "     |      problems.\n",
      "     |  sklearn.neighbors.BallTree : Ball tree for fast generalized N-point\n",
      "     |      problems.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Compute a gaussian kernel density estimate with a fixed bandwidth.\n",
      "     |  \n",
      "     |  >>> from sklearn.neighbors import KernelDensity\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> rng = np.random.RandomState(42)\n",
      "     |  >>> X = rng.random_sample((100, 3))\n",
      "     |  >>> kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X)\n",
      "     |  >>> log_density = kde.score_samples(X[:3])\n",
      "     |  >>> log_density\n",
      "     |  array([-1.52955942, -1.51462041, -1.60244657])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KernelDensity\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, bandwidth=1.0, algorithm='auto', kernel='gaussian', metric='euclidean', atol=0, rtol=0, breadth_first=True, leaf_size=40, metric_params=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Fit the Kernel Density model on the data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          List of n_features-dimensional data points.  Each row\n",
      "     |          corresponds to a single data point.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored. This parameter exists only for compatibility with\n",
      "     |          :class:`~sklearn.pipeline.Pipeline`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          List of sample weights attached to the data X.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.20\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  sample(self, n_samples=1, random_state=None)\n",
      "     |      Generate random samples from the model.\n",
      "     |      \n",
      "     |      Currently, this is implemented only for gaussian and tophat kernels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n_samples : int, default=1\n",
      "     |          Number of samples to generate.\n",
      "     |      \n",
      "     |      random_state : int, RandomState instance or None, default=None\n",
      "     |          Determines random number generation used to generate\n",
      "     |          random samples. Pass an int for reproducible results\n",
      "     |          across multiple function calls.\n",
      "     |          See :term:`Glossary <random_state>`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          List of samples.\n",
      "     |  \n",
      "     |  score(self, X, y=None)\n",
      "     |      Compute the total log-likelihood under the model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          List of n_features-dimensional data points.  Each row\n",
      "     |          corresponds to a single data point.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored. This parameter exists only for compatibility with\n",
      "     |          :class:`~sklearn.pipeline.Pipeline`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Total log-likelihood of the data in X. This is normalized to be a\n",
      "     |          probability density, so the value will be low for high-dimensional\n",
      "     |          data.\n",
      "     |  \n",
      "     |  score_samples(self, X)\n",
      "     |      Compute the log-likelihood of each sample under the model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query.  Last dimension should match dimension\n",
      "     |          of training data (n_features).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      density : ndarray of shape (n_samples,)\n",
      "     |          Log-likelihood of each sample in `X`. These are normalized to be\n",
      "     |          probability densities, so values will be low for high-dimensional\n",
      "     |          data.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LocalOutlierFactor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.OutlierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "     |  LocalOutlierFactor(n_neighbors=20, *, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, contamination='auto', novelty=False, n_jobs=None)\n",
      "     |  \n",
      "     |  Unsupervised Outlier Detection using the Local Outlier Factor (LOF).\n",
      "     |  \n",
      "     |  The anomaly score of each sample is called the Local Outlier Factor.\n",
      "     |  It measures the local deviation of the density of a given sample with respect\n",
      "     |  to its neighbors.\n",
      "     |  It is local in that the anomaly score depends on how isolated the object\n",
      "     |  is with respect to the surrounding neighborhood.\n",
      "     |  More precisely, locality is given by k-nearest neighbors, whose distance\n",
      "     |  is used to estimate the local density.\n",
      "     |  By comparing the local density of a sample to the local densities of its\n",
      "     |  neighbors, one can identify samples that have a substantially lower density\n",
      "     |  than their neighbors. These are considered outliers.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_neighbors : int, default=20\n",
      "     |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      "     |      If n_neighbors is larger than the number of samples provided,\n",
      "     |      all samples will be used.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method.\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf is size passed to :class:`BallTree` or :class:`KDTree`. This can\n",
      "     |      affect the speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree. The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  metric : str or callable, default='minkowski'\n",
      "     |      The metric is used for distance computation. Any metric from scikit-learn\n",
      "     |      or scipy.spatial.distance can be used.\n",
      "     |  \n",
      "     |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "     |      must be square. X may be a sparse matrix, in which case only \"nonzero\"\n",
      "     |      elements may be considered neighbors.\n",
      "     |  \n",
      "     |      If metric is a callable function, it is called on each\n",
      "     |      pair of instances (rows) and the resulting value recorded. The callable\n",
      "     |      should take two arrays as input and return one value indicating the\n",
      "     |      distance between them. This works for Scipy's metrics, but is less\n",
      "     |      efficient than passing the metric name as a string.\n",
      "     |  \n",
      "     |      Valid values for metric are:\n",
      "     |  \n",
      "     |      - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "     |        'manhattan']\n",
      "     |  \n",
      "     |      - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "     |        'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "     |        'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "     |        'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "     |        'yule']\n",
      "     |  \n",
      "     |      See the documentation for scipy.spatial.distance for details on these\n",
      "     |      metrics:\n",
      "     |      https://docs.scipy.org/doc/scipy/reference/spatial.distance.html.\n",
      "     |  \n",
      "     |  p : int, default=2\n",
      "     |      Parameter for the Minkowski metric from\n",
      "     |      :func:`sklearn.metrics.pairwise.pairwise_distances`. When p = 1, this\n",
      "     |      is equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  contamination : 'auto' or float, default='auto'\n",
      "     |      The amount of contamination of the data set, i.e. the proportion\n",
      "     |      of outliers in the data set. When fitting this is used to define the\n",
      "     |      threshold on the scores of the samples.\n",
      "     |  \n",
      "     |      - if 'auto', the threshold is determined as in the\n",
      "     |        original paper,\n",
      "     |      - if a float, the contamination should be in the range (0, 0.5].\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``contamination`` changed from 0.1\n",
      "     |         to ``'auto'``.\n",
      "     |  \n",
      "     |  novelty : bool, default=False\n",
      "     |      By default, LocalOutlierFactor is only meant to be used for outlier\n",
      "     |      detection (novelty=False). Set novelty to True if you want to use\n",
      "     |      LocalOutlierFactor for novelty detection. In this case be aware that\n",
      "     |      you should only use predict, decision_function and score_samples\n",
      "     |      on new unseen data and not on the training set.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  negative_outlier_factor_ : ndarray of shape (n_samples,)\n",
      "     |      The opposite LOF of the training samples. The higher, the more normal.\n",
      "     |      Inliers tend to have a LOF score close to 1\n",
      "     |      (``negative_outlier_factor_`` close to -1), while outliers tend to have\n",
      "     |      a larger LOF score.\n",
      "     |  \n",
      "     |      The local outlier factor (LOF) of a sample captures its\n",
      "     |      supposed 'degree of abnormality'.\n",
      "     |      It is the average of the ratio of the local reachability density of\n",
      "     |      a sample and those of its k-nearest neighbors.\n",
      "     |  \n",
      "     |  n_neighbors_ : int\n",
      "     |      The actual number of neighbors used for :meth:`kneighbors` queries.\n",
      "     |  \n",
      "     |  offset_ : float\n",
      "     |      Offset used to obtain binary labels from the raw scores.\n",
      "     |      Observations having a negative_outlier_factor smaller than `offset_`\n",
      "     |      are detected as abnormal.\n",
      "     |      The offset is set to -1.5 (inliers score around -1), except when a\n",
      "     |      contamination parameter different than \"auto\" is provided. In that\n",
      "     |      case, the offset is defined in such a way we obtain the expected\n",
      "     |      number of outliers in training.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  effective_metric_ : str\n",
      "     |      The effective metric used for the distance computation.\n",
      "     |  \n",
      "     |  effective_metric_params_ : dict\n",
      "     |      The effective additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_samples_fit_ : int\n",
      "     |      It is the number of samples in the fitted data.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  ----------\n",
      "     |  sklearn.svm.OneClassSVM: Unsupervised Outlier Detection using\n",
      "     |      Support Vector Machine.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000, May).\n",
      "     |         LOF: identifying density-based local outliers. In ACM sigmod record.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.neighbors import LocalOutlierFactor\n",
      "     |  >>> X = [[-1.1], [0.2], [101.1], [0.3]]\n",
      "     |  >>> clf = LocalOutlierFactor(n_neighbors=2)\n",
      "     |  >>> clf.fit_predict(X)\n",
      "     |  array([ 1,  1, -1,  1])\n",
      "     |  >>> clf.negative_outlier_factor_\n",
      "     |  array([ -0.9821...,  -1.0370..., -73.3697...,  -0.9821...])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LocalOutlierFactor\n",
      "     |      sklearn.neighbors._base.KNeighborsMixin\n",
      "     |      sklearn.base.OutlierMixin\n",
      "     |      sklearn.neighbors._base.NeighborsBase\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_neighbors=20, *, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, contamination='auto', novelty=False, n_jobs=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Shifted opposite of the Local Outlier Factor of X.\n",
      "     |      \n",
      "     |      Bigger is better, i.e. large values correspond to inliers.\n",
      "     |      \n",
      "     |      **Only available for novelty detection (when novelty is set to True).**\n",
      "     |      The shift offset allows a zero threshold for being an outlier.\n",
      "     |      The argument X is supposed to contain *new data*: if X contains a\n",
      "     |      point from training, it considers the later in its own neighborhood.\n",
      "     |      Also, the samples in X are not considered in the neighborhood of any\n",
      "     |      point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The query sample or samples to compute the Local Outlier Factor\n",
      "     |          w.r.t. the training samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      shifted_opposite_lof_scores : ndarray of shape (n_samples,)\n",
      "     |          The shifted opposite of the Local Outlier Factor of each input\n",
      "     |          samples. The lower, the more abnormal. Negative scores represent\n",
      "     |          outliers, positive scores represent inliers.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit the local outlier factor detector from the training dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : LocalOutlierFactor\n",
      "     |          The fitted local outlier factor detector.\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None)\n",
      "     |      Fit the model to the training set X and return the labels.\n",
      "     |      \n",
      "     |      **Not available for novelty detection (when novelty is set to True).**\n",
      "     |      Label is 1 for an inlier and -1 for an outlier according to the LOF\n",
      "     |      score and the contamination parameter.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features), default=None\n",
      "     |          The query sample or samples to compute the Local Outlier Factor\n",
      "     |          w.r.t. to the training samples.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      is_inlier : ndarray of shape (n_samples,)\n",
      "     |          Returns -1 for anomalies/outliers and 1 for inliers.\n",
      "     |  \n",
      "     |  predict(self, X=None)\n",
      "     |      Predict the labels (1 inlier, -1 outlier) of X according to LOF.\n",
      "     |      \n",
      "     |      **Only available for novelty detection (when novelty is set to True).**\n",
      "     |      This method allows to generalize prediction to *new observations* (not\n",
      "     |      in the training set).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The query sample or samples to compute the Local Outlier Factor\n",
      "     |          w.r.t. to the training samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      is_inlier : ndarray of shape (n_samples,)\n",
      "     |          Returns -1 for anomalies/outliers and +1 for inliers.\n",
      "     |  \n",
      "     |  score_samples(self, X)\n",
      "     |      Opposite of the Local Outlier Factor of X.\n",
      "     |      \n",
      "     |      It is the opposite as bigger is better, i.e. large values correspond\n",
      "     |      to inliers.\n",
      "     |      \n",
      "     |      **Only available for novelty detection (when novelty is set to True).**\n",
      "     |      The argument X is supposed to contain *new data*: if X contains a\n",
      "     |      point from training, it considers the later in its own neighborhood.\n",
      "     |      Also, the samples in X are not considered in the neighborhood of any\n",
      "     |      point.\n",
      "     |      The score_samples on training data is available by considering the\n",
      "     |      the ``negative_outlier_factor_`` attribute.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The query sample or samples to compute the Local Outlier Factor\n",
      "     |          w.r.t. the training samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      opposite_lof_scores : ndarray of shape (n_samples,)\n",
      "     |          The opposite of the Local Outlier Factor of each input samples.\n",
      "     |          The lower, the more abnormal.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      "     |      Find the K-neighbors of a point.\n",
      "     |      \n",
      "     |      Returns indices of and distances to the neighbors of each point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors required for each sample. The default is the\n",
      "     |          value passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Array representing the lengths to points, only present if\n",
      "     |          return_distance=True.\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Indices of the nearest points in the population matrix.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NearestNeighbors\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1,1,1]\n",
      "     |      \n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
      "     |      (array([[0.5]]), array([[2]]))\n",
      "     |      \n",
      "     |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      "     |      element is at distance 0.5 and is the third element of samples\n",
      "     |      (indexes start at 0). You can also query for multiple points:\n",
      "     |      \n",
      "     |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      "     |      >>> neigh.kneighbors(X, return_distance=False)\n",
      "     |      array([[1],\n",
      "     |             [2]]...)\n",
      "     |  \n",
      "     |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      "     |      Compute the (weighted) graph of k-Neighbors for points in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |          For ``metric='precomputed'`` the shape should be\n",
      "     |          (n_queries, n_indexed). Otherwise the shape should be\n",
      "     |          (n_queries, n_features).\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors for each sample. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are distances between points, type of distance\n",
      "     |          depends on the selected metric parameter in\n",
      "     |          NearestNeighbors class.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data.\n",
      "     |          `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n",
      "     |          The matrix is of CSR format.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n",
      "     |          of Neighbors for points in X.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> A = neigh.kneighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 1.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class NearestCentroid(sklearn.base.ClassifierMixin, sklearn.base.BaseEstimator)\n",
      "     |  NearestCentroid(metric='euclidean', *, shrink_threshold=None)\n",
      "     |  \n",
      "     |  Nearest centroid classifier.\n",
      "     |  \n",
      "     |  Each class is represented by its centroid, with test samples classified to\n",
      "     |  the class with the nearest centroid.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <nearest_centroid_classifier>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  metric : str or callable, default=\"euclidian\"\n",
      "     |      The metric to use when calculating distance between instances in a\n",
      "     |      feature array. If metric is a string or callable, it must be one of\n",
      "     |      the options allowed by\n",
      "     |      :func:`~sklearn.metrics.pairwise_distances` for its metric\n",
      "     |      parameter. The centroids for the samples corresponding to each class is\n",
      "     |      the point from which the sum of the distances (according to the metric)\n",
      "     |      of all samples that belong to that particular class are minimized.\n",
      "     |      If the `\"manhattan\"` metric is provided, this centroid is the median\n",
      "     |      and for all other metrics, the centroid is now set to be the mean.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.19\n",
      "     |          `metric='precomputed'` was deprecated and now raises an error\n",
      "     |  \n",
      "     |  shrink_threshold : float, default=None\n",
      "     |      Threshold for shrinking centroids to remove features.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  centroids_ : array-like of shape (n_classes, n_features)\n",
      "     |      Centroid of each class.\n",
      "     |  \n",
      "     |  classes_ : array of shape (n_classes,)\n",
      "     |      The unique classes labels.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  KNeighborsClassifier : Nearest neighbors classifier.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  When used for text classification with tf-idf vectors, this classifier is\n",
      "     |  also known as the Rocchio classifier.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Tibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of\n",
      "     |  multiple cancer types by shrunken centroids of gene expression. Proceedings\n",
      "     |  of the National Academy of Sciences of the United States of America,\n",
      "     |  99(10), 6567-6572. The National Academy of Sciences.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.neighbors import NearestCentroid\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      "     |  >>> y = np.array([1, 1, 1, 2, 2, 2])\n",
      "     |  >>> clf = NearestCentroid()\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  NearestCentroid()\n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NearestCentroid\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, metric='euclidean', *, shrink_threshold=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the NearestCentroid model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vector, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |          Note that centroid shrinking cannot be used with sparse matrices.\n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on an array of test vectors `X`.\n",
      "     |      \n",
      "     |      The predicted class `C` for each sample in `X` is returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : ndarray of shape (n_samples,)\n",
      "     |          The predicted classes.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If the metric constructor parameter is `\"precomputed\"`, `X` is assumed\n",
      "     |      to be the distance matrix between the data to be predicted and\n",
      "     |      `self.centroids_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class NearestNeighbors(sklearn.neighbors._base.KNeighborsMixin, sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "     |  NearestNeighbors(*, n_neighbors=5, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)\n",
      "     |  \n",
      "     |  Unsupervised learner for implementing neighbor searches.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <unsupervised_neighbors>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.9\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_neighbors : int, default=5\n",
      "     |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      "     |  \n",
      "     |  radius : float, default=1.0\n",
      "     |      Range of parameter space to use by default for :meth:`radius_neighbors`\n",
      "     |      queries.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method.\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      "     |      speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree.  The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  metric : str or callable, default='minkowski'\n",
      "     |      The distance metric to use for the tree.  The default metric is\n",
      "     |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      "     |      metric. For a list of available metrics, see the documentation of\n",
      "     |      :class:`~sklearn.metrics.DistanceMetric`.\n",
      "     |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "     |      must be square during fit. X may be a :term:`sparse graph`,\n",
      "     |      in which case only \"nonzero\" elements may be considered neighbors.\n",
      "     |  \n",
      "     |  p : int, default=2\n",
      "     |      Parameter for the Minkowski metric from\n",
      "     |      sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n",
      "     |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  effective_metric_ : str\n",
      "     |      Metric used to compute distances to neighbors.\n",
      "     |  \n",
      "     |  effective_metric_params_ : dict\n",
      "     |      Parameters for the metric used to compute distances to neighbors.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_samples_fit_ : int\n",
      "     |      Number of samples in the fitted data.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  KNeighborsClassifier : Classifier implementing the k-nearest neighbors\n",
      "     |      vote.\n",
      "     |  RadiusNeighborsClassifier : Classifier implementing a vote among neighbors\n",
      "     |      within a given radius.\n",
      "     |  KNeighborsRegressor : Regression based on k-nearest neighbors.\n",
      "     |  RadiusNeighborsRegressor : Regression based on neighbors within a fixed\n",
      "     |      radius.\n",
      "     |  BallTree : Space partitioning data structure for organizing points in a\n",
      "     |      multi-dimensional space, used for nearest neighbor search.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      "     |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |  >>> samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n",
      "     |  \n",
      "     |  >>> neigh = NearestNeighbors(n_neighbors=2, radius=0.4)\n",
      "     |  >>> neigh.fit(samples)\n",
      "     |  NearestNeighbors(...)\n",
      "     |  \n",
      "     |  >>> neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)\n",
      "     |  array([[2, 0]]...)\n",
      "     |  \n",
      "     |  >>> nbrs = neigh.radius_neighbors(\n",
      "     |  ...    [[0, 0, 1.3]], 0.4, return_distance=False\n",
      "     |  ... )\n",
      "     |  >>> np.asarray(nbrs[0][0])\n",
      "     |  array(2)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NearestNeighbors\n",
      "     |      sklearn.neighbors._base.KNeighborsMixin\n",
      "     |      sklearn.neighbors._base.RadiusNeighborsMixin\n",
      "     |      sklearn.neighbors._base.NeighborsBase\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_neighbors=5, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit the nearest neighbors estimator from the training dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : NearestNeighbors\n",
      "     |          The fitted nearest neighbors estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      "     |      Find the K-neighbors of a point.\n",
      "     |      \n",
      "     |      Returns indices of and distances to the neighbors of each point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors required for each sample. The default is the\n",
      "     |          value passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Array representing the lengths to points, only present if\n",
      "     |          return_distance=True.\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Indices of the nearest points in the population matrix.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NearestNeighbors\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1,1,1]\n",
      "     |      \n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
      "     |      (array([[0.5]]), array([[2]]))\n",
      "     |      \n",
      "     |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      "     |      element is at distance 0.5 and is the third element of samples\n",
      "     |      (indexes start at 0). You can also query for multiple points:\n",
      "     |      \n",
      "     |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      "     |      >>> neigh.kneighbors(X, return_distance=False)\n",
      "     |      array([[1],\n",
      "     |             [2]]...)\n",
      "     |  \n",
      "     |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      "     |      Compute the (weighted) graph of k-Neighbors for points in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |          For ``metric='precomputed'`` the shape should be\n",
      "     |          (n_queries, n_indexed). Otherwise the shape should be\n",
      "     |          (n_queries, n_features).\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors for each sample. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are distances between points, type of distance\n",
      "     |          depends on the selected metric parameter in\n",
      "     |          NearestNeighbors class.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data.\n",
      "     |          `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n",
      "     |          The matrix is of CSR format.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n",
      "     |          of Neighbors for points in X.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> A = neigh.kneighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 1.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.RadiusNeighborsMixin:\n",
      "     |  \n",
      "     |  radius_neighbors(self, X=None, radius=None, return_distance=True, sort_results=False)\n",
      "     |      Find the neighbors within a given radius of a point or points.\n",
      "     |      \n",
      "     |      Return the indices and distances of each point from the dataset\n",
      "     |      lying in a ball with size ``radius`` around the points of the query\n",
      "     |      array. Points lying on the boundary are included in the results.\n",
      "     |      \n",
      "     |      The result points are *not* necessarily sorted by distance to their\n",
      "     |      query point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of (n_samples, n_features), default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      radius : float, default=None\n",
      "     |          Limiting distance of neighbors to return. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      sort_results : bool, default=False\n",
      "     |          If True, the distances and indices will be sorted by increasing\n",
      "     |          distances before being returned. If False, the results may not\n",
      "     |          be sorted. If `return_distance=False`, setting `sort_results=True`\n",
      "     |          will result in an error.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.22\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_samples,) of arrays\n",
      "     |          Array representing the distances to each point, only present if\n",
      "     |          `return_distance=True`. The distance values are computed according\n",
      "     |          to the ``metric`` constructor parameter.\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_samples,) of arrays\n",
      "     |          An array of arrays of indices of the approximate nearest points\n",
      "     |          from the population matrix that lie within a ball of size\n",
      "     |          ``radius`` around the query points.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Because the number of neighbors of each point is not necessarily\n",
      "     |      equal, the results for multiple query points cannot be fit in a\n",
      "     |      standard data array.\n",
      "     |      For efficiency, `radius_neighbors` returns arrays of objects, where\n",
      "     |      each object is a 1D array of indices or distances.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NeighborsClassifier\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1, 1, 1]:\n",
      "     |      \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(radius=1.6)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(radius=1.6)\n",
      "     |      >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n",
      "     |      >>> print(np.asarray(rng[0][0]))\n",
      "     |      [1.5 0.5]\n",
      "     |      >>> print(np.asarray(rng[1][0]))\n",
      "     |      [1 2]\n",
      "     |      \n",
      "     |      The first array returned contains the distances to all points which\n",
      "     |      are closer than 1.6, while the second array returned contains their\n",
      "     |      indices.  In general, multiple points can be queried at the same time.\n",
      "     |  \n",
      "     |  radius_neighbors_graph(self, X=None, radius=None, mode='connectivity', sort_results=False)\n",
      "     |      Compute the (weighted) graph of Neighbors for points in X.\n",
      "     |      \n",
      "     |      Neighborhoods are restricted the points at a distance lower than\n",
      "     |      radius.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features), default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      radius : float, default=None\n",
      "     |          Radius of neighborhoods. The default is the value passed to the\n",
      "     |          constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are distances between points, type of distance\n",
      "     |          depends on the selected metric parameter in\n",
      "     |          NearestNeighbors class.\n",
      "     |      \n",
      "     |      sort_results : bool, default=False\n",
      "     |          If True, in each row of the result, the non-zero entries will be\n",
      "     |          sorted by increasing distances. If False, the non-zero entries may\n",
      "     |          not be sorted. Only used with mode='distance'.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.22\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data.\n",
      "     |          `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n",
      "     |          The matrix is of CSR format.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n",
      "     |          points in X.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(radius=1.5)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(radius=1.5)\n",
      "     |      >>> A = neigh.radius_neighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 0.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class NeighborhoodComponentsAnalysis(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  NeighborhoodComponentsAnalysis(n_components=None, *, init='auto', warm_start=False, max_iter=50, tol=1e-05, callback=None, verbose=0, random_state=None)\n",
      "     |  \n",
      "     |  Neighborhood Components Analysis.\n",
      "     |  \n",
      "     |  Neighborhood Component Analysis (NCA) is a machine learning algorithm for\n",
      "     |  metric learning. It learns a linear transformation in a supervised fashion\n",
      "     |  to improve the classification accuracy of a stochastic nearest neighbors\n",
      "     |  rule in the transformed space.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <nca>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_components : int, default=None\n",
      "     |      Preferred dimensionality of the projected space.\n",
      "     |      If None it will be set to `n_features`.\n",
      "     |  \n",
      "     |  init : {'auto', 'pca', 'lda', 'identity', 'random'} or ndarray of shape             (n_features_a, n_features_b), default='auto'\n",
      "     |      Initialization of the linear transformation. Possible options are\n",
      "     |      `'auto'`, `'pca'`, `'lda'`, `'identity'`, `'random'`, and a numpy\n",
      "     |      array of shape `(n_features_a, n_features_b)`.\n",
      "     |  \n",
      "     |      - `'auto'`\n",
      "     |          Depending on `n_components`, the most reasonable initialization\n",
      "     |          will be chosen. If `n_components <= n_classes` we use `'lda'`, as\n",
      "     |          it uses labels information. If not, but\n",
      "     |          `n_components < min(n_features, n_samples)`, we use `'pca'`, as\n",
      "     |          it projects data in meaningful directions (those of higher\n",
      "     |          variance). Otherwise, we just use `'identity'`.\n",
      "     |  \n",
      "     |      - `'pca'`\n",
      "     |          `n_components` principal components of the inputs passed\n",
      "     |          to :meth:`fit` will be used to initialize the transformation.\n",
      "     |          (See :class:`~sklearn.decomposition.PCA`)\n",
      "     |  \n",
      "     |      - `'lda'`\n",
      "     |          `min(n_components, n_classes)` most discriminative\n",
      "     |          components of the inputs passed to :meth:`fit` will be used to\n",
      "     |          initialize the transformation. (If `n_components > n_classes`,\n",
      "     |          the rest of the components will be zero.) (See\n",
      "     |          :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`)\n",
      "     |  \n",
      "     |      - `'identity'`\n",
      "     |          If `n_components` is strictly smaller than the\n",
      "     |          dimensionality of the inputs passed to :meth:`fit`, the identity\n",
      "     |          matrix will be truncated to the first `n_components` rows.\n",
      "     |  \n",
      "     |      - `'random'`\n",
      "     |          The initial transformation will be a random array of shape\n",
      "     |          `(n_components, n_features)`. Each value is sampled from the\n",
      "     |          standard normal distribution.\n",
      "     |  \n",
      "     |      - numpy array\n",
      "     |          `n_features_b` must match the dimensionality of the inputs passed\n",
      "     |          to :meth:`fit` and n_features_a must be less than or equal to that.\n",
      "     |          If `n_components` is not `None`, `n_features_a` must match it.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      If `True` and :meth:`fit` has been called before, the solution of the\n",
      "     |      previous call to :meth:`fit` is used as the initial linear\n",
      "     |      transformation (`n_components` and `init` will be ignored).\n",
      "     |  \n",
      "     |  max_iter : int, default=50\n",
      "     |      Maximum number of iterations in the optimization.\n",
      "     |  \n",
      "     |  tol : float, default=1e-5\n",
      "     |      Convergence tolerance for the optimization.\n",
      "     |  \n",
      "     |  callback : callable, default=None\n",
      "     |      If not `None`, this function is called after every iteration of the\n",
      "     |      optimizer, taking as arguments the current solution (flattened\n",
      "     |      transformation matrix) and the number of iterations. This might be\n",
      "     |      useful in case one wants to examine or store the transformation\n",
      "     |      found after each iteration.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      If 0, no progress messages will be printed.\n",
      "     |      If 1, progress messages will be printed to stdout.\n",
      "     |      If > 1, progress messages will be printed and the `disp`\n",
      "     |      parameter of :func:`scipy.optimize.minimize` will be set to\n",
      "     |      `verbose - 2`.\n",
      "     |  \n",
      "     |  random_state : int or numpy.RandomState, default=None\n",
      "     |      A pseudo random number generator object or a seed for it if int. If\n",
      "     |      `init='random'`, `random_state` is used to initialize the random\n",
      "     |      transformation. If `init='pca'`, `random_state` is passed as an\n",
      "     |      argument to PCA when initializing the transformation. Pass an int\n",
      "     |      for reproducible results across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  components_ : ndarray of shape (n_components, n_features)\n",
      "     |      The linear transformation learned during fitting.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Counts the number of iterations performed by the optimizer.\n",
      "     |  \n",
      "     |  random_state_ : numpy.RandomState\n",
      "     |      Pseudo random number generator object used during initialization.\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.discriminant_analysis.LinearDiscriminantAnalysis : Linear\n",
      "     |      Discriminant Analysis.\n",
      "     |  sklearn.decomposition.PCA : Principal component analysis (PCA).\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov.\n",
      "     |         \"Neighbourhood Components Analysis\". Advances in Neural Information\n",
      "     |         Processing Systems. 17, 513-520, 2005.\n",
      "     |         http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf\n",
      "     |  \n",
      "     |  .. [2] Wikipedia entry on Neighborhood Components Analysis\n",
      "     |         https://en.wikipedia.org/wiki/Neighbourhood_components_analysis\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
      "     |  >>> from sklearn.neighbors import KNeighborsClassifier\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> X, y = load_iris(return_X_y=True)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
      "     |  ... stratify=y, test_size=0.7, random_state=42)\n",
      "     |  >>> nca = NeighborhoodComponentsAnalysis(random_state=42)\n",
      "     |  >>> nca.fit(X_train, y_train)\n",
      "     |  NeighborhoodComponentsAnalysis(...)\n",
      "     |  >>> knn = KNeighborsClassifier(n_neighbors=3)\n",
      "     |  >>> knn.fit(X_train, y_train)\n",
      "     |  KNeighborsClassifier(...)\n",
      "     |  >>> print(knn.score(X_test, y_test))\n",
      "     |  0.933333...\n",
      "     |  >>> knn.fit(nca.transform(X_train), y_train)\n",
      "     |  KNeighborsClassifier(...)\n",
      "     |  >>> print(knn.score(nca.transform(X_test), y_test))\n",
      "     |  0.961904...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NeighborhoodComponentsAnalysis\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_components=None, *, init='auto', warm_start=False, max_iter=50, tol=1e-05, callback=None, verbose=0, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The training samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The corresponding training labels.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Apply the learned transformation to the given data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Data samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_embedded: ndarray of shape (n_samples, n_components)\n",
      "     |          The data samples transformed.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      NotFittedError\n",
      "     |          If :meth:`fit` has not been called before.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RadiusNeighborsClassifier(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "     |  RadiusNeighborsClassifier(radius=1.0, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', outlier_label=None, metric_params=None, n_jobs=None, **kwargs)\n",
      "     |  \n",
      "     |  Classifier implementing a vote among neighbors within a given radius.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <classification>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  radius : float, default=1.0\n",
      "     |      Range of parameter space to use by default for :meth:`radius_neighbors`\n",
      "     |      queries.\n",
      "     |  \n",
      "     |  weights : {'uniform', 'distance'} or callable, default='uniform'\n",
      "     |      Weight function used in prediction.  Possible values:\n",
      "     |  \n",
      "     |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      "     |        are weighted equally.\n",
      "     |      - 'distance' : weight points by the inverse of their distance.\n",
      "     |        in this case, closer neighbors of a query point will have a\n",
      "     |        greater influence than neighbors which are further away.\n",
      "     |      - [callable] : a user-defined function which accepts an\n",
      "     |        array of distances, and returns an array of the same shape\n",
      "     |        containing the weights.\n",
      "     |  \n",
      "     |      Uniform weights are used by default.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method.\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      "     |      speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree.  The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  p : int, default=2\n",
      "     |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      "     |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric : str or callable, default='minkowski'\n",
      "     |      Distance metric to use for the tree.  The default metric is\n",
      "     |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      "     |      metric. For a list of available metrics, see the documentation of\n",
      "     |      :class:`~sklearn.metrics.DistanceMetric`.\n",
      "     |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "     |      must be square during fit. X may be a :term:`sparse graph`,\n",
      "     |      in which case only \"nonzero\" elements may be considered neighbors.\n",
      "     |  \n",
      "     |  outlier_label : {manual label, 'most_frequent'}, default=None\n",
      "     |      Label for outlier samples (samples with no neighbors in given radius).\n",
      "     |  \n",
      "     |      - manual label: str or int label (should be the same type as y)\n",
      "     |        or list of manual labels if multi-output is used.\n",
      "     |      - 'most_frequent' : assign the most frequent label of y to outliers.\n",
      "     |      - None : when any outlier is detected, ValueError will be raised.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  **kwargs : dict\n",
      "     |      Additional keyword arguments passed to the constructor.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          The RadiusNeighborsClassifier class will not longer accept extra\n",
      "     |          keyword parameters in 1.2 since they are unused.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      Class labels known to the classifier.\n",
      "     |  \n",
      "     |  effective_metric_ : str or callable\n",
      "     |      The distance metric used. It will be same as the `metric` parameter\n",
      "     |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      "     |      'minkowski' and `p` parameter set to 2.\n",
      "     |  \n",
      "     |  effective_metric_params_ : dict\n",
      "     |      Additional keyword arguments for the metric function. For most metrics\n",
      "     |      will be same with `metric_params` parameter, but may also contain the\n",
      "     |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      "     |      'minkowski'.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_samples_fit_ : int\n",
      "     |      Number of samples in the fitted data.\n",
      "     |  \n",
      "     |  outlier_label_ : int or array-like of shape (n_class,)\n",
      "     |      Label which is given for outlier samples (samples with no neighbors\n",
      "     |      on given radius).\n",
      "     |  \n",
      "     |  outputs_2d_ : bool\n",
      "     |      False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n",
      "     |      otherwise True.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  KNeighborsClassifier : Classifier implementing the k-nearest neighbors\n",
      "     |      vote.\n",
      "     |  RadiusNeighborsRegressor : Regression based on neighbors within a\n",
      "     |      fixed radius.\n",
      "     |  KNeighborsRegressor : Regression based on k-nearest neighbors.\n",
      "     |  NearestNeighbors : Unsupervised learner for implementing neighbor\n",
      "     |      searches.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      "     |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> X = [[0], [1], [2], [3]]\n",
      "     |  >>> y = [0, 0, 1, 1]\n",
      "     |  >>> from sklearn.neighbors import RadiusNeighborsClassifier\n",
      "     |  >>> neigh = RadiusNeighborsClassifier(radius=1.0)\n",
      "     |  >>> neigh.fit(X, y)\n",
      "     |  RadiusNeighborsClassifier(...)\n",
      "     |  >>> print(neigh.predict([[1.5]]))\n",
      "     |  [0]\n",
      "     |  >>> print(neigh.predict_proba([[1.0]]))\n",
      "     |  [[0.66666667 0.33333333]]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RadiusNeighborsClassifier\n",
      "     |      sklearn.neighbors._base.RadiusNeighborsMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.neighbors._base.NeighborsBase\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, radius=1.0, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', outlier_label=None, metric_params=None, n_jobs=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the radius neighbors classifier from the training dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : RadiusNeighborsClassifier\n",
      "     |          The fitted radius neighbors classifier.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict the class labels for the provided data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n",
      "     |          Class labels for each data sample.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Return probability estimates for the test data X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_queries, n_classes), or a list of                 n_outputs of such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. Classes are ordered\n",
      "     |          by lexicographic order.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.RadiusNeighborsMixin:\n",
      "     |  \n",
      "     |  radius_neighbors(self, X=None, radius=None, return_distance=True, sort_results=False)\n",
      "     |      Find the neighbors within a given radius of a point or points.\n",
      "     |      \n",
      "     |      Return the indices and distances of each point from the dataset\n",
      "     |      lying in a ball with size ``radius`` around the points of the query\n",
      "     |      array. Points lying on the boundary are included in the results.\n",
      "     |      \n",
      "     |      The result points are *not* necessarily sorted by distance to their\n",
      "     |      query point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of (n_samples, n_features), default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      radius : float, default=None\n",
      "     |          Limiting distance of neighbors to return. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      sort_results : bool, default=False\n",
      "     |          If True, the distances and indices will be sorted by increasing\n",
      "     |          distances before being returned. If False, the results may not\n",
      "     |          be sorted. If `return_distance=False`, setting `sort_results=True`\n",
      "     |          will result in an error.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.22\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_samples,) of arrays\n",
      "     |          Array representing the distances to each point, only present if\n",
      "     |          `return_distance=True`. The distance values are computed according\n",
      "     |          to the ``metric`` constructor parameter.\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_samples,) of arrays\n",
      "     |          An array of arrays of indices of the approximate nearest points\n",
      "     |          from the population matrix that lie within a ball of size\n",
      "     |          ``radius`` around the query points.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Because the number of neighbors of each point is not necessarily\n",
      "     |      equal, the results for multiple query points cannot be fit in a\n",
      "     |      standard data array.\n",
      "     |      For efficiency, `radius_neighbors` returns arrays of objects, where\n",
      "     |      each object is a 1D array of indices or distances.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NeighborsClassifier\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1, 1, 1]:\n",
      "     |      \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(radius=1.6)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(radius=1.6)\n",
      "     |      >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n",
      "     |      >>> print(np.asarray(rng[0][0]))\n",
      "     |      [1.5 0.5]\n",
      "     |      >>> print(np.asarray(rng[1][0]))\n",
      "     |      [1 2]\n",
      "     |      \n",
      "     |      The first array returned contains the distances to all points which\n",
      "     |      are closer than 1.6, while the second array returned contains their\n",
      "     |      indices.  In general, multiple points can be queried at the same time.\n",
      "     |  \n",
      "     |  radius_neighbors_graph(self, X=None, radius=None, mode='connectivity', sort_results=False)\n",
      "     |      Compute the (weighted) graph of Neighbors for points in X.\n",
      "     |      \n",
      "     |      Neighborhoods are restricted the points at a distance lower than\n",
      "     |      radius.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features), default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      radius : float, default=None\n",
      "     |          Radius of neighborhoods. The default is the value passed to the\n",
      "     |          constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are distances between points, type of distance\n",
      "     |          depends on the selected metric parameter in\n",
      "     |          NearestNeighbors class.\n",
      "     |      \n",
      "     |      sort_results : bool, default=False\n",
      "     |          If True, in each row of the result, the non-zero entries will be\n",
      "     |          sorted by increasing distances. If False, the non-zero entries may\n",
      "     |          not be sorted. Only used with mode='distance'.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.22\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data.\n",
      "     |          `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n",
      "     |          The matrix is of CSR format.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n",
      "     |          points in X.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(radius=1.5)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(radius=1.5)\n",
      "     |      >>> A = neigh.radius_neighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 0.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.neighbors._base.RadiusNeighborsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RadiusNeighborsRegressor(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "     |  RadiusNeighborsRegressor(radius=1.0, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
      "     |  \n",
      "     |  Regression based on neighbors within a fixed radius.\n",
      "     |  \n",
      "     |  The target is predicted by local interpolation of the targets\n",
      "     |  associated of the nearest neighbors in the training set.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <regression>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.9\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  radius : float, default=1.0\n",
      "     |      Range of parameter space to use by default for :meth:`radius_neighbors`\n",
      "     |      queries.\n",
      "     |  \n",
      "     |  weights : {'uniform', 'distance'} or callable, default='uniform'\n",
      "     |      Weight function used in prediction.  Possible values:\n",
      "     |  \n",
      "     |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      "     |        are weighted equally.\n",
      "     |      - 'distance' : weight points by the inverse of their distance.\n",
      "     |        in this case, closer neighbors of a query point will have a\n",
      "     |        greater influence than neighbors which are further away.\n",
      "     |      - [callable] : a user-defined function which accepts an\n",
      "     |        array of distances, and returns an array of the same shape\n",
      "     |        containing the weights.\n",
      "     |  \n",
      "     |      Uniform weights are used by default.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method.\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      "     |      speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree.  The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  p : int, default=2\n",
      "     |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      "     |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric : str or callable, default='minkowski'\n",
      "     |      The distance metric to use for the tree.  The default metric is\n",
      "     |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      "     |      metric. See the documentation of :class:`DistanceMetric` for a\n",
      "     |      list of available metrics.\n",
      "     |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "     |      must be square during fit. X may be a :term:`sparse graph`,\n",
      "     |      in which case only \"nonzero\" elements may be considered neighbors.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  effective_metric_ : str or callable\n",
      "     |      The distance metric to use. It will be same as the `metric` parameter\n",
      "     |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      "     |      'minkowski' and `p` parameter set to 2.\n",
      "     |  \n",
      "     |  effective_metric_params_ : dict\n",
      "     |      Additional keyword arguments for the metric function. For most metrics\n",
      "     |      will be same with `metric_params` parameter, but may also contain the\n",
      "     |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      "     |      'minkowski'.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_samples_fit_ : int\n",
      "     |      Number of samples in the fitted data.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  NearestNeighbors : Regression based on nearest neighbors.\n",
      "     |  KNeighborsRegressor : Regression based on k-nearest neighbors.\n",
      "     |  KNeighborsClassifier : Classifier based on the k-nearest neighbors.\n",
      "     |  RadiusNeighborsClassifier : Classifier based on neighbors within a given radius.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      "     |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> X = [[0], [1], [2], [3]]\n",
      "     |  >>> y = [0, 0, 1, 1]\n",
      "     |  >>> from sklearn.neighbors import RadiusNeighborsRegressor\n",
      "     |  >>> neigh = RadiusNeighborsRegressor(radius=1.0)\n",
      "     |  >>> neigh.fit(X, y)\n",
      "     |  RadiusNeighborsRegressor(...)\n",
      "     |  >>> print(neigh.predict([[1.5]]))\n",
      "     |  [0.5]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RadiusNeighborsRegressor\n",
      "     |      sklearn.neighbors._base.RadiusNeighborsMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.neighbors._base.NeighborsBase\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, radius=1.0, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the radius neighbors regressor from the training dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : RadiusNeighborsRegressor\n",
      "     |          The fitted radius neighbors regressor.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict the target for the provided data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_queries,) or (n_queries, n_outputs),                 dtype=double\n",
      "     |          Target values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.RadiusNeighborsMixin:\n",
      "     |  \n",
      "     |  radius_neighbors(self, X=None, radius=None, return_distance=True, sort_results=False)\n",
      "     |      Find the neighbors within a given radius of a point or points.\n",
      "     |      \n",
      "     |      Return the indices and distances of each point from the dataset\n",
      "     |      lying in a ball with size ``radius`` around the points of the query\n",
      "     |      array. Points lying on the boundary are included in the results.\n",
      "     |      \n",
      "     |      The result points are *not* necessarily sorted by distance to their\n",
      "     |      query point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of (n_samples, n_features), default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      radius : float, default=None\n",
      "     |          Limiting distance of neighbors to return. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      sort_results : bool, default=False\n",
      "     |          If True, the distances and indices will be sorted by increasing\n",
      "     |          distances before being returned. If False, the results may not\n",
      "     |          be sorted. If `return_distance=False`, setting `sort_results=True`\n",
      "     |          will result in an error.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.22\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_samples,) of arrays\n",
      "     |          Array representing the distances to each point, only present if\n",
      "     |          `return_distance=True`. The distance values are computed according\n",
      "     |          to the ``metric`` constructor parameter.\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_samples,) of arrays\n",
      "     |          An array of arrays of indices of the approximate nearest points\n",
      "     |          from the population matrix that lie within a ball of size\n",
      "     |          ``radius`` around the query points.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Because the number of neighbors of each point is not necessarily\n",
      "     |      equal, the results for multiple query points cannot be fit in a\n",
      "     |      standard data array.\n",
      "     |      For efficiency, `radius_neighbors` returns arrays of objects, where\n",
      "     |      each object is a 1D array of indices or distances.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NeighborsClassifier\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1, 1, 1]:\n",
      "     |      \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(radius=1.6)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(radius=1.6)\n",
      "     |      >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n",
      "     |      >>> print(np.asarray(rng[0][0]))\n",
      "     |      [1.5 0.5]\n",
      "     |      >>> print(np.asarray(rng[1][0]))\n",
      "     |      [1 2]\n",
      "     |      \n",
      "     |      The first array returned contains the distances to all points which\n",
      "     |      are closer than 1.6, while the second array returned contains their\n",
      "     |      indices.  In general, multiple points can be queried at the same time.\n",
      "     |  \n",
      "     |  radius_neighbors_graph(self, X=None, radius=None, mode='connectivity', sort_results=False)\n",
      "     |      Compute the (weighted) graph of Neighbors for points in X.\n",
      "     |      \n",
      "     |      Neighborhoods are restricted the points at a distance lower than\n",
      "     |      radius.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features), default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      radius : float, default=None\n",
      "     |          Radius of neighborhoods. The default is the value passed to the\n",
      "     |          constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are distances between points, type of distance\n",
      "     |          depends on the selected metric parameter in\n",
      "     |          NearestNeighbors class.\n",
      "     |      \n",
      "     |      sort_results : bool, default=False\n",
      "     |          If True, in each row of the result, the non-zero entries will be\n",
      "     |          sorted by increasing distances. If False, the non-zero entries may\n",
      "     |          not be sorted. Only used with mode='distance'.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.22\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data.\n",
      "     |          `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n",
      "     |          The matrix is of CSR format.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n",
      "     |          points in X.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(radius=1.5)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(radius=1.5)\n",
      "     |      >>> A = neigh.radius_neighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 0.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.neighbors._base.RadiusNeighborsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RadiusNeighborsTransformer(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.TransformerMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "     |  RadiusNeighborsTransformer(*, mode='distance', radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=1)\n",
      "     |  \n",
      "     |  Transform X into a (weighted) graph of neighbors nearer than a radius.\n",
      "     |  \n",
      "     |  The transformed data is a sparse graph as returned by\n",
      "     |  `radius_neighbors_graph`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <neighbors_transformer>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  mode : {'distance', 'connectivity'}, default='distance'\n",
      "     |      Type of returned matrix: 'connectivity' will return the connectivity\n",
      "     |      matrix with ones and zeros, and 'distance' will return the distances\n",
      "     |      between neighbors according to the given metric.\n",
      "     |  \n",
      "     |  radius : float, default=1.0\n",
      "     |      Radius of neighborhood in the transformed sparse graph.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method.\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      "     |      speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree.  The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  metric : str or callable, default='minkowski'\n",
      "     |      Metric to use for distance computation. Any metric from scikit-learn\n",
      "     |      or scipy.spatial.distance can be used.\n",
      "     |  \n",
      "     |      If metric is a callable function, it is called on each\n",
      "     |      pair of instances (rows) and the resulting value recorded. The callable\n",
      "     |      should take two arrays as input and return one value indicating the\n",
      "     |      distance between them. This works for Scipy's metrics, but is less\n",
      "     |      efficient than passing the metric name as a string.\n",
      "     |  \n",
      "     |      Distance matrices are not supported.\n",
      "     |  \n",
      "     |      Valid values for metric are:\n",
      "     |  \n",
      "     |      - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "     |        'manhattan']\n",
      "     |  \n",
      "     |      - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "     |        'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "     |        'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "     |        'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "     |        'yule']\n",
      "     |  \n",
      "     |      See the documentation for scipy.spatial.distance for details on these\n",
      "     |      metrics.\n",
      "     |  \n",
      "     |  p : int, default=2\n",
      "     |      Parameter for the Minkowski metric from\n",
      "     |      sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n",
      "     |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  n_jobs : int, default=1\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  effective_metric_ : str or callable\n",
      "     |      The distance metric used. It will be same as the `metric` parameter\n",
      "     |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      "     |      'minkowski' and `p` parameter set to 2.\n",
      "     |  \n",
      "     |  effective_metric_params_ : dict\n",
      "     |      Additional keyword arguments for the metric function. For most metrics\n",
      "     |      will be same with `metric_params` parameter, but may also contain the\n",
      "     |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      "     |      'minkowski'.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_samples_fit_ : int\n",
      "     |      Number of samples in the fitted data.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  kneighbors_graph : Compute the weighted graph of k-neighbors for\n",
      "     |      points in X.\n",
      "     |  KNeighborsTransformer : Transform X into a weighted graph of k\n",
      "     |      nearest neighbors.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.datasets import load_wine\n",
      "     |  >>> from sklearn.cluster import DBSCAN\n",
      "     |  >>> from sklearn.neighbors import RadiusNeighborsTransformer\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> X, _ = load_wine(return_X_y=True)\n",
      "     |  >>> estimator = make_pipeline(\n",
      "     |  ...     RadiusNeighborsTransformer(radius=42.0, mode='distance'),\n",
      "     |  ...     DBSCAN(eps=25.0, metric='precomputed'))\n",
      "     |  >>> X_clustered = estimator.fit_predict(X)\n",
      "     |  >>> clusters, counts = np.unique(X_clustered, return_counts=True)\n",
      "     |  >>> print(counts)\n",
      "     |  [ 29  15 111  11  12]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RadiusNeighborsTransformer\n",
      "     |      sklearn.neighbors._base.RadiusNeighborsMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.neighbors._base.NeighborsBase\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, mode='distance', radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit the radius neighbors transformer from the training dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : RadiusNeighborsTransformer\n",
      "     |          The fitted radius neighbors transformer.\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : sparse matrix of shape (n_samples, n_samples)\n",
      "     |          Xt[i, j] is assigned the weight of edge that connects i to j.\n",
      "     |          Only the neighbors have an explicit value.\n",
      "     |          The diagonal is always explicit.\n",
      "     |          The matrix is of CSR format.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Compute the (weighted) graph of Neighbors for points in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples_transform, n_features)\n",
      "     |          Sample data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : sparse matrix of shape (n_samples_transform, n_samples_fit)\n",
      "     |          Xt[i, j] is assigned the weight of edge that connects i to j.\n",
      "     |          Only the neighbors have an explicit value.\n",
      "     |          The diagonal is always explicit.\n",
      "     |          The matrix is of CSR format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.RadiusNeighborsMixin:\n",
      "     |  \n",
      "     |  radius_neighbors(self, X=None, radius=None, return_distance=True, sort_results=False)\n",
      "     |      Find the neighbors within a given radius of a point or points.\n",
      "     |      \n",
      "     |      Return the indices and distances of each point from the dataset\n",
      "     |      lying in a ball with size ``radius`` around the points of the query\n",
      "     |      array. Points lying on the boundary are included in the results.\n",
      "     |      \n",
      "     |      The result points are *not* necessarily sorted by distance to their\n",
      "     |      query point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of (n_samples, n_features), default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      radius : float, default=None\n",
      "     |          Limiting distance of neighbors to return. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      sort_results : bool, default=False\n",
      "     |          If True, the distances and indices will be sorted by increasing\n",
      "     |          distances before being returned. If False, the results may not\n",
      "     |          be sorted. If `return_distance=False`, setting `sort_results=True`\n",
      "     |          will result in an error.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.22\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_samples,) of arrays\n",
      "     |          Array representing the distances to each point, only present if\n",
      "     |          `return_distance=True`. The distance values are computed according\n",
      "     |          to the ``metric`` constructor parameter.\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_samples,) of arrays\n",
      "     |          An array of arrays of indices of the approximate nearest points\n",
      "     |          from the population matrix that lie within a ball of size\n",
      "     |          ``radius`` around the query points.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Because the number of neighbors of each point is not necessarily\n",
      "     |      equal, the results for multiple query points cannot be fit in a\n",
      "     |      standard data array.\n",
      "     |      For efficiency, `radius_neighbors` returns arrays of objects, where\n",
      "     |      each object is a 1D array of indices or distances.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NeighborsClassifier\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1, 1, 1]:\n",
      "     |      \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(radius=1.6)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(radius=1.6)\n",
      "     |      >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n",
      "     |      >>> print(np.asarray(rng[0][0]))\n",
      "     |      [1.5 0.5]\n",
      "     |      >>> print(np.asarray(rng[1][0]))\n",
      "     |      [1 2]\n",
      "     |      \n",
      "     |      The first array returned contains the distances to all points which\n",
      "     |      are closer than 1.6, while the second array returned contains their\n",
      "     |      indices.  In general, multiple points can be queried at the same time.\n",
      "     |  \n",
      "     |  radius_neighbors_graph(self, X=None, radius=None, mode='connectivity', sort_results=False)\n",
      "     |      Compute the (weighted) graph of Neighbors for points in X.\n",
      "     |      \n",
      "     |      Neighborhoods are restricted the points at a distance lower than\n",
      "     |      radius.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features), default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      radius : float, default=None\n",
      "     |          Radius of neighborhoods. The default is the value passed to the\n",
      "     |          constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are distances between points, type of distance\n",
      "     |          depends on the selected metric parameter in\n",
      "     |          NearestNeighbors class.\n",
      "     |      \n",
      "     |      sort_results : bool, default=False\n",
      "     |          If True, in each row of the result, the non-zero entries will be\n",
      "     |          sorted by increasing distances. If False, the non-zero entries may\n",
      "     |          not be sorted. Only used with mode='distance'.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.22\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data.\n",
      "     |          `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n",
      "     |          The matrix is of CSR format.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n",
      "     |          points in X.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(radius=1.5)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(radius=1.5)\n",
      "     |      >>> A = neigh.radius_neighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 0.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.neighbors._base.RadiusNeighborsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "\n",
      "FUNCTIONS\n",
      "    kneighbors_graph(X, n_neighbors, *, mode='connectivity', metric='minkowski', p=2, metric_params=None, include_self=False, n_jobs=None)\n",
      "        Computes the (weighted) graph of k-Neighbors for points in X\n",
      "        \n",
      "        Read more in the :ref:`User Guide <unsupervised_neighbors>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features) or BallTree\n",
      "            Sample data, in the form of a numpy array or a precomputed\n",
      "            :class:`BallTree`.\n",
      "        \n",
      "        n_neighbors : int\n",
      "            Number of neighbors for each sample.\n",
      "        \n",
      "        mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "            Type of returned matrix: 'connectivity' will return the connectivity\n",
      "            matrix with ones and zeros, and 'distance' will return the distances\n",
      "            between neighbors according to the given metric.\n",
      "        \n",
      "        metric : str, default='minkowski'\n",
      "            The distance metric to use for the tree. The default metric is\n",
      "            minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      "            metric.\n",
      "            For a list of available metrics, see the documentation of\n",
      "            :class:`~sklearn.metrics.DistanceMetric`.\n",
      "        \n",
      "        p : int, default=2\n",
      "            Power parameter for the Minkowski metric. When p = 1, this is\n",
      "            equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "            (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "        \n",
      "        metric_params : dict, default=None\n",
      "            additional keyword arguments for the metric function.\n",
      "        \n",
      "        include_self : bool or 'auto', default=False\n",
      "            Whether or not to mark each sample as the first nearest neighbor to\n",
      "            itself. If 'auto', then True is used for mode='connectivity' and False\n",
      "            for mode='distance'.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of parallel jobs to run for neighbors search.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        A : sparse matrix of shape (n_samples, n_samples)\n",
      "            Graph where A[i, j] is assigned the weight of edge that\n",
      "            connects i to j. The matrix is of CSR format.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> X = [[0], [3], [1]]\n",
      "        >>> from sklearn.neighbors import kneighbors_graph\n",
      "        >>> A = kneighbors_graph(X, 2, mode='connectivity', include_self=True)\n",
      "        >>> A.toarray()\n",
      "        array([[1., 0., 1.],\n",
      "               [0., 1., 1.],\n",
      "               [1., 0., 1.]])\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        radius_neighbors_graph\n",
      "    \n",
      "    radius_neighbors_graph(X, radius, *, mode='connectivity', metric='minkowski', p=2, metric_params=None, include_self=False, n_jobs=None)\n",
      "        Computes the (weighted) graph of Neighbors for points in X\n",
      "        \n",
      "        Neighborhoods are restricted the points at a distance lower than\n",
      "        radius.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <unsupervised_neighbors>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features) or BallTree\n",
      "            Sample data, in the form of a numpy array or a precomputed\n",
      "            :class:`BallTree`.\n",
      "        \n",
      "        radius : float\n",
      "            Radius of neighborhoods.\n",
      "        \n",
      "        mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "            Type of returned matrix: 'connectivity' will return the connectivity\n",
      "            matrix with ones and zeros, and 'distance' will return the distances\n",
      "            between neighbors according to the given metric.\n",
      "        \n",
      "        metric : str, default='minkowski'\n",
      "            The distance metric to use for the tree. The default metric is\n",
      "            minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      "            metric.\n",
      "            For a list of available metrics, see the documentation of\n",
      "            :class:`~sklearn.metrics.DistanceMetric`.\n",
      "        \n",
      "        p : int, default=2\n",
      "            Power parameter for the Minkowski metric. When p = 1, this is\n",
      "            equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "            (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "        \n",
      "        metric_params : dict, default=None\n",
      "            additional keyword arguments for the metric function.\n",
      "        \n",
      "        include_self : bool or 'auto', default=False\n",
      "            Whether or not to mark each sample as the first nearest neighbor to\n",
      "            itself. If 'auto', then True is used for mode='connectivity' and False\n",
      "            for mode='distance'.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of parallel jobs to run for neighbors search.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        A : sparse matrix of shape (n_samples, n_samples)\n",
      "            Graph where A[i, j] is assigned the weight of edge that connects\n",
      "            i to j. The matrix is of CSR format.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> X = [[0], [3], [1]]\n",
      "        >>> from sklearn.neighbors import radius_neighbors_graph\n",
      "        >>> A = radius_neighbors_graph(X, 1.5, mode='connectivity',\n",
      "        ...                            include_self=True)\n",
      "        >>> A.toarray()\n",
      "        array([[1., 0., 1.],\n",
      "               [0., 1., 0.],\n",
      "               [1., 0., 1.]])\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kneighbors_graph\n",
      "\n",
      "DATA\n",
      "    VALID_METRICS = {'ball_tree': ['euclidean', 'l2', 'minkowski', 'p', 'm...\n",
      "    VALID_METRICS_SPARSE = {'ball_tree': [], 'brute': {'cityblock', 'cosin...\n",
      "    __all__ = ['BallTree', 'DistanceMetric', 'KDTree', 'KNeighborsClassifi...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1ef4c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.tree in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.tree\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.tree` module includes decision tree-based models for\n",
      "    classification and regression.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _classes\n",
      "    _criterion\n",
      "    _export\n",
      "    _reingold_tilford\n",
      "    _splitter\n",
      "    _tree\n",
      "    _utils\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        sklearn.tree._classes.BaseDecisionTree(sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.tree._classes.DecisionTreeClassifier(sklearn.base.ClassifierMixin, sklearn.tree._classes.BaseDecisionTree)\n",
      "                sklearn.tree._classes.ExtraTreeClassifier\n",
      "            sklearn.tree._classes.DecisionTreeRegressor(sklearn.base.RegressorMixin, sklearn.tree._classes.BaseDecisionTree)\n",
      "                sklearn.tree._classes.ExtraTreeRegressor\n",
      "    sklearn.base.ClassifierMixin(builtins.object)\n",
      "        sklearn.tree._classes.DecisionTreeClassifier(sklearn.base.ClassifierMixin, sklearn.tree._classes.BaseDecisionTree)\n",
      "            sklearn.tree._classes.ExtraTreeClassifier\n",
      "    sklearn.base.MultiOutputMixin(builtins.object)\n",
      "        sklearn.tree._classes.BaseDecisionTree(sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.tree._classes.DecisionTreeClassifier(sklearn.base.ClassifierMixin, sklearn.tree._classes.BaseDecisionTree)\n",
      "                sklearn.tree._classes.ExtraTreeClassifier\n",
      "            sklearn.tree._classes.DecisionTreeRegressor(sklearn.base.RegressorMixin, sklearn.tree._classes.BaseDecisionTree)\n",
      "                sklearn.tree._classes.ExtraTreeRegressor\n",
      "    sklearn.base.RegressorMixin(builtins.object)\n",
      "        sklearn.tree._classes.DecisionTreeRegressor(sklearn.base.RegressorMixin, sklearn.tree._classes.BaseDecisionTree)\n",
      "            sklearn.tree._classes.ExtraTreeRegressor\n",
      "    \n",
      "    class BaseDecisionTree(sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "     |  BaseDecisionTree(*, criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, random_state, min_impurity_decrease, class_weight=None, ccp_alpha=0.0)\n",
      "     |  \n",
      "     |  Base class for decision trees.\n",
      "     |  \n",
      "     |  Warning: This class should not be used directly.\n",
      "     |  Use derived classes instead.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BaseDecisionTree\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, random_state, min_impurity_decrease, class_weight=None, ccp_alpha=0.0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  apply(self, X, check_input=True)\n",
      "     |      Return the index of the leaf that each sample is predicted as.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array-like of shape (n_samples,)\n",
      "     |          For each datapoint x in X, return the index of the leaf x\n",
      "     |          ends up in. Leaves are numbered within\n",
      "     |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      "     |          numbering.\n",
      "     |  \n",
      "     |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
      "     |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
      "     |      \n",
      "     |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
      "     |      process.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (class labels) as integers or strings.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. Splits are also\n",
      "     |          ignored if they would result in any single class carrying a\n",
      "     |          negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ccp_path : :class:`~sklearn.utils.Bunch`\n",
      "     |          Dictionary-like object, with the following attributes.\n",
      "     |      \n",
      "     |          ccp_alphas : ndarray\n",
      "     |              Effective alphas of subtree during pruning.\n",
      "     |      \n",
      "     |          impurities : ndarray\n",
      "     |              Sum of the impurities of the subtree leaves for the\n",
      "     |              corresponding alpha value in ``ccp_alphas``.\n",
      "     |  \n",
      "     |  decision_path(self, X, check_input=True)\n",
      "     |      Return the decision path in the tree.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      "     |          Return a node indicator CSR matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted='deprecated')\n",
      "     |  \n",
      "     |  get_depth(self)\n",
      "     |      Return the depth of the decision tree.\n",
      "     |      \n",
      "     |      The depth of a tree is the maximum distance between the root\n",
      "     |      and any leaf.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self.tree_.max_depth : int\n",
      "     |          The maximum depth of the tree.\n",
      "     |  \n",
      "     |  get_n_leaves(self)\n",
      "     |      Return the number of leaves of the decision tree.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self.tree_.n_leaves : int\n",
      "     |          Number of leaves.\n",
      "     |  \n",
      "     |  predict(self, X, check_input=True)\n",
      "     |      Predict class or regression value for X.\n",
      "     |      \n",
      "     |      For a classification model, the predicted class for each sample in X is\n",
      "     |      returned. For a regression model, the predicted value based on X is\n",
      "     |      returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The predicted classes, or the predict values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances.\n",
      "     |      \n",
      "     |      The importance of a feature is computed as the (normalized) total\n",
      "     |      reduction of the criterion brought by that feature.\n",
      "     |      It is also known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          Normalized total reduction of criteria by feature\n",
      "     |          (Gini importance).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__init__'})\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class DecisionTreeClassifier(sklearn.base.ClassifierMixin, BaseDecisionTree)\n",
      "     |  DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
      "     |  \n",
      "     |  A decision tree classifier.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <tree>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      "     |      The function to measure the quality of a split. Supported criteria are\n",
      "     |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      "     |  \n",
      "     |  splitter : {\"best\", \"random\"}, default=\"best\"\n",
      "     |      The strategy used to choose the split at each node. Supported\n",
      "     |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      "     |      the best random split.\n",
      "     |  \n",
      "     |  max_depth : int, default=None\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |          - If int, then consider `max_features` features at each split.\n",
      "     |          - If float, then `max_features` is a fraction and\n",
      "     |            `int(max_features * n_features)` features are considered at each\n",
      "     |            split.\n",
      "     |          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      "     |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |          - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |          - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the randomness of the estimator. The features are always\n",
      "     |      randomly permuted at each split, even if ``splitter`` is set to\n",
      "     |      ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      "     |      select ``max_features`` at random at each split before finding the best\n",
      "     |      split among them. But the best found split may vary across different\n",
      "     |      runs, even if ``max_features=n_features``. That is the case, if the\n",
      "     |      improvement of the criterion is identical for several splits and one\n",
      "     |      split has to be selected at random. To obtain a deterministic behaviour\n",
      "     |      during fitting, ``random_state`` has to be fixed to an integer.\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  class_weight : dict, list of dict or \"balanced\", default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If None, all classes are supposed to have weight one. For\n",
      "     |      multi-output problems, a list of dicts can be provided in the same\n",
      "     |      order as the columns of y.\n",
      "     |  \n",
      "     |      Note that for multioutput (including multilabel) weights should be\n",
      "     |      defined for each class of every column in its own dict. For example,\n",
      "     |      for four-class multilabel classification weights should be\n",
      "     |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      "     |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |      For multi-output, the weights of each column of y will be multiplied.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |  ccp_alpha : non-negative float, default=0.0\n",
      "     |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "     |      subtree with the largest cost complexity that is smaller than\n",
      "     |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "     |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
      "     |      The classes labels (single output problem),\n",
      "     |      or a list of arrays of class labels (multi-output problem).\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances.\n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance [4]_.\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  max_features_ : int\n",
      "     |      The inferred value of max_features.\n",
      "     |  \n",
      "     |  n_classes_ : int or list of int\n",
      "     |      The number of classes (for single output problems),\n",
      "     |      or a list containing the number of classes for each\n",
      "     |      output (for multi-output problems).\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when ``fit`` is performed.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |         `n_features_` is deprecated in 1.0 and will be removed in\n",
      "     |         1.2. Use `n_features_in_` instead.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  tree_ : Tree instance\n",
      "     |      The underlying Tree object. Please refer to\n",
      "     |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      "     |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      "     |      for basic usage of these attributes.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  DecisionTreeRegressor : A decision tree regressor.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  The :meth:`predict` method operates using the :func:`numpy.argmax`\n",
      "     |  function on the outputs of :meth:`predict_proba`. This means that in\n",
      "     |  case the highest predicted probabilities are tied, the classifier will\n",
      "     |  predict the tied class with the lowest index in :term:`classes_`.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      "     |  \n",
      "     |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      "     |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      "     |  \n",
      "     |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      "     |         Learning\", Springer, 2009.\n",
      "     |  \n",
      "     |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      "     |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.model_selection import cross_val_score\n",
      "     |  >>> from sklearn.tree import DecisionTreeClassifier\n",
      "     |  >>> clf = DecisionTreeClassifier(random_state=0)\n",
      "     |  >>> iris = load_iris()\n",
      "     |  >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
      "     |  ...                             # doctest: +SKIP\n",
      "     |  ...\n",
      "     |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
      "     |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DecisionTreeClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      BaseDecisionTree\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted='deprecated')\n",
      "     |      Build a decision tree classifier from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (class labels) as integers or strings.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. Splits are also\n",
      "     |          ignored if they would result in any single class carrying a\n",
      "     |          negative weight in either child node.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      X_idx_sorted : deprecated, default=\"deprecated\"\n",
      "     |          This parameter is deprecated and has no effect.\n",
      "     |          It will be removed in 1.1 (renaming of 0.26).\n",
      "     |      \n",
      "     |          .. deprecated:: 0.24\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : DecisionTreeClassifier\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities of the input samples X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      "     |          The class log-probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X, check_input=True)\n",
      "     |      Predict class probabilities of the input samples X.\n",
      "     |      \n",
      "     |      The predicted class probability is the fraction of samples of the same\n",
      "     |      class in a leaf.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  n_features_\n",
      "     |      DEPRECATED: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseDecisionTree:\n",
      "     |  \n",
      "     |  apply(self, X, check_input=True)\n",
      "     |      Return the index of the leaf that each sample is predicted as.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array-like of shape (n_samples,)\n",
      "     |          For each datapoint x in X, return the index of the leaf x\n",
      "     |          ends up in. Leaves are numbered within\n",
      "     |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      "     |          numbering.\n",
      "     |  \n",
      "     |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
      "     |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
      "     |      \n",
      "     |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
      "     |      process.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (class labels) as integers or strings.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. Splits are also\n",
      "     |          ignored if they would result in any single class carrying a\n",
      "     |          negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ccp_path : :class:`~sklearn.utils.Bunch`\n",
      "     |          Dictionary-like object, with the following attributes.\n",
      "     |      \n",
      "     |          ccp_alphas : ndarray\n",
      "     |              Effective alphas of subtree during pruning.\n",
      "     |      \n",
      "     |          impurities : ndarray\n",
      "     |              Sum of the impurities of the subtree leaves for the\n",
      "     |              corresponding alpha value in ``ccp_alphas``.\n",
      "     |  \n",
      "     |  decision_path(self, X, check_input=True)\n",
      "     |      Return the decision path in the tree.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      "     |          Return a node indicator CSR matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |  \n",
      "     |  get_depth(self)\n",
      "     |      Return the depth of the decision tree.\n",
      "     |      \n",
      "     |      The depth of a tree is the maximum distance between the root\n",
      "     |      and any leaf.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self.tree_.max_depth : int\n",
      "     |          The maximum depth of the tree.\n",
      "     |  \n",
      "     |  get_n_leaves(self)\n",
      "     |      Return the number of leaves of the decision tree.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self.tree_.n_leaves : int\n",
      "     |          Number of leaves.\n",
      "     |  \n",
      "     |  predict(self, X, check_input=True)\n",
      "     |      Predict class or regression value for X.\n",
      "     |      \n",
      "     |      For a classification model, the predicted class for each sample in X is\n",
      "     |      returned. For a regression model, the predicted value based on X is\n",
      "     |      returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The predicted classes, or the predict values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseDecisionTree:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances.\n",
      "     |      \n",
      "     |      The importance of a feature is computed as the (normalized) total\n",
      "     |      reduction of the criterion brought by that feature.\n",
      "     |      It is also known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          Normalized total reduction of criteria by feature\n",
      "     |          (Gini importance).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class DecisionTreeRegressor(sklearn.base.RegressorMixin, BaseDecisionTree)\n",
      "     |  DecisionTreeRegressor(*, criterion='squared_error', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, ccp_alpha=0.0)\n",
      "     |  \n",
      "     |  A decision tree regressor.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <tree>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  criterion : {\"squared_error\", \"friedman_mse\", \"absolute_error\",             \"poisson\"}, default=\"squared_error\"\n",
      "     |      The function to measure the quality of a split. Supported criteria\n",
      "     |      are \"squared_error\" for the mean squared error, which is equal to\n",
      "     |      variance reduction as feature selection criterion and minimizes the L2\n",
      "     |      loss using the mean of each terminal node, \"friedman_mse\", which uses\n",
      "     |      mean squared error with Friedman's improvement score for potential\n",
      "     |      splits, \"absolute_error\" for the mean absolute error, which minimizes\n",
      "     |      the L1 loss using the median of each terminal node, and \"poisson\" which\n",
      "     |      uses reduction in Poisson deviance to find splits.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Mean Absolute Error (MAE) criterion.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |          Poisson deviance criterion.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          Criterion \"mse\" was deprecated in v1.0 and will be removed in\n",
      "     |          version 1.2. Use `criterion=\"squared_error\"` which is equivalent.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          Criterion \"mae\" was deprecated in v1.0 and will be removed in\n",
      "     |          version 1.2. Use `criterion=\"absolute_error\"` which is equivalent.\n",
      "     |  \n",
      "     |  splitter : {\"best\", \"random\"}, default=\"best\"\n",
      "     |      The strategy used to choose the split at each node. Supported\n",
      "     |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      "     |      the best random split.\n",
      "     |  \n",
      "     |  max_depth : int, default=None\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `int(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=n_features`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the randomness of the estimator. The features are always\n",
      "     |      randomly permuted at each split, even if ``splitter`` is set to\n",
      "     |      ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      "     |      select ``max_features`` at random at each split before finding the best\n",
      "     |      split among them. But the best found split may vary across different\n",
      "     |      runs, even if ``max_features=n_features``. That is the case, if the\n",
      "     |      improvement of the criterion is identical for several splits and one\n",
      "     |      split has to be selected at random. To obtain a deterministic behaviour\n",
      "     |      during fitting, ``random_state`` has to be fixed to an integer.\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  ccp_alpha : non-negative float, default=0.0\n",
      "     |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "     |      subtree with the largest cost complexity that is smaller than\n",
      "     |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "     |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The feature importances.\n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the\n",
      "     |      (normalized) total reduction of the criterion brought\n",
      "     |      by that feature. It is also known as the Gini importance [4]_.\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  max_features_ : int\n",
      "     |      The inferred value of max_features.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when ``fit`` is performed.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |         `n_features_` is deprecated in 1.0 and will be removed in\n",
      "     |         1.2. Use `n_features_in_` instead.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  tree_ : Tree instance\n",
      "     |      The underlying Tree object. Please refer to\n",
      "     |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      "     |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      "     |      for basic usage of these attributes.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  DecisionTreeClassifier : A decision tree classifier.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      "     |  \n",
      "     |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      "     |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      "     |  \n",
      "     |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      "     |         Learning\", Springer, 2009.\n",
      "     |  \n",
      "     |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      "     |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_diabetes\n",
      "     |  >>> from sklearn.model_selection import cross_val_score\n",
      "     |  >>> from sklearn.tree import DecisionTreeRegressor\n",
      "     |  >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |  >>> regressor = DecisionTreeRegressor(random_state=0)\n",
      "     |  >>> cross_val_score(regressor, X, y, cv=10)\n",
      "     |  ...                    # doctest: +SKIP\n",
      "     |  ...\n",
      "     |  array([-0.39..., -0.46...,  0.02...,  0.06..., -0.50...,\n",
      "     |         0.16...,  0.11..., -0.73..., -0.30..., -0.00...])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DecisionTreeRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      BaseDecisionTree\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, criterion='squared_error', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, ccp_alpha=0.0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted='deprecated')\n",
      "     |      Build a decision tree regressor from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (real numbers). Use ``dtype=np.float64`` and\n",
      "     |          ``order='C'`` for maximum efficiency.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      X_idx_sorted : deprecated, default=\"deprecated\"\n",
      "     |          This parameter is deprecated and has no effect.\n",
      "     |          It will be removed in 1.1 (renaming of 0.26).\n",
      "     |      \n",
      "     |          .. deprecated:: 0.24\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : DecisionTreeRegressor\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  n_features_\n",
      "     |      DEPRECATED: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseDecisionTree:\n",
      "     |  \n",
      "     |  apply(self, X, check_input=True)\n",
      "     |      Return the index of the leaf that each sample is predicted as.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array-like of shape (n_samples,)\n",
      "     |          For each datapoint x in X, return the index of the leaf x\n",
      "     |          ends up in. Leaves are numbered within\n",
      "     |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      "     |          numbering.\n",
      "     |  \n",
      "     |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
      "     |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
      "     |      \n",
      "     |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
      "     |      process.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (class labels) as integers or strings.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. Splits are also\n",
      "     |          ignored if they would result in any single class carrying a\n",
      "     |          negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ccp_path : :class:`~sklearn.utils.Bunch`\n",
      "     |          Dictionary-like object, with the following attributes.\n",
      "     |      \n",
      "     |          ccp_alphas : ndarray\n",
      "     |              Effective alphas of subtree during pruning.\n",
      "     |      \n",
      "     |          impurities : ndarray\n",
      "     |              Sum of the impurities of the subtree leaves for the\n",
      "     |              corresponding alpha value in ``ccp_alphas``.\n",
      "     |  \n",
      "     |  decision_path(self, X, check_input=True)\n",
      "     |      Return the decision path in the tree.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      "     |          Return a node indicator CSR matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |  \n",
      "     |  get_depth(self)\n",
      "     |      Return the depth of the decision tree.\n",
      "     |      \n",
      "     |      The depth of a tree is the maximum distance between the root\n",
      "     |      and any leaf.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self.tree_.max_depth : int\n",
      "     |          The maximum depth of the tree.\n",
      "     |  \n",
      "     |  get_n_leaves(self)\n",
      "     |      Return the number of leaves of the decision tree.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self.tree_.n_leaves : int\n",
      "     |          Number of leaves.\n",
      "     |  \n",
      "     |  predict(self, X, check_input=True)\n",
      "     |      Predict class or regression value for X.\n",
      "     |      \n",
      "     |      For a classification model, the predicted class for each sample in X is\n",
      "     |      returned. For a regression model, the predicted value based on X is\n",
      "     |      returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The predicted classes, or the predict values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseDecisionTree:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances.\n",
      "     |      \n",
      "     |      The importance of a feature is computed as the (normalized) total\n",
      "     |      reduction of the criterion brought by that feature.\n",
      "     |      It is also known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          Normalized total reduction of criteria by feature\n",
      "     |          (Gini importance).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class ExtraTreeClassifier(DecisionTreeClassifier)\n",
      "     |  ExtraTreeClassifier(*, criterion='gini', splitter='random', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
      "     |  \n",
      "     |  An extremely randomized tree classifier.\n",
      "     |  \n",
      "     |  Extra-trees differ from classic decision trees in the way they are built.\n",
      "     |  When looking for the best split to separate the samples of a node into two\n",
      "     |  groups, random splits are drawn for each of the `max_features` randomly\n",
      "     |  selected features and the best split among those is chosen. When\n",
      "     |  `max_features` is set 1, this amounts to building a totally random\n",
      "     |  decision tree.\n",
      "     |  \n",
      "     |  Warning: Extra-trees should only be used within ensemble methods.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <tree>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      "     |      The function to measure the quality of a split. Supported criteria are\n",
      "     |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      "     |  \n",
      "     |  splitter : {\"random\", \"best\"}, default=\"random\"\n",
      "     |      The strategy used to choose the split at each node. Supported\n",
      "     |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      "     |      the best random split.\n",
      "     |  \n",
      "     |  max_depth : int, default=None\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_features : int, float, {\"auto\", \"sqrt\", \"log2\"} or None, default=\"auto\"\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |          - If int, then consider `max_features` features at each split.\n",
      "     |          - If float, then `max_features` is a fraction and\n",
      "     |            `int(max_features * n_features)` features are considered at each\n",
      "     |            split.\n",
      "     |          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      "     |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |          - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |          - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Used to pick randomly the `max_features` used at each split.\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  class_weight : dict, list of dict or \"balanced\", default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If None, all classes are supposed to have weight one. For\n",
      "     |      multi-output problems, a list of dicts can be provided in the same\n",
      "     |      order as the columns of y.\n",
      "     |  \n",
      "     |      Note that for multioutput (including multilabel) weights should be\n",
      "     |      defined for each class of every column in its own dict. For example,\n",
      "     |      for four-class multilabel classification weights should be\n",
      "     |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      "     |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |      For multi-output, the weights of each column of y will be multiplied.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |  ccp_alpha : non-negative float, default=0.0\n",
      "     |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "     |      subtree with the largest cost complexity that is smaller than\n",
      "     |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "     |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
      "     |      The classes labels (single output problem),\n",
      "     |      or a list of arrays of class labels (multi-output problem).\n",
      "     |  \n",
      "     |  max_features_ : int\n",
      "     |      The inferred value of max_features.\n",
      "     |  \n",
      "     |  n_classes_ : int or list of int\n",
      "     |      The number of classes (for single output problems),\n",
      "     |      or a list containing the number of classes for each\n",
      "     |      output (for multi-output problems).\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances.\n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when ``fit`` is performed.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |         `n_features_` is deprecated in 1.0 and will be removed in\n",
      "     |         1.2. Use `n_features_in_` instead.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  tree_ : Tree instance\n",
      "     |      The underlying Tree object. Please refer to\n",
      "     |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      "     |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      "     |      for basic usage of these attributes.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  ExtraTreeRegressor : An extremely randomized tree regressor.\n",
      "     |  sklearn.ensemble.ExtraTreesClassifier : An extra-trees classifier.\n",
      "     |  sklearn.ensemble.ExtraTreesRegressor : An extra-trees regressor.\n",
      "     |  sklearn.ensemble.RandomForestClassifier : A random forest classifier.\n",
      "     |  sklearn.ensemble.RandomForestRegressor : A random forest regressor.\n",
      "     |  sklearn.ensemble.RandomTreesEmbedding : An ensemble of\n",
      "     |      totally random trees.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
      "     |         Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.ensemble import BaggingClassifier\n",
      "     |  >>> from sklearn.tree import ExtraTreeClassifier\n",
      "     |  >>> X, y = load_iris(return_X_y=True)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |  ...    X, y, random_state=0)\n",
      "     |  >>> extra_tree = ExtraTreeClassifier(random_state=0)\n",
      "     |  >>> cls = BaggingClassifier(extra_tree, random_state=0).fit(\n",
      "     |  ...    X_train, y_train)\n",
      "     |  >>> cls.score(X_test, y_test)\n",
      "     |  0.8947...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExtraTreeClassifier\n",
      "     |      DecisionTreeClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      BaseDecisionTree\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, criterion='gini', splitter='random', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DecisionTreeClassifier:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted='deprecated')\n",
      "     |      Build a decision tree classifier from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (class labels) as integers or strings.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. Splits are also\n",
      "     |          ignored if they would result in any single class carrying a\n",
      "     |          negative weight in either child node.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      X_idx_sorted : deprecated, default=\"deprecated\"\n",
      "     |          This parameter is deprecated and has no effect.\n",
      "     |          It will be removed in 1.1 (renaming of 0.26).\n",
      "     |      \n",
      "     |          .. deprecated:: 0.24\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : DecisionTreeClassifier\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities of the input samples X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      "     |          The class log-probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X, check_input=True)\n",
      "     |      Predict class probabilities of the input samples X.\n",
      "     |      \n",
      "     |      The predicted class probability is the fraction of samples of the same\n",
      "     |      class in a leaf.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from DecisionTreeClassifier:\n",
      "     |  \n",
      "     |  n_features_\n",
      "     |      DEPRECATED: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseDecisionTree:\n",
      "     |  \n",
      "     |  apply(self, X, check_input=True)\n",
      "     |      Return the index of the leaf that each sample is predicted as.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array-like of shape (n_samples,)\n",
      "     |          For each datapoint x in X, return the index of the leaf x\n",
      "     |          ends up in. Leaves are numbered within\n",
      "     |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      "     |          numbering.\n",
      "     |  \n",
      "     |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
      "     |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
      "     |      \n",
      "     |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
      "     |      process.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (class labels) as integers or strings.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. Splits are also\n",
      "     |          ignored if they would result in any single class carrying a\n",
      "     |          negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ccp_path : :class:`~sklearn.utils.Bunch`\n",
      "     |          Dictionary-like object, with the following attributes.\n",
      "     |      \n",
      "     |          ccp_alphas : ndarray\n",
      "     |              Effective alphas of subtree during pruning.\n",
      "     |      \n",
      "     |          impurities : ndarray\n",
      "     |              Sum of the impurities of the subtree leaves for the\n",
      "     |              corresponding alpha value in ``ccp_alphas``.\n",
      "     |  \n",
      "     |  decision_path(self, X, check_input=True)\n",
      "     |      Return the decision path in the tree.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      "     |          Return a node indicator CSR matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |  \n",
      "     |  get_depth(self)\n",
      "     |      Return the depth of the decision tree.\n",
      "     |      \n",
      "     |      The depth of a tree is the maximum distance between the root\n",
      "     |      and any leaf.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self.tree_.max_depth : int\n",
      "     |          The maximum depth of the tree.\n",
      "     |  \n",
      "     |  get_n_leaves(self)\n",
      "     |      Return the number of leaves of the decision tree.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self.tree_.n_leaves : int\n",
      "     |          Number of leaves.\n",
      "     |  \n",
      "     |  predict(self, X, check_input=True)\n",
      "     |      Predict class or regression value for X.\n",
      "     |      \n",
      "     |      For a classification model, the predicted class for each sample in X is\n",
      "     |      returned. For a regression model, the predicted value based on X is\n",
      "     |      returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The predicted classes, or the predict values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseDecisionTree:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances.\n",
      "     |      \n",
      "     |      The importance of a feature is computed as the (normalized) total\n",
      "     |      reduction of the criterion brought by that feature.\n",
      "     |      It is also known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          Normalized total reduction of criteria by feature\n",
      "     |          (Gini importance).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class ExtraTreeRegressor(DecisionTreeRegressor)\n",
      "     |  ExtraTreeRegressor(*, criterion='squared_error', splitter='random', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', random_state=None, min_impurity_decrease=0.0, max_leaf_nodes=None, ccp_alpha=0.0)\n",
      "     |  \n",
      "     |  An extremely randomized tree regressor.\n",
      "     |  \n",
      "     |  Extra-trees differ from classic decision trees in the way they are built.\n",
      "     |  When looking for the best split to separate the samples of a node into two\n",
      "     |  groups, random splits are drawn for each of the `max_features` randomly\n",
      "     |  selected features and the best split among those is chosen. When\n",
      "     |  `max_features` is set 1, this amounts to building a totally random\n",
      "     |  decision tree.\n",
      "     |  \n",
      "     |  Warning: Extra-trees should only be used within ensemble methods.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <tree>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  criterion : {\"squared_error\", \"friedman_mse\"}, default=\"squared_error\"\n",
      "     |      The function to measure the quality of a split. Supported criteria\n",
      "     |      are \"squared_error\" for the mean squared error, which is equal to\n",
      "     |      variance reduction as feature selection criterion and \"mae\" for the\n",
      "     |      mean absolute error.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Mean Absolute Error (MAE) criterion.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |          Poisson deviance criterion.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          Criterion \"mse\" was deprecated in v1.0 and will be removed in\n",
      "     |          version 1.2. Use `criterion=\"squared_error\"` which is equivalent.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          Criterion \"mae\" was deprecated in v1.0 and will be removed in\n",
      "     |          version 1.2. Use `criterion=\"absolute_error\"` which is equivalent.\n",
      "     |  \n",
      "     |  splitter : {\"random\", \"best\"}, default=\"random\"\n",
      "     |      The strategy used to choose the split at each node. Supported\n",
      "     |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      "     |      the best random split.\n",
      "     |  \n",
      "     |  max_depth : int, default=None\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_features : int, float, {\"auto\", \"sqrt\", \"log2\"} or None, default=\"auto\"\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `int(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=n_features`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Used to pick randomly the `max_features` used at each split.\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  ccp_alpha : non-negative float, default=0.0\n",
      "     |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "     |      subtree with the largest cost complexity that is smaller than\n",
      "     |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "     |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  max_features_ : int\n",
      "     |      The inferred value of max_features.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when ``fit`` is performed.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |         `n_features_` is deprecated in 1.0 and will be removed in\n",
      "     |         1.2. Use `n_features_in_` instead.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      Return impurity-based feature importances (the higher, the more\n",
      "     |      important the feature).\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  tree_ : Tree instance\n",
      "     |      The underlying Tree object. Please refer to\n",
      "     |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      "     |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      "     |      for basic usage of these attributes.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  ExtraTreeClassifier : An extremely randomized tree classifier.\n",
      "     |  sklearn.ensemble.ExtraTreesClassifier : An extra-trees classifier.\n",
      "     |  sklearn.ensemble.ExtraTreesRegressor : An extra-trees regressor.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
      "     |         Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_diabetes\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.ensemble import BaggingRegressor\n",
      "     |  >>> from sklearn.tree import ExtraTreeRegressor\n",
      "     |  >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |  ...     X, y, random_state=0)\n",
      "     |  >>> extra_tree = ExtraTreeRegressor(random_state=0)\n",
      "     |  >>> reg = BaggingRegressor(extra_tree, random_state=0).fit(\n",
      "     |  ...     X_train, y_train)\n",
      "     |  >>> reg.score(X_test, y_test)\n",
      "     |  0.33...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExtraTreeRegressor\n",
      "     |      DecisionTreeRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      BaseDecisionTree\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, criterion='squared_error', splitter='random', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', random_state=None, min_impurity_decrease=0.0, max_leaf_nodes=None, ccp_alpha=0.0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DecisionTreeRegressor:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted='deprecated')\n",
      "     |      Build a decision tree regressor from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (real numbers). Use ``dtype=np.float64`` and\n",
      "     |          ``order='C'`` for maximum efficiency.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      X_idx_sorted : deprecated, default=\"deprecated\"\n",
      "     |          This parameter is deprecated and has no effect.\n",
      "     |          It will be removed in 1.1 (renaming of 0.26).\n",
      "     |      \n",
      "     |          .. deprecated:: 0.24\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : DecisionTreeRegressor\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from DecisionTreeRegressor:\n",
      "     |  \n",
      "     |  n_features_\n",
      "     |      DEPRECATED: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseDecisionTree:\n",
      "     |  \n",
      "     |  apply(self, X, check_input=True)\n",
      "     |      Return the index of the leaf that each sample is predicted as.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array-like of shape (n_samples,)\n",
      "     |          For each datapoint x in X, return the index of the leaf x\n",
      "     |          ends up in. Leaves are numbered within\n",
      "     |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      "     |          numbering.\n",
      "     |  \n",
      "     |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
      "     |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
      "     |      \n",
      "     |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
      "     |      process.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (class labels) as integers or strings.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. Splits are also\n",
      "     |          ignored if they would result in any single class carrying a\n",
      "     |          negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ccp_path : :class:`~sklearn.utils.Bunch`\n",
      "     |          Dictionary-like object, with the following attributes.\n",
      "     |      \n",
      "     |          ccp_alphas : ndarray\n",
      "     |              Effective alphas of subtree during pruning.\n",
      "     |      \n",
      "     |          impurities : ndarray\n",
      "     |              Sum of the impurities of the subtree leaves for the\n",
      "     |              corresponding alpha value in ``ccp_alphas``.\n",
      "     |  \n",
      "     |  decision_path(self, X, check_input=True)\n",
      "     |      Return the decision path in the tree.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      "     |          Return a node indicator CSR matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |  \n",
      "     |  get_depth(self)\n",
      "     |      Return the depth of the decision tree.\n",
      "     |      \n",
      "     |      The depth of a tree is the maximum distance between the root\n",
      "     |      and any leaf.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self.tree_.max_depth : int\n",
      "     |          The maximum depth of the tree.\n",
      "     |  \n",
      "     |  get_n_leaves(self)\n",
      "     |      Return the number of leaves of the decision tree.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self.tree_.n_leaves : int\n",
      "     |          Number of leaves.\n",
      "     |  \n",
      "     |  predict(self, X, check_input=True)\n",
      "     |      Predict class or regression value for X.\n",
      "     |      \n",
      "     |      For a classification model, the predicted class for each sample in X is\n",
      "     |      returned. For a regression model, the predicted value based on X is\n",
      "     |      returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The predicted classes, or the predict values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseDecisionTree:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances.\n",
      "     |      \n",
      "     |      The importance of a feature is computed as the (normalized) total\n",
      "     |      reduction of the criterion brought by that feature.\n",
      "     |      It is also known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          Normalized total reduction of criteria by feature\n",
      "     |          (Gini importance).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "\n",
      "FUNCTIONS\n",
      "    export_graphviz(decision_tree, out_file=None, *, max_depth=None, feature_names=None, class_names=None, label='all', filled=False, leaves_parallel=False, impurity=True, node_ids=False, proportion=False, rotate=False, rounded=False, special_characters=False, precision=3, fontname='helvetica')\n",
      "        Export a decision tree in DOT format.\n",
      "        \n",
      "        This function generates a GraphViz representation of the decision tree,\n",
      "        which is then written into `out_file`. Once exported, graphical renderings\n",
      "        can be generated using, for example::\n",
      "        \n",
      "            $ dot -Tps tree.dot -o tree.ps      (PostScript format)\n",
      "            $ dot -Tpng tree.dot -o tree.png    (PNG format)\n",
      "        \n",
      "        The sample counts that are shown are weighted with any sample_weights that\n",
      "        might be present.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <tree>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        decision_tree : decision tree classifier\n",
      "            The decision tree to be exported to GraphViz.\n",
      "        \n",
      "        out_file : object or str, default=None\n",
      "            Handle or name of the output file. If ``None``, the result is\n",
      "            returned as a string.\n",
      "        \n",
      "            .. versionchanged:: 0.20\n",
      "                Default of out_file changed from \"tree.dot\" to None.\n",
      "        \n",
      "        max_depth : int, default=None\n",
      "            The maximum depth of the representation. If None, the tree is fully\n",
      "            generated.\n",
      "        \n",
      "        feature_names : list of str, default=None\n",
      "            Names of each of the features.\n",
      "            If None generic names will be used (\"feature_0\", \"feature_1\", ...).\n",
      "        \n",
      "        class_names : list of str or bool, default=None\n",
      "            Names of each of the target classes in ascending numerical order.\n",
      "            Only relevant for classification and not supported for multi-output.\n",
      "            If ``True``, shows a symbolic representation of the class name.\n",
      "        \n",
      "        label : {'all', 'root', 'none'}, default='all'\n",
      "            Whether to show informative labels for impurity, etc.\n",
      "            Options include 'all' to show at every node, 'root' to show only at\n",
      "            the top root node, or 'none' to not show at any node.\n",
      "        \n",
      "        filled : bool, default=False\n",
      "            When set to ``True``, paint nodes to indicate majority class for\n",
      "            classification, extremity of values for regression, or purity of node\n",
      "            for multi-output.\n",
      "        \n",
      "        leaves_parallel : bool, default=False\n",
      "            When set to ``True``, draw all leaf nodes at the bottom of the tree.\n",
      "        \n",
      "        impurity : bool, default=True\n",
      "            When set to ``True``, show the impurity at each node.\n",
      "        \n",
      "        node_ids : bool, default=False\n",
      "            When set to ``True``, show the ID number on each node.\n",
      "        \n",
      "        proportion : bool, default=False\n",
      "            When set to ``True``, change the display of 'values' and/or 'samples'\n",
      "            to be proportions and percentages respectively.\n",
      "        \n",
      "        rotate : bool, default=False\n",
      "            When set to ``True``, orient tree left to right rather than top-down.\n",
      "        \n",
      "        rounded : bool, default=False\n",
      "            When set to ``True``, draw node boxes with rounded corners.\n",
      "        \n",
      "        special_characters : bool, default=False\n",
      "            When set to ``False``, ignore special characters for PostScript\n",
      "            compatibility.\n",
      "        \n",
      "        precision : int, default=3\n",
      "            Number of digits of precision for floating point in the values of\n",
      "            impurity, threshold and value attributes of each node.\n",
      "        \n",
      "        fontname : str, default='helvetica'\n",
      "            Name of font used to render text.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dot_data : str\n",
      "            String representation of the input tree in GraphViz dot format.\n",
      "            Only returned if ``out_file`` is None.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.datasets import load_iris\n",
      "        >>> from sklearn import tree\n",
      "        \n",
      "        >>> clf = tree.DecisionTreeClassifier()\n",
      "        >>> iris = load_iris()\n",
      "        \n",
      "        >>> clf = clf.fit(iris.data, iris.target)\n",
      "        >>> tree.export_graphviz(clf)\n",
      "        'digraph Tree {...\n",
      "    \n",
      "    export_text(decision_tree, *, feature_names=None, max_depth=10, spacing=3, decimals=2, show_weights=False)\n",
      "        Build a text report showing the rules of a decision tree.\n",
      "        \n",
      "        Note that backwards compatibility may not be supported.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        decision_tree : object\n",
      "            The decision tree estimator to be exported.\n",
      "            It can be an instance of\n",
      "            DecisionTreeClassifier or DecisionTreeRegressor.\n",
      "        \n",
      "        feature_names : list of str, default=None\n",
      "            A list of length n_features containing the feature names.\n",
      "            If None generic names will be used (\"feature_0\", \"feature_1\", ...).\n",
      "        \n",
      "        max_depth : int, default=10\n",
      "            Only the first max_depth levels of the tree are exported.\n",
      "            Truncated branches will be marked with \"...\".\n",
      "        \n",
      "        spacing : int, default=3\n",
      "            Number of spaces between edges. The higher it is, the wider the result.\n",
      "        \n",
      "        decimals : int, default=2\n",
      "            Number of decimal digits to display.\n",
      "        \n",
      "        show_weights : bool, default=False\n",
      "            If true the classification weights will be exported on each leaf.\n",
      "            The classification weights are the number of samples each class.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        report : str\n",
      "            Text summary of all the rules in the decision tree.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        >>> from sklearn.datasets import load_iris\n",
      "        >>> from sklearn.tree import DecisionTreeClassifier\n",
      "        >>> from sklearn.tree import export_text\n",
      "        >>> iris = load_iris()\n",
      "        >>> X = iris['data']\n",
      "        >>> y = iris['target']\n",
      "        >>> decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
      "        >>> decision_tree = decision_tree.fit(X, y)\n",
      "        >>> r = export_text(decision_tree, feature_names=iris['feature_names'])\n",
      "        >>> print(r)\n",
      "        |--- petal width (cm) <= 0.80\n",
      "        |   |--- class: 0\n",
      "        |--- petal width (cm) >  0.80\n",
      "        |   |--- petal width (cm) <= 1.75\n",
      "        |   |   |--- class: 1\n",
      "        |   |--- petal width (cm) >  1.75\n",
      "        |   |   |--- class: 2\n",
      "    \n",
      "    plot_tree(decision_tree, *, max_depth=None, feature_names=None, class_names=None, label='all', filled=False, impurity=True, node_ids=False, proportion=False, rounded=False, precision=3, ax=None, fontsize=None)\n",
      "        Plot a decision tree.\n",
      "        \n",
      "        The sample counts that are shown are weighted with any sample_weights that\n",
      "        might be present.\n",
      "        \n",
      "        The visualization is fit automatically to the size of the axis.\n",
      "        Use the ``figsize`` or ``dpi`` arguments of ``plt.figure``  to control\n",
      "        the size of the rendering.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <tree>`.\n",
      "        \n",
      "        .. versionadded:: 0.21\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        decision_tree : decision tree regressor or classifier\n",
      "            The decision tree to be plotted.\n",
      "        \n",
      "        max_depth : int, default=None\n",
      "            The maximum depth of the representation. If None, the tree is fully\n",
      "            generated.\n",
      "        \n",
      "        feature_names : list of strings, default=None\n",
      "            Names of each of the features.\n",
      "            If None, generic names will be used (\"X[0]\", \"X[1]\", ...).\n",
      "        \n",
      "        class_names : list of str or bool, default=None\n",
      "            Names of each of the target classes in ascending numerical order.\n",
      "            Only relevant for classification and not supported for multi-output.\n",
      "            If ``True``, shows a symbolic representation of the class name.\n",
      "        \n",
      "        label : {'all', 'root', 'none'}, default='all'\n",
      "            Whether to show informative labels for impurity, etc.\n",
      "            Options include 'all' to show at every node, 'root' to show only at\n",
      "            the top root node, or 'none' to not show at any node.\n",
      "        \n",
      "        filled : bool, default=False\n",
      "            When set to ``True``, paint nodes to indicate majority class for\n",
      "            classification, extremity of values for regression, or purity of node\n",
      "            for multi-output.\n",
      "        \n",
      "        impurity : bool, default=True\n",
      "            When set to ``True``, show the impurity at each node.\n",
      "        \n",
      "        node_ids : bool, default=False\n",
      "            When set to ``True``, show the ID number on each node.\n",
      "        \n",
      "        proportion : bool, default=False\n",
      "            When set to ``True``, change the display of 'values' and/or 'samples'\n",
      "            to be proportions and percentages respectively.\n",
      "        \n",
      "        rounded : bool, default=False\n",
      "            When set to ``True``, draw node boxes with rounded corners and use\n",
      "            Helvetica fonts instead of Times-Roman.\n",
      "        \n",
      "        precision : int, default=3\n",
      "            Number of digits of precision for floating point in the values of\n",
      "            impurity, threshold and value attributes of each node.\n",
      "        \n",
      "        ax : matplotlib axis, default=None\n",
      "            Axes to plot to. If None, use current axis. Any previous content\n",
      "            is cleared.\n",
      "        \n",
      "        fontsize : int, default=None\n",
      "            Size of text font. If None, determined automatically to fit figure.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        annotations : list of artists\n",
      "            List containing the artists for the annotation boxes making up the\n",
      "            tree.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.datasets import load_iris\n",
      "        >>> from sklearn import tree\n",
      "        \n",
      "        >>> clf = tree.DecisionTreeClassifier(random_state=0)\n",
      "        >>> iris = load_iris()\n",
      "        \n",
      "        >>> clf = clf.fit(iris.data, iris.target)\n",
      "        >>> tree.plot_tree(clf)\n",
      "        [...]\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BaseDecisionTree', 'DecisionTreeClassifier', 'DecisionTree...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\tree\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a94413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn\n",
      "\n",
      "DESCRIPTION\n",
      "    Machine learning module for Python\n",
      "    ==================================\n",
      "    \n",
      "    sklearn is a Python module integrating classical machine\n",
      "    learning algorithms in the tightly-knit world of scientific Python\n",
      "    packages (numpy, scipy, matplotlib).\n",
      "    \n",
      "    It aims to provide simple and efficient solutions to learning problems\n",
      "    that are accessible to everybody and reusable in various contexts:\n",
      "    machine-learning as a versatile tool for science and engineering.\n",
      "    \n",
      "    See http://scikit-learn.org for complete documentation.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __check_build (package)\n",
      "    _build_utils (package)\n",
      "    _config\n",
      "    _distributor_init\n",
      "    _isotonic\n",
      "    _loss (package)\n",
      "    _min_dependencies\n",
      "    base\n",
      "    calibration\n",
      "    cluster (package)\n",
      "    compose (package)\n",
      "    conftest\n",
      "    covariance (package)\n",
      "    cross_decomposition (package)\n",
      "    datasets (package)\n",
      "    decomposition (package)\n",
      "    discriminant_analysis\n",
      "    dummy\n",
      "    ensemble (package)\n",
      "    exceptions\n",
      "    experimental (package)\n",
      "    externals (package)\n",
      "    feature_extraction (package)\n",
      "    feature_selection (package)\n",
      "    gaussian_process (package)\n",
      "    impute (package)\n",
      "    inspection (package)\n",
      "    isotonic\n",
      "    kernel_approximation\n",
      "    kernel_ridge\n",
      "    linear_model (package)\n",
      "    manifold (package)\n",
      "    metrics (package)\n",
      "    mixture (package)\n",
      "    model_selection (package)\n",
      "    multiclass\n",
      "    multioutput\n",
      "    naive_bayes\n",
      "    neighbors (package)\n",
      "    neural_network (package)\n",
      "    pipeline\n",
      "    preprocessing (package)\n",
      "    random_projection\n",
      "    semi_supervised (package)\n",
      "    setup\n",
      "    svm (package)\n",
      "    tests (package)\n",
      "    tree (package)\n",
      "    utils (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    clone(estimator, *, safe=True)\n",
      "        Construct a new unfitted estimator with the same parameters.\n",
      "        \n",
      "        Clone does a deep copy of the model in an estimator\n",
      "        without actually copying attached data. It returns a new estimator\n",
      "        with the same parameters that has not been fitted on any data.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : {list, tuple, set} of estimator instance or a single             estimator instance\n",
      "            The estimator or group of estimators to be cloned.\n",
      "        safe : bool, default=True\n",
      "            If safe is False, clone will fall back to a deep copy on objects\n",
      "            that are not estimators.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        estimator : object\n",
      "            The deep copy of the input, an estimator if input is an estimator.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If the estimator's `random_state` parameter is an integer (or if the\n",
      "        estimator doesn't have a `random_state` parameter), an *exact clone* is\n",
      "        returned: the clone and the original estimator will give the exact same\n",
      "        results. Otherwise, *statistical clone* is returned: the clone might\n",
      "        return different results from the original estimator. More details can be\n",
      "        found in :ref:`randomness`.\n",
      "    \n",
      "    config_context(*, assume_finite=None, working_memory=None, print_changed_only=None, display=None)\n",
      "        Context manager for global scikit-learn configuration.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, default=None\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error. If None, the existing value won't change.\n",
      "            The default value is False.\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. If None, the existing value won't change.\n",
      "            The default value is 1024.\n",
      "        \n",
      "        print_changed_only : bool, default=None\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()', but would print\n",
      "            'SVC(C=1.0, cache_size=200, ...)' with all the non-changed parameters\n",
      "            when False. If None, the existing value won't change.\n",
      "            The default value is True.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Default changed from False to True.\n",
      "        \n",
      "        display : {'text', 'diagram'}, default=None\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. If None, the existing value won't change.\n",
      "            The default value is 'text'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        Yields\n",
      "        ------\n",
      "        None.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        set_config : Set global scikit-learn configuration.\n",
      "        get_config : Retrieve current values of the global configuration.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        All settings, not just those presently modified, will be returned to\n",
      "        their previous values when the context manager is exited.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import sklearn\n",
      "        >>> from sklearn.utils.validation import assert_all_finite\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     assert_all_finite([float('nan')])\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     with sklearn.config_context(assume_finite=False):\n",
      "        ...         assert_all_finite([float('nan')])\n",
      "        Traceback (most recent call last):\n",
      "        ...\n",
      "        ValueError: Input contains NaN...\n",
      "    \n",
      "    get_config()\n",
      "        Retrieve current values for configuration set by :func:`set_config`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        config : dict\n",
      "            Keys are parameter names that can be passed to :func:`set_config`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context : Context manager for global scikit-learn configuration.\n",
      "        set_config : Set global scikit-learn configuration.\n",
      "    \n",
      "    set_config(assume_finite=None, working_memory=None, print_changed_only=None, display=None)\n",
      "        Set global scikit-learn configuration\n",
      "        \n",
      "        .. versionadded:: 0.19\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, default=None\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error.  Global default: False.\n",
      "        \n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. Global default: 1024.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        print_changed_only : bool, default=None\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()' while the default\n",
      "            behaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\n",
      "            all the non-changed parameters.\n",
      "        \n",
      "            .. versionadded:: 0.21\n",
      "        \n",
      "        display : {'text', 'diagram'}, default=None\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. Default is 'text'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context : Context manager for global scikit-learn configuration.\n",
      "        get_config : Retrieve current values of the global configuration.\n",
      "    \n",
      "    show_versions()\n",
      "        Print useful debugging information\"\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "\n",
      "DATA\n",
      "    __SKLEARN_SETUP__ = False\n",
      "    __all__ = ['calibration', 'cluster', 'covariance', 'cross_decompositio...\n",
      "\n",
      "VERSION\n",
      "    1.0.2\n",
      "\n",
      "FILE\n",
      "    c:\\users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297a098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
